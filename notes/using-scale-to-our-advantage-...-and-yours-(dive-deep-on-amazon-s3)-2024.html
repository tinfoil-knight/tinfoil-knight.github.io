<!DOCTYPE html>
<html><head><title>Using scale to our advantage ... and yours (Dive deep on Amazon S3) (2024)</title><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Using scale to our advantage ... and yours (Dive deep on Amazon S3) (2024)"/><link rel="icon" href="../static/favicon.ico"/><meta name="generator" content="Quartz"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Open Sans:wght@400;700&amp;family=Spectral:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="notes/using-scale-to-our-advantage-...-and-yours-(dive-deep-on-amazon-s3)-2024"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title "><a href="..">Machines Fail</a></h1><div class="spacer mobile-only"></div><div class="search "><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="results-container"></div></div></div></div><div class="darkmode "><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Light mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Dark mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container " aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../notes/">notes</a></div></nav><h1 class="article-title ">Using scale to our advantage ... and yours (Dive deep on Amazon S3) (2024)</h1><p class="content-meta ">Dec 08, 2024 | 1740 words</p><ul class="tags "><li><a href="../tags/talks" class="internal tag-link">#talks</a></li><li><a href="../tags/aws-reinvent" class="internal tag-link">#aws-reinvent</a></li></ul><div class="content-frontmatter"><div style="list-style: none; margin-left: 0; padding-left: 0;">Source : <a href="https://www.youtube.com/watch?v=NXehLy7IiPM" class="external" style="color: inherit;">https://www.youtube.com/watch?v=NXehLy7IiPM</a></div><div style="list-style: none; margin-left: 0; padding-left: 0;">From : <span>Seth Markle, James Bornholt</span></div></div><div class="toc "><button type="button" id="toc" class="collapsed"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#physics-of-data" data-for="physics-of-data">Physics of Data</a></li><li class="depth-0"><a href="#designing-de-correlated-systems" data-for="designing-de-correlated-systems">Designing de-correlated systems</a></li><li class="depth-0"><a href="#engineering-for-failure-is-engineering-for-velocity" data-for="engineering-for-failure-is-engineering-for-velocity">Engineering for failure is engineering for velocity</a></li><li class="depth-0"><a href="#appendix-and-further-reading" data-for="appendix-and-further-reading">Appendix and Further Reading</a></li></ul></div></div></div></div><article class="popover-hint"><p>Seth Markle : Senior Principal Engineer ; working on S3 for the last 15 years, worked on object indexing, disk fleet, frontend systems, S3 metadata systems</p>
<p><a href="https://jamesbornholt.com/" class="external alias">James Bornholt</a>  : Principal Engineer at S3 ; worked on disk fleet, S3 Tables &amp; connectors like mountpoint, pytorch</p>
<p>Most talks on scale focus on how to tolerate getting larger . This talk focuses on how the system improves because it’s larger.</p>
<p>Stats on AWS S3 at scale</p>
<ul>
<li>Over 400 trillion objects</li>
<li>Over a quadrillion requests per year</li>
<li>Over 200 billion events daily</li>
<li>Over 1 PB/s transferred at peak</li>
</ul>
<h2 id="physics-of-data">Physics of Data<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#physics-of-data" class="internal alias"> §</a></h2>
<ul>
<li>S3 has tens of millions of drives and they run really hot in terms of their workload.</li>
<li>A drive can only absorb a limited amount of work.</li>
<li>If traffic can’t be spread across those 10s of millions of drives, then it limits what our customer can do even though we’ve the capacity on hand to serve that traffic.</li>
<li>Some patterns emerge at scale.</li>
<li>Actors in the system: Customer, Hardware, Software</li>
</ul>
<h3 id="the-hardware-hdds">The hardware, HDDs<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-hardware-hdds" class="internal alias"> §</a></h3>
<p>Mechanical components with 2 primary movements required to read data: spinning platters, actuators (with read/write head).</p>
<p>When you read data, you provide an address (called a Logical Block Address or LBA) which is just an offset from 0 to the capacity of the drive. The actuator swings back and forth to move into location. Once it’s on the right track, it needs to wait for the data to come around if it’s not directly underneath it (half a rotation on average).</p>
<p>Factors that cause drives to take time when you’re reading data</p>
<ul>
<li>Seek time: Seeking is the act of actuator moving. Time for that to happen is called seek time.</li>
<li>Rotational latency: Time spent waiting for rotation is called rotational latency.</li>
</ul>
<p>See <a href="https://animagraffs.com/hard-disk-drive" class="external">https://animagraffs.com/hard-disk-drive</a></p>
<p><img src="https://animagraffs.com/wp-content/uploads/how-hard-disk-drives-work-1.png" alt/></p>
<h3 id="the-software-replication">The software, Replication<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-software-replication" class="internal alias"> §</a></h3>
<p><img src="../attachments/pasted-image-20241208021153.png" width="auto" height="auto"/></p>
<ul>
<li>When you store an object in S3, it’s broken apart into pieces called shards.</li>
<li>Shards are stored redundantly across the disk fleet using erasure coding.</li>
<li>The drives run use a filesystem software called <strong>ShardStore</strong>. ShardStore is a log structured file system.</li>
</ul>
<p><img src="../attachments/pasted-image-20241208021455.png" width="auto" height="auto"/></p>
<ul>
<li>Writes becomes more efficient since the actuator doesn’t have to move back and forth.</li>
<li>Reads require random IO since the customer can request any data.</li>
</ul>
<h3 id="individual-workloads">Individual workloads<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#individual-workloads" class="internal alias"> §</a></h3>
<ul>
<li>Storage workloads tend to be bursty (idle for a long time and then suddenly demanding)</li>
<li>Customer Example: FINRA
<ul>
<li>Regulates securities market. Collect, process, analyzes all txns. across equity and option markets.</li>
<li>Involves ingesting 100s of billions of records each day.
<ul>
<li>In August ‘24, they had a peak day w/ 906 billion records processed two days in a row.</li>
</ul>
</li>
<li>On average, they process 30TB of data everyday.</li>
<li>They’ve a strict 4 hour SLA for processing this data.</li>
<li>Rest of the time they ingest records but bulk processing happens in a short window only.</li>
</ul>
</li>
<li>Another example: How many drives would an example workload need to operate?
<ul>
<li>Workload: Hold 1PB of data, 1MB per object, Process the data in a 1hr</li>
<li>i.e. Access Rate: 275GB/s at peak;  ~ 1 billion objects</li>
<li>Physics of data
<ul>
<li>Rotation = 4ms, average (7200 RPM drive i.e. 8ms per rotation and half a rotation on avg.)</li>
<li>Seek = 4ms, average (move half the radius on the platter on avg.)</li>
<li>Say, 1MB objects are chunked into 0.5MB shards.</li>
<li>0.5MB transfer = 2ms, average</li>
<li>Total = 10ms per read (rotation + seek + transfer)
<ul>
<li>i.e. 50MB/s at 0.5MB per read</li>
</ul>
</li>
</ul>
</li>
<li>For just storing 1PB of data they’d need 50 drives at 20TB per drive</li>
<li>To access the data at 275GB/s, they’d need 5500 drives at 50MB/s
<ul>
<li>100x difference to support bursts</li>
<li>System is idle the rest of the time</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>While individual workloads are bursty, aggregated de-correlated workloads aren’t. When the workloads are layered together, they become more predictable at S3’s scale. There are some patterns though like younger data is “hotter” data, small objects are accessed more frequently.</p>
</blockquote>
<blockquote>
<p>Shards for a customer can thus be spread across a large number of disks than their storage would typically require and get the additional throughput.<br/>
Peaks for some workloads map to valleys for others.</p>
</blockquote>
<h3 id="thermodynamics--balancing-the-aggregates">Thermodynamics : Balancing the aggregates<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#thermodynamics--balancing-the-aggregates" class="internal alias"> §</a></h3>
<ul>
<li>While aggregate workloads are predictable, you still need to pay attention to how traffic distributes across the fleet to prevent hot &amp; cold spots across the disk. Doesn’t happen automatically.</li>
<li>Objects follow some predictable patterns in aggregate (eg. younger data is hotter data) but over time drives “cools off” as it gets older.
<ul>
<li>Deletes might poke some holes and make some data on the old drive “hot” but not nearly as much as the new drives.</li>
<li>Uncorrected, the system tends towards “coldness”.</li>
</ul>
</li>
<li>At S3’s scale, they’ve storage racks of every traffic profile in the system. There’s a really large surface area for rebalancing &amp; re-encoding when they get things wrong. They constantly move data around to balance storage temperatures across racks and hard drives.</li>
<li>One useful opportunity for them to do rebalancing is when they get a new storage rack.
<ul>
<li>They weight 1000s of pounds.</li>
<li>Called “JBODs” (just a bunch of disks).</li>
<li>20TB / disk. Racks have physical capacity of 20PB.</li>
</ul>
</li>
<li>Can’t start sending all traffic to the new drives because it’d overwhelm those drives.
<ul>
<li>So, new racks are brought online &amp; filled ~80% w/ existing cooler S3 data.</li>
<li>This frees up capacity in aggregate across pre-existing spindles &amp; thus there’s a lot of space on old spindles for hot data to land &amp; not just in the new spindles.</li>
</ul>
</li>
</ul>
<h2 id="designing-de-correlated-systems">Designing de-correlated systems<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#designing-de-correlated-systems" class="internal alias"> §</a></h2>
<ul>
<li>Every part of S3 is designed to de-correlate workloads.</li>
<li>Eg: If you’ve 2 customers running workloads then those 2 workloads should be de-correlated from each other. And even a single customer, your workloads should be de-correlated from itself.</li>
</ul>
<h3 id="assigning-buckets-to-their-storage">Assigning buckets to their storage<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#assigning-buckets-to-their-storage" class="internal alias"> §</a></h3>
<p>A straw-man approach: You assign a hard drive to a bucket and you keep putting objects in the same drive until its full. You then assign another drive and so on.</p>
<ul>
<li>Advantage: Simple &amp; easy to operate and reason about.</li>
<li>Issues
<ul>
<li>Hard drives fail so data needs to be stored on multiple drives.</li>
<li>Not cost-efficient to assign an entire drive esp. for a small customer.</li>
<li>A small customer is going to share the drive w/ other customers &amp; many workloads will hit the same drive.</li>
<li>For large customers, performance is constrained by their storage footprint.</li>
</ul>
</li>
</ul>
<h4 id="shuffle-sharding">Shuffle sharding<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#shuffle-sharding" class="internal alias"> §</a></h4>
<ul>
<li>Workloads are randomly spread across the fleet, across the drives. Even for a PUT to the same bucket or the same key.
<ul>
<li>Not constrained by static resources i.e. if you’ve 1PB of data, you can be spread across more than 1PB worth of drives.</li>
<li>A small customer is insulated against the “noisy neighbor” problem. If an object from another customer is “hot”, you won’t be affected by it even if you share some drives since there are other drives where you can get that data.</li>
<li>Shuffle sharding is also used for DNS resolution of S3 buckets so that your requests are spread across multiple (front-end) hosts.</li>
</ul>
</li>
<li>Placing data for shuffle sharding
<ul>
<li>Can’t look at every drive and figure out the optimal set of drives for every PUT operation because S3 uses 10s of millions of drives &amp; gets millions of RPS.</li>
<li>Completely randomly leads to a bell curve distribution in a graph for capacity usage over time. Over a long time, some drives end up being 10-20% less full than other drives. Wasted capacity.</li>
<li><strong>The power of two random choices</strong>: Pick 2 drives totally at random &amp; figure out which one is least used and use that drive.</li>
</ul>
</li>
</ul>
<p><img src="../attachments/pasted-image-20250102005353.png" width="auto" height="auto"/></p>
<blockquote>
<p>Sidenote: Shuffle sharding is also used in AWS Common runtime (CRT) which is a low-level library for working with S3 and implements retries, parallelization of requests etc. to get high throughput for S3. It’s not used by most customers directly and is included in SDKs.<br/>
To improve performance when you talk to S3, the CRT keeps a live estimate of the tail latency of the requests you’re making. CRT intentionally cancels &amp; retries requests that go past a certain threshold (say p95).</p>
<p>It’s very likely that the new request will go to a different front-end host &amp; different drive &amp; the retry will be fast.</p>
</blockquote>
<p><img src="../attachments/pasted-image-20250102004306.png" width="auto" height="auto"/></p>
<h2 id="engineering-for-failure-is-engineering-for-velocity">Engineering for failure is engineering for velocity<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#engineering-for-failure-is-engineering-for-velocity" class="internal alias"> §</a></h2>
<ul>
<li>At scale, knowing that a system has failed is surprisingly hard.</li>
<li>All bytes stored are constantly scanned to check for integrity.
<ul>
<li>There are alarms on p100 longest time, the most time a byte has gone w/o being scanned.</li>
</ul>
</li>
<li>Erasure coding for fault tolerance
<ul>
<li>Replication is a simple way to tolerate faults - both individual drives and entire AZs but comes w/ high overhead.</li>
<li>Erasure coding splits an object into K shards &amp; creates extra parity shards. The object can be rebuilt from any K of the shards.
<ul>
<li>Eg: Object is split into 5 shards, 4 extra parity shards are added. These 9 shards can be distributed across 3 AZs. Even if an entire AZ goes done, the original data can still be recovered from any 5 shards.</li>
</ul>
</li>
<li>Compared to replication where the overhead would be 3x for storing a copy in 2 separate AZs (in practice it’d much higher to guarantee S3’s 11 nines durability), erasure coding only has 1.8x overhead.</li>
<li>Erasure coding allows them to deploy new software or hardware safely by exposing only a few shards to the changes. Eg: some new software could just be deployed on a single host or rack, a new hard drive type can just be used for 1, 10 or 100 drives.</li>
</ul>
</li>
<li>Using shuffle sharding at scale, there’s going to be some objects that are overly exposed to new buggy hardware or software.
<ul>
<li>S3’s shuffle sharding system has a guardrail. It knows about new hardware and software. When it picks a new random assignment, it limits itself to putting only 1 shard on that new hardware or software.</li>
<li>When ShardStore (storage node software) was rolled out for S3, they started w/ only storing 1 shard for a while. The system was tuned to eventually raise the no. of shards being stored &amp; eventually the guardrail was removed.</li>
</ul>
</li>
<li>Improved performance through fault tolerance
<ul>
<li>Similar to AWS CRT, additional shards &amp; shuffle sharding allow hedging against tail latency by over-reading. Reading an extra shard lets them ignore the slowest shard &amp; pick another to get the object back. The request to the slowest shard is cancelled.</li>
</ul>
</li>
</ul>
<h2 id="appendix-and-further-reading">Appendix and Further Reading<a aria-hidden="true" tabindex="-1" data-no-popover="true" href="#appendix-and-further-reading" class="internal alias"> §</a></h2>
<ul>
<li>Slides: <a href="https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/stg/STG302_Dive-deep-on-Amazon-S3.pdf" class="external">https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/stg/STG302_Dive-deep-on-Amazon-S3.pdf</a></li>
<li><a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html" class="external alias">Building and operating a pretty big storage system called S3</a></li>
<li><a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/" class="external alias">Workload isolation using shuffle-sharding</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3477132.3483540" class="external alias">Using Lightweight Formal Methods to Validate a Key-Value Storage Node in Amazon S3</a></li>
</ul></article></div><div class="right sidebar"></div></div><footer class><hr/><ul style="display:flex;"><li><a href="https://github.com/tinfoil-knight">GitHub</a></li><li><a href="https://twitter.com/machines_fail">Twitter</a></li><li><a href="https://www.linkedin.com/in/kunal-kundu/">LinkedIn</a></li><li style="flex-grow:1;text-align:right;">Built with <a href="https://quartz.jzhao.xyz/">Quartz</a></li></ul></footer></div></body><script type="application/javascript">// quartz/components/scripts/quartz/components/scripts/callout.inline.ts
function toggleCallout() {
  const outerBlock = this.parentElement;
  outerBlock.classList.toggle(`is-collapsed`);
  const collapsed = outerBlock.classList.contains(`is-collapsed`);
  const height = collapsed ? this.scrollHeight : outerBlock.scrollHeight;
  outerBlock.style.maxHeight = height + `px`;
  let current = outerBlock;
  let parent = outerBlock.parentElement;
  while (parent) {
    if (!parent.classList.contains(`callout`)) {
      return;
    }
    const collapsed2 = parent.classList.contains(`is-collapsed`);
    const height2 = collapsed2 ? parent.scrollHeight : parent.scrollHeight + current.scrollHeight;
    parent.style.maxHeight = height2 + `px`;
    current = parent;
    parent = parent.parentElement;
  }
}
function setupCallout() {
  const collapsible = document.getElementsByClassName(
    `callout is-collapsible`
  );
  for (const div of collapsible) {
    const title = div.firstElementChild;
    if (title) {
      title.removeEventListener(`click`, toggleCallout);
      title.addEventListener(`click`, toggleCallout);
      const collapsed = div.classList.contains(`is-collapsed`);
      const height = collapsed ? title.scrollHeight : div.scrollHeight;
      div.style.maxHeight = height + `px`;
    }
  }
}
document.addEventListener(`nav`, setupCallout);
window.addEventListener(`resize`, setupCallout);
</script><script type="module">
          import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
          const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
          mermaid.initialize({
            startOnLoad: false,
            securityLevel: 'loose',
            theme: darkMode ? 'dark' : 'default'
          });
          document.addEventListener('nav', async () => {
            await mermaid.run({
              querySelector: '.mermaid'
            })
          });
          </script><script src="../postscript.js" type="module"></script></html>