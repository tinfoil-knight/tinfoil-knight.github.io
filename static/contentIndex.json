{"index":{"title":"kunal","links":["notes/"],"tags":[],"content":"I’m a software engineer working at (Redacted) on AI native workspaces and reliable infrastructure for GPU-enabled services.\nI’ve been focusing more on things closer to computer systems for the past year.\nI’m studying up on database systems right now. See my notes here.\nI’m always on the lookout for weird book recommendations.\nSee what I’m reading on Goodreads.\nYou can reach out to me at kunal99kundu@gmail.com or on Twitter."},"notes/5-lessons-from-5-years-of-building-databases-at-scale-2024":{"title":"5 Lessons from 5 Years of Building Databases at Scale (2024)","links":[],"tags":["db","talks","systems-distributed-conf"],"content":"Sammy Steele ssteele@figma.com is currently the Senior Staff Engineer and Tech Lead for Figma’s Databases Team.\n\nWorked at Dropbox before.\nWas part of the 5 person team that built Dropbox’s petabyte scale search system - Nautilus.\n\nDropbox operated data-centers on-prem since managed cloud services weren’t as good yet.\nDropbox stored petabyte scale metadata and exabyte scale file data. In 2013, the metadata was stored in horizontally sharded MySQL w/ 2000 shards.\nThey were outgrowing this system. A single organization (MIT) was going to outgrow a single database which their system didn’t account for.\nBeyond Sharded SQL\n\nThey ended up building their own Spanner-inspired distributed key-value store in-house (called Panda) since no open-source solutions were scaling to their usage. One org could now be split multiple times to get smaller shard ranges.\n(Project Theseus) Hardest part of the project wasn’t building Panda but running the migration to re-build the entire metadata stack on top of Panda with zero downtime. It took 3 years and a team of 20 engineers.\nImplemented atomic cross-shard txns. (2PC) across MySQL and Panda to enable the migration. Increased %age rollout of cross-shard txns. over time and eventually swapped reads from the legacy system.\n\nJoined Figma at a time when they were just in the process of scaling out their databases. Back in 2022, Figma was on a single RDS instance. DB Traffic was growing 3x every single year for the last 4 years.\nBuild vs. Buy for Database Scalability\n\nSammy was pretty anti-build after Dropbox where everything was in-house. While building new systems in fun as an infra engineer, someone has to keep maintaining the system.\nFigma was operating on extremely tight time horizons with a small team and 6 months of runway (before the system grew out of scale).\nWhy not NoSQL?\n\nNoSQL does really well at scale but Figma relied a lot on joins, unique indexes and complicated relational models.\nWould have to rewrite the entire system from scratch to use a NoSQL DB.\n\n\nSpent 3 months exploring a NewSQL solution (like Cockroach, Vitess or Spanner) but couldn’t get it to work at their scale.\nTeam had a lot of expertise and historical context in RDS Postgres. Needed an incremental migration path to deliver value fast.\nBuilt in-house horizontally shared-system on top of RDS Postgres.\n\nLogical vs. Physical Sharding\n\nA physical shard could have multiple logical shards.\nCan logically shard in-place w/ no data movement by using a virtual database layer.\nBenefits of logical sharding:\n\nIncremental %age based rollouts\n\nCan use logical shards for reads but let the writes go to the un-sharded database\n\n\nLow-risk fallback path\nDe-risk latency availability, correctness before physical sharding\n\nAll query, txn. logic, application changes that happen with horizontal sharding would already be running in production through the virtual shards.\n\n\n\n\n\n\nFigma doesn’t use micro-services. They’ve a few large services including a service in Go that powers the real-time feature and a no. of Ruby services including a large Ruby monolith.\n\nDBProxy §\n\nThey already run a slightly modified version of pgBouncer as their database proxy. DBProxy is a Go service that sits b/w application and pgBouncer layers.\n“Brains” of horizontal sharding. Application doesn’t need to know how the backend is actually sharded beyond including a shard key in queries.\nAlso enables load-shedding, improved observability, replica hedging etc.\n\nIf a replica goes down, then the request is sent to multiple replicas and returns when the first one succeeds.\nMost critical routes only talk to replicas. Very few of them issue reads.\n\n\nLightweight query engine at its core.\nQuery parser taken from CockroachDB and it has worked well for them.  Logical and physical planner was built in-house.\n\n\n\n(Scatter Gather query) When a shard key isn’t specified, DBProxy aggregates the results from all the shards.\n\nQuery Language\n\nAll single-shard queries are allowed.\nNo nested SQL queries.\nMulti-shard writes, aggregation and group-bys are allowed but discouraged.\nJoins are allowed iff the join is on the sharding key.\n\nPhysical Sharding Operation\n\nBuilt in-house.\nUses Postgres Logical Replication to sync state.\nReplica failovers are a no-downtime operation.\nPrimary failovers take ~10s of downtime during the critical section (In comparison, AWS RDS may take anywhere from minutes to half an hour for major upgrades).\n\n\n\nMulti-step state machine w/ automatic rollback and recovery\n\nDisable reads at pgBouncer (PGB) layer\nRevoke permissions from source RDS\nKill active queries\n\nSince revoking permissions doesn’t stop in-flight queries\n\n\nWait for replication sync\nReverse logical replication\n\nWas logically replicating from old to new. Now replicating from source back to destination (for rollback).\n\n\nRe-route pgBouncer instances to point to target\nRe-enable permissions on target RDS\nRe-enable pgBouncer\n\n\nCore failover operation enables vertical partitoning, horizontal shard-splits and major pg version upgrades.\n\n\nLessons Learned §\n\nYou can get a long way on one database… Until you can’t\n\nFigma was on a single pg instance from 2012 to 2020 w/o connection pooling or replicas.\nVery few companies hit the scale where vanilla pg stops working.\n\n\nDon’t be afraid to cross abstraction boundaries\n\nEngineers often get stuck behind abstractions.\nIf something isn’t working, go read the code yourself. Applies to orgs too. Go talk to product engineers, read product code and review product designs.\nFor example, we had issues with Postgres vacuuming. Txn. wrap-around occurred regularly since pg uses a 32-bit integer for txn ids (i.e. 2^32 ~ 4 billion txn. ids). Vacuuming is very expensive and they observed that their servers would hit 100% CPU at the end of a vacuum. They had to read the pg source code to understand why this was happening and find a workaround. (There was a single lock that’s being contended on that read the entire file system into memory because query plans have been invalidated at the very end of a vacuum or analyze operation.)\n\n\nData migrations are scary, so work twice as hard to make them safe.\n\nDo data migrations with the gradual, %age based rollouts and have the ability to rollback from unknowns.\n\n\nKnow your customers and product\n\nUnderstand how infra tradeoffs map to your organization’s priorities and ensure they align.\nThe databases team had to become experts in Figma’s product to ship horizontal sharding.\nDBProxy does txns. by creating them on every database, issue a SELECT before commit to ensure that the DBs are still alive and then you commit in parallel. There is a span of 5ms where the DB could die and you could have a partial commit failure. At their commit volume, it’d only happen 2-3 times a year so it wasn’t a problem to worry about except for some critical txns. which were fixed.\n\n\nCertain early company decisions are very hard to walk back\n\nYour data model, your ORM, and your consistency model are extremely hard to change.\n\nAt Dropbox, they had perfect consistency from day one and folks got used to that. Part of why the project took 3 years was that we couldn’t take that guarantee away from customers and had to keep giving them perfect atomic cross-shard txns.\nAt Figma, they gave replicas with nothing but a stale reads guarantee so all the product code was written to deal with that from day one.\n\n\nThere is implicit tension b/w the goal of building products fast in a company’s early days vs. needing to care about scale and reliability once you’re successful. Choose your battles wisely.\n\n\nCentralized on less technologies done well\n\nGolden paths enables small infra teams to support large product orgs\nOptimize for the good of the company over individual team’s desires\nMore technologies add complexity, reduces reliability and increases developer costs\nDon’t gate-keep if you’re blocking the core business\n\n\n\nReferences\n\nFuture-proofing our metadata stack with Panda, a scalable key-value store (2022)\nThe growing pains of database architecture (2023)\nHow Figma’s databases team lived to tell the scale (2024)\n"},"notes/building-koi-pond---simulating-millions-of-slack-clients-2023":{"title":"Building Koi Pond - Simulating Millions of Slack Clients (2023)","links":[],"tags":["talks"],"content":"Maude Lemaire is a Sr. Staff Software Engineer @ Slack and technical lead for the backend performance infrastructure team.\nSlack didn’t have any load testing tooling. They had a big customer that was putting strain on their infra.\nThe initial tool (API Blast) just made API requests to the server. It had a few parameters for specifying concurrency, rate limits etc. but that’s it.\n\nWhy did they not use off the shelf tool which already provide these functionalities?\n\nAPI Blast didn’t test the web-socket stack. The only thing that was tested is ingest. Message propagation wasn’t tested since there were no incoming clients.\n\n\nEdge API is for serving data cached in various Points-of-Presence around the world &amp; required instantaneously. (Like the quick switcher, search bar that comes up on  ⌘ + K)\nReal time services maintain all active web-socket connections with all users around the world. It organizes those connections by channels.\n\nThis worked for a while but a newer customer wanted a channel to house all 300K users where 100K users are likely to be active at the same time.\nSlack is susceptible to load-related performance problems in 3 key ways:\n\nMassive fan-out (users sending messages in big channels )\nEvent floods\nThundering herd\n\nEg: If someone posts a message that goes to 100K active users and even 10 users send 1 reaction, those reactions need to be propagated to 100K clients which leads to a 1 million web-socket events.\n\n\n\nIn mid-2019, they built a tool called Puppet Show.\n\nSimulated real desktop users by spinning up 1000s of headless chrome browsers logged into Slack (distinct users, distinct token) across a K8s cluster.\nThey had a script simulate different actions like switching to a channel &amp; posting a message, switching to another &amp; adding a reaction, trolling Slackbot etc.\nA central component (Puppeteer) oversaw all the puppets. The puppets would check in regularly to update the puppeteer about their state. Puppets would receive a script from Puppeteer and start executing it.\n\nSidenote: Don’t confuse Puppeteer with the Node.js library used to control Chromium.\n\n\nPros\n\nHigh fidelity. Nothing better than logging into the slack client and executing actions.\nFlexible scripting using Javascript\n\n\nCons\n\nCosts a lot. For each puppet instance, the cost was 37 cents per day. Running 100K instances would cost 37K USD everyday.\nSpinning up 100K instances took several days and pods would crash frequently.\n\n\nOnce it was verified that Slack can handle the load, they stopped using this tool.\n\nThey signed up a customer (in 2020, around the pandemic) that wanted support for 500K (IBM probably) users in the same Slack instance.\nThe headless chrome browsers were replaced w/ lightweight client simulators written in Go.\n\nA koi is a Slack client simulation (single Go routine)\nA school is a collection of koi (single Go program that runs in a single pod on K8s)\nThe keeper manages the schools and keeps track of the overall load test state and parameters.\n\nA JSON configuration file needs to be provided when you boot up a load test that tells a “koi” what to do once booted.\nEach action is mapped to a probability of being performed. Then there’s a set of implementation w/ each action that the koi should perform with each of those actions.\n{\n    &quot;behaviors&quot;: {\n        &quot;chat.postMessage&quot;: {\n            &quot;frequency&quot;: 0.043\n        }\n    },\n    &quot;sequences&quot;: {\n        &quot;chat.postMessage&quot;: {\n            &quot;doc&quot;: &quot;Sends a message to a random channel.&quot;,\n            &quot;steps&quot;: [\n                ...\n            ]\n        }\n    }\n}\n\nEvery tick (configurable but 1s by default), a koi runs through its entire configuration and performs the actions based on their odds.\nThis was good enough to simulate massive fan-out and event floods but not thundering herds since they couldn’t simulate coordinated behavior. That’s why they have “formations” that allows specifying the percentage of users participating over a period of time.\n{\n    &quot;formations&quot;: [\n        {\n            &quot;name&quot;: &quot;Populate announcement channel with reactions&quot;,\n            &quot;begin_within_secs&quot;: 30,\n            &quot;percent&quot;: 1.0,\n            &quot;sequence&quot;: {\n                &quot;steps&quot;: [\n                    ...\n                ]\n            }\n        }\n    ]\n}\n\nA koi cost 0.1 cents to run per day.\nAppendix §\n\nSlides: https://speakerdeck.com/qcmaude/building-koi-pond-simulating-millions-of-slack-clients\nhttps://slack.engineering/%EF%B8%8F-surfs-up-preparing-for-huge-waves-of-traffic-via-load-testing/\nhttps://slack.engineering/load-testing-with-koi-pond/\nhttps://slack.engineering/continuous-load-testing/\n"},"notes/cmu-intro-to-database-systems-2022":{"title":"CMU Intro to Database Systems (2022)","links":[],"tags":["db"],"content":"Lectures: https://15445.courses.cs.cmu.edu/fall2022/schedule.html\nAssignments: https://15445.courses.cs.cmu.edu/spring2023/assignments.html\n1 : Relational Model &amp; Relational Algebra §\nInventor of Relational Model : Edgar Frank “Ted” Codd at IBM\n\n\nA Relational Model of Data for Large Shared Data Banks\n\n\nDerivability, Redundancy and Consistency of Relations stored in Large Data Banks\n\n\nRelation / Table : Unordered set containing relationship of attributes that represent an entity.\n\n\nTuple / Row / Record : Set of attribute values (domain) in the relation.\n\n\nPrimary key identifies a single tuple.\n\n\nData Manipulation Languages (DML)\n\nProcedural : Relational Algebra\nDeclarative (like SQL) : Relational Calculus\n\n\n\nRelational Algebra\n\nBased on Sets\nBasic Operators:  Select, Projection, Union, Intersection, Difference, Product, Join\nProjection: Generate relation with tuples that only contain the specified attributes. Can rearrange ordering of attributes &amp; can manipulate the values.\n\nFun Fact: SQL was Sequel (Structured English Query Language) initially but IBM had to change the name after they got sued.\n2 : Modern SQL §\n\nSQL is a combination of\n\nData Manipulation Language (DML) : retrieve or modify data\nData Definition Language (DDL) : specify what data looks like, indexes, namespaces, triggers, functions\nData Control Language (DCL) : access control\n\n\nSQL is not based on sets (no duplicates) but on bags (can have duplicates).\n\n3 : Database Storage 1 §\n\n\nSequential access can be faster than random access\n\nCase in point: Think about getting 1MB of data that is contiguously laid out\nSometimes algorithms might seem slow on paper but we want to optimise for sequential access in DBMS.\n\n\n\n“Memory” in this course will refer to DRAM.\n\n\nDisk Oriented DBMS\n\nDisk: Blocks/Pages\nMemory: Buffer Pool / Page Cache / Buffer Manager\n\n\n\nWe don’t want to rely on the OS to do anything apart from giving the DBMS memory and not letting the process get killed.\n\n\nmmap §\n\nmmap can store contents of a file into the address space of a program but it has some issues.\n\nTransaction Safety : OS can flush dirty pages at any time (you might not want to save changes yet if you’re updating multiple pages but the OS will flush the pages and lead to corrupt data).\nI/O Stalls :  DBMS doesn’t know which pages are in memory so it’ll get a page fault every time it tries to access a page not in memory &amp; be stalled by the OS then.\nError Handling : Difficult to validate pages (DBMS can maintain checksums if it is controlling read, writes unlike mmap).\nPerformance Issues : OS has it’s own data structure for page table which can become a bottleneck.\n\n\nmmap might be okay for really light workloads on small DBs. It’s quick and easy to start with.\nSidenote: mmap is used by elasticsearch, leveldb ; sqlite also partially uses mmap ; mmap was partially used by mongodb earlier but they stopped using it since they faced problems.\nFor more info on why you shouldn’t use mmap for a DBMS: Read https://db.cs.cmu.edu/mmap-cidr2022/\n\nHow the DBMS represents the database in files on disk §\n\nFile Storage\n\nA file is a collection of pages.\n\nPage is a fixed-size block of data.\nPage can also have metadata about itself.\n\n\nDifferent notions of “pages” in a DBMS\n\nHardware : usually 4KB\nOS : usually 4KB\nDatabase : 512B - 16KB\n\n\nA hardware page is the largest block of data that the storage device can guarantee failsafe writes.\nWe want to keep OS pages larger to make less syscalls for read/writes. We also don’t want to enlarge it too much since then we’ll be fetching unnecessary data.\n\n\nPage Storage Architecture\n\nHeap\n\nunordered collection of pages\nsupport create, get, write, delete, iteration over pages\npage directory are special pages which track location of data pages in DB files\na data pages being self-contained (having metadata about it’s location, table etc.) can help in data recovery if something happens to page directory\n\n\nTree\nSequential / Sorted\nHashing\n\n\nSome reasons for storing things in small files\n\neasier to obtain a lock on specific data\nreduces the blast radius if a single file gets corrupted or deleted\nwe can symlink particular directories on a faster disk for faster access\n\n\nPage Layout\n\nLog Oriented\nTuple Oriented\n\nStrawman Idea: Keep track of number of tuples in a page and then just append a new tuple to end.\n\nIssues\n\nDeleting a tuple\n\nIf a tuple is deleted from between, there’s no way to know that free space is available there for putting a tuple in\n\n\nVariable-length attribute\n\nAllocating empty space is wasteful\n\n\n\n\n\n\nSlotted Pages\n\nSlot Array in the beginning of the page maps to the starting positions of the tuples. (Also contains tuple size)\nSlot Array grows from beginning to end. Tuple data grows from end to beginning.\nWe consider the page full when we can’t enter any more slots or tuples.\nHeader keeps track of number of used slots.\nYou can readjust tuple locations if you want to on deletion to reduce fragmentation in a particular page.\nYou can use other compaction processes to reduce the amount of space required.\n\n\nRecord Ids : DBMS needs a way to keep track of individual tuples. Each tuple has unique record id.\n\nsimple method: page_id + offset/slot\ncan also contain file location info\nalso: see ctid in postgres (you can query it)\n\nVACCUM in pg would does the re-adjustment of deleted rows\nsqlite has rowid\n\n\npg’s auto-vaccum keeps tracks of pages changes since last Vaccum &amp; then compacts only those pages\n\npages can also be merged together if they’re half-full\n\n\n\n\n\n\n\n\nTuple Layout\n\nTuples also have a header which can store visibility info &amp; bit map for null values in the data attributes.\nFor variable length data, you can also have pointers to some other location stored in the tuple.\n\n\nMisc\n\nPostgres does MVCC the wrong way:\n\nTo Read: https://db.cs.cmu.edu/papers/2017/p781-wu.pdf\n\n“An Empirical Evaluation of In-Memory Multi-Version Concurrency Control” by  Andrew Pavlo\n\n\n\n\nQuotes from Chapter 12 (12.1 - 12.4)\n\n“Requests for disk I/O are typically generated by the file system but can be generated directly by the database system. Each request specifies the address on the disk to be referenced; that address is in the form of a block number. A disk block is a logical unit of storage allocation and retrieval, and block sizes today typically range from 4 to 16 kilobytes. Data are transferred between disk and main memory in units of blocks. The term page is often used to refer to blocks”\n“In a sequential access pattern, successive requests are for successive block numbers, which are on the same track, or on adjacent tracks. To read blocks in sequential access, a disk seek may be required for the first block, but successive requests would either not require a seek, or require a seek to an adjacent track, which is faster than a seek to a track that is farther away.”\n“In contrast, in a random access pattern, successive requests are for blocks that are randomly located on disk. Each such request would require a seek.”\nI/O operations per second (IOPS) : “number of random block accesses that can be satisfied by a disk in a second”\n“Compared to magnetic disks, SSDs can provide much faster random access: the latency to retrieve a page of data ranges from 20 to 100 mi- croseconds for SSDs, whereas a random access on disk would take 5 to 10 milliseconds.”\n\n\nTODO: Read about RAID in Chapter 12\n\n\n\n4 : Database Storage 2 §\n\nLast Lecture: Disk Oriented DBMS -&gt; Page Oriented Architecture\nIssues with Slotted Page Design\n\nFragmentation (may not get entire tuple in a single page, may need to update multiple pages)\nUseless Disk I/O (page may have data we don’t care about which we’ll need to read since we can’t get exact bytes)\nRandom Disk I/O (eg: tuples that need to updated may reside on different pages)\nNote to Remember: Sequential I/O is great for non-volatile storage compared to random access.\n\n\n\nLog Structured Storage §\n\nWhat if DBMS couldn’t overwrite data in pages &amp; could only create new pages?\n\nEg: AWS S3, HDFS\n\n\n(Think of just a key-value store here, PUT contains the entire value that needs to be stored)\nDBMS stores log records that contain changes to tuples (PUT, DELETE).\n\nEach log record must contain the tuple’s unique id.\nPut records contain the tuple contents.\nDeletes marks the tuple as deleted.\n\n\nWhen the page gets full, the DBMS writes it from in-memory page out to disk.\nWrites are batched (in-memory page is fast, updating 20 tuples at once just involves appending the changes to log now)\nGetting a tuple with some Id\n\nScanning log from newest to oldest is slow\nMaintain an index mapping tuple id to newest log record for that id\n\n\nCompaction\n\nMerging pages to reduce the amount of space taken up by actions on the same tuple id.\nTODO: Write a simple script that merges two files with data in the format: ACTION ID Value(if applicable)\nAfter a page is compacted, the DBMS doesn’t need to maintain temporal ordering of records within the page since each tuple id appears at most once in the page.\n(We don’t need to track deletes post compaction, could just remove the key from index)\nDBMS can sort page by id to improve lookup. See SSTables.\nTypes of Compaction\n\nUniversal : combine 2 contiguous sorted files\nLevel : Files of same “level” are compacted one by one as they move up each level. (used by LevelDB &amp; it’s fork RocksDB)\n\n\n\n\nEg: RocksDB, LevelDB, Apache HBASE, Fauna, AWS Aurora etc.\nDownsides\n\nwrite-amplification : same record is getting compacted multiple times even if it doesn’t change\ncompaction is expensive\n\n\nTuple Storage\n\nDBMS catalogs contain schema info about tables that is used to figure out tuple’s layout.\nData Representation\n\nCommon Types: Integers, Floats or Numeric/Decimal, Varchar or Blob, Time\nVariable Precision Numbers, Fixed Precision Numbers\nHandling fixed precision numbers is slow\nLarge values (like Varchar) that are larger than a page use separate overflow storage pages. Fields in tuple point to these overflow pages instead of storing the data in the tuple itself.\nNumber of columns is also limited since a tuple can’t exceed the size of the page in most DBMS.\nSome systems allow storing really large value in external files. DBMS can’t update the contents of these files.\n\n\nPaper Suggestion: ‘To Blob or Not to Blob: Large Object Storage in a Database or a Filesystem” by Jim Gray\n\n\n\nQuotes from Chapter 24 (24.2) §\n\n“B+-tree indices are not efficient for workloads with a very high number of writes” Why?\n“The key idea of the log-structured merge tree (LSM tree) is to replace random I/O operations during tree inserts, updates, and deletes with a smaller number of sequen- tial I/O operations.”\nIndex Inserts &amp; Lookups\n\n“An LSM tree consists of several B+-trees, starting with an in-memory tree, called L0, and on-disk trees L1, L2, … , Lk for some k, where k is called the level.”\n“An index lookup is performed by using separate lookup operations on each of the trees L0, … , Lk, and merging the results of the lookups.”\n“When a record is first inserted into an LSM tree, it is inserted into the in-memory B+- tree structure L0. A fairly large amount of memory space is allocated for this tree.” How much space though?\n“As the tree grows to fill the memory allocated to it, we need to move data from the in-memory structure to a B+-tree on disk.”\n\nIf L1 is empty the entire in-memory tree L0 is written to disk to create the initial tree L1.\nIf L1 is not empty, the leaf level of L0 is scanned in increasing key order, and entries are merged with the leaf level entries of L1. The merged entries are used to create a new B+-tree using the bottom-up build process. The new tree with the merged entries then replaces the old L1.\nAll entries in the leaf level of the old L1 tree, including those in leaf nodes that do not have any updates, are copied to the new tree instead of being inserted into the existing L1 tree node.\n\nThe leaves of the new tree are sequentially located, avoiding random I/O during subsequent merges.\nThe leaves are full, avoiding the overhead of partially occupied leaves that can occur with page splits.\n\n\n\n\n“Cost to using the LSM structure: the entire contents of the tree are copied each time a set of entries from L0 are copied into L1.”\n“To ensure we get a benefit for cases where the index size on disk is much bigger than the in-memory index, the maximum size of L1 is chosen as k times the target size of L0, for some k. Similarly, the maximum size of each Li+1 is set to k times the target size of Li. Once a particular Li reaches its maximum size, its entries are merged into the next component Li+1. When Li+1 reaches its target size, its entries are in turn merged into Li+2, and so on.”\n“We assumed for simplicity that when a particular level is full, its entries are entirely merged with the next level. This would result in more I/O load during merges with an unused I/O capacity between merges. To avoid this problem, merging is done on a continuous basis; this is called rolling merge. With rolling merge, a few pages of Li are merged into corresponding pages of Li+1 at a time, and removed from Li.”\n\n\n\n5 : Storage Models &amp; Compression §\n\nDB Workloads\n\nOLTP\nOLAP\nHybrid (OLTP + OLAP) (also called HTAP)\n\n\nRow OR n-ary storage model (NSM) : Data pertaining to a specific record is packed together on a page\n\nGood for OLTP workloads since we usually require a lot of attributes at once\nBad for OLAP workloads since we’d need to check every page even for an analysis on a single attribute\n\n\nColumn OR Decomposition Storage Model (DSM) : The values for an attribute are stored together one after another.\n\nTuple Identification\n\nFixed-length Offsets (used by most systems)\nEmbedded Tuple Ids\n\nOffsets for each column are stored for a tuple id in a lookup table.\n\n\n\n\nDBMS only reads the data it needs.\nSlow for point queries, inserts, updates, deletes because of tuple splitting / stitching.\n\n\nVertica (a column store) is a fork of Postgres rewritten as a column storage system.\nSybase (an older system made in the 90s) was an in-memory column store working on-top of a row store for faster query results.\nI/O is the primary bottleneck in fetching data. Compressing the DB reduces DRAM requirement. It may decrease CPU costs during query exec.\nThere can be many repeated values in a single DB attribute.\nGoals for DB Compression\n\nProduce fixed-length values. (Variable length data can be stored in separate pool)\nPostpone decompression for as long as possible during query exec (late materialization)\nMust be lossless\n\n\nCompression Granularity\n\nBlock\nTuple (NSM only)\nAttribute\nColumn (DSM only)\n\n\n\nNaive Compression §\n\nDB doesn’t know about the data\nThings to consider: Computation overhead, Compress vs. Decompress speed\nMySQL InnoDB Compression\n\nDefault size is 16KB\nPages are compressed &amp; then padded to nearest power of 2 (1, 2, 4, 8 KB)\nPage goes to buffer pool when it needs to be used\nUpdate queries won’t necessarily need to know the previous value. If an update just appends the value to the page it can be put after the compressed data in the page &amp; metadata can store offsets for compressed &amp; non-compressed data.\n\n\nDBMS must decompress data first before reading &amp; (potentially) modifying it. Naive schemes also don’t consider high-level semantics of the data.\n\nColumnar Compression §\n\nRun-length Encoding\n\ndata to be sorted for max compression\nvalue -&gt; (value, start offset, # of elements in the run)\nworks for highly categorical data (eg: list of values with genders as M/F/Other)\n\n\nBit-packing\n\nstore value as smaller data type (eg: int64 -&gt; int8) if the value doesn’t need the full size\nDBs handle the conversion during aggregations. TODO: check in detail\nA marker is stored for larger values (eg: larger than int8) which maps to a separate lookup table.\n\n\nBitmap\n\nIf number of distinct values is low (i.e. value cardinality is low), each unique value can be stored as a bitmap\ni-th position in bitmap corresponds to i-th tuple\n\n\nDelta\n\nInstead of storing all values, record the difference in values\nCan combine w/ RLE to get better compression ratios if repetition is likely\nEg: temperature stored as 99, +1, +1 instead of being stored as actual values 99, 100, 101\n\n\nIncremental\n\nType of delta compression that avoids storing common prefix / suffix b/w consecutive tuple.\nEg: rob, robbed, robbing, robot -&gt; rob, 3bed, 4ing, 3ot (common prefix: “rob” -&gt; “robb” -&gt; “rob”)\n\n\nDictionary\n- Map variable length values to a small int identifier\n- Lookups for data can use the compressed data instead of the actual value if the DB is aware. For eg: If a string “Andrea” is mapped to an int “30” then the DB can look for that value without decompressing the data.\n- This isn’t hashing since the dict needs to support both encoding, decoding.\n- Order Preserving Encoding (sort and assign keys basis sorted order) for queries like “find greater than some value”, “find value that starts with xyz”\n- How would we handle new values?\n- We don’t do this in OLTP due to the large number of separate inserts which would make the sorted order useless.\n- Cleanup &amp; regeneration can happen in OLAP (still expensive)\n- IDEA: What if we use floats and store the new values at half of 2 values in a row? (We’d eventually run out, larger float types also take a lot of space, plus they aren’t precise)\n\nTODO: Compress the works of Shakespeare (publicly available) from scratch. Compare with production algorithms.\n\n\n\n6 : Memory Management §\n\nHow the DBMS manages its memory and moves data back &amp; forth from disk?\nGoal: Build DB that can store more data than memory available to it.\nSpatial Control (layout of data on device), Temporal Control (when to read from, write data to disk)\n\nBuffer Pool Manager §\n\nAlso knows by : Buffer Manager / Page Cache / Buffer Cache\nMemory regions organised as array of fixed size pages. Each entry in the array is called frame.\nWhen DBMS requests a page, an exact copy is placed into one of these frames.\nDirty pages are buffered &amp; not written to disk immediately. (Write back cache)\n\nDirty Page : it is different from its corresponding page on disk.\n\n\nPage table keeps track of pages currently in memory along with metadata like dirty flag, pin/reference counter for each page.\n\nDirty flag tells if the page has been modified by a query &amp; not saved to disk yet.\n\nSidenote: DB waits for entry to be written to the log page before it makes changes in the table page.\n\n\nPin prevents buffer pool from evicting the page when you’re running a query that’ll need the page. Pin counter tracks how many queries are using the particular page.\nLatch can be acquired on page table for a page that’ll prevent other thread from using the same page &amp; overwriting it.\n\nLock Vs Latch\n\nLock\n\nprotects DB’s logical contents from other transactions\nheld for transaction duration\nneed to be able to rollback changes\n\n\nLatch\n\nprotects critical sections of DB’s internal data structure from other threads\nheld for operation duration\ndon’t need to be able to rollback changes\nmutex\n\n\n\n\n\n\nWhy not use a hashmap instead of maintaining 2 tables?\n\nProf mentions that the page table is like a hashtable only.\n\n\n\n\nDo we lose the contents of buffer pool on crash? Yes\n\nProf mentions that we want to do this for correctness reasons. Why?\n\n\nPage Table Vs Page Directory\n\nPage directory is the mapping from page ids to page locations in the DB files. (on disk)\nPage table is the mapping from page ids to a copy of the page in buffer pool frames. (in memory)\n\n\nAllocation Policies\n\nGoal: Evicting, Pre-fetching, Writing optimally\nGlobal Policies\nLocal Policies\n\n\n\nBuffer Pool Optimisations §\n\nMultiple Buffer Pools (per DB, table, index and so on)\n\nalso improves latch contention\napproaches:\n\nmaintain an object id that points to specific buffer pool\nuse hashing to select the buffer pool\n\n\n\n\nPre Fetching\n\nSequential Scans\nIndex Scans\n\n\nScan Sharing / Synchronised Scans\n\ndifferent from result caching\nallow multiple queries to attach to a single cursor that scans a table\n\nqueries don’t need to be the same\ncan also share partial results\n\n\n\n\nBuffer Pool Bypass (using private buffer pool)\n\navoids overhead of using the buffer pool (like acquiring latch, getting frames etc.)\nmemory is local to running query (can’t be shared across queries)\nworks well if operator needs to read large sequence of  contiguous pages on disk\ncan also be used for temporary data\n\n\nOS Page Cache (specific to Postgres)\n\nOS maintains its own filesystem cache (aka page or buffer cache)\nMost DBMS use direct I/O (using O_DIRECT) to bypass OS cache.\n\nprevents redundant copies\nOS has different eviction policies\nloss of control over file I/O\n\n\nSidenote: Amazon’s pg fork (Aurora) doesn’t use OS’s page cache.\nEXPLAIN (BUFFER) (sql statement here)\npg_prewarm\n\n\n\nBuffer Replacement Policies §\n\nWhich page to evict from buffer pool when freeing up frames?\nLRU\nClock (like LRU)\n\npages organised in circular buffer\neach page has a reference bit which is set to 1 when page is accessed\nwhen the clock hand moves to the next page\n\nif ref bit 0 then evict\nif ref bit 1 then set to 0\n\n\n\n\nIssue with LRU, Clock\n\nSusceptible to sequential flooding (query does sequential scan which pollutes the buffer pool with pages that might not be read again)\n\n\nLRU-K\n\ntracks history of last K refs to each page as timestamps &amp; compute the interval b/w subsequent access\nhistory is used to estimate the next time page is going to be accessed &amp; eviction is done basis that\n\n\nPriority Hints\n\nDBMS can provided hints on whether page is important or not to buffer pool\n\n\nDirty Pages\n\nin the buffer pool, if a page\n\nisn’t dirty: drop it (fast path but might need this page soon)\nis dirty: need to write back to disk (slow path &amp; might not need page in near future)\nwhich to drop? tradeoff b/w fast eviction vs. dirty writing page not needed in future\nDBMS can periodically walk through the page table and write dirty pages to disk to optimise this\n\n\n\n\nOther Memory Pools\n\nfor sorting+join buffer, query cache, maintenance buffer, log buffer, dictionary cache\nmight not be backed by disk\n\n\n\n7 : Hash Tables §\n\nCourse Progress (bottoms-up)\n\nDisk Manager -&gt; Buffer Pool Manager -&gt; Access Methods -&gt; Operator Execution -&gt; Query Planning\n\n\nHow to support DBMS’s execution engine to read/write data from pages?\nData Structures: Hash Tables, Trees\nUsage of Data Structures: Internal Meta-data, Core Data Storage, Temp. Data Structures, Table Indexes\nDesign Decisions: Data Organization (in pages / memory), Concurrency\nHash Table: unordered. associative array mapping key to value, uses hash function to compute offset into this array for given key\n\nSpace Complexity: O(n), Time Complexity: Average: O(1), Worst: O(n)\nNote: the position in the array points to some external storage area with the actual keys\n\n\nSimple Hash Table\n\nallocate giant array that has 1 slot for every element you need to store\nissues\n\nwe need to know the no. of key ahead of time\nhash function needs to be perfect i.e. for no key1, key2: hash(key1) != hash(key2)\nwe have to guarantee key uniqueness\n\n\n\n\nDesign Decisions\n\nHash Function: map a large key space into a smaller domain\n\ntrade-off: being fast vs. collision rate\n\n\nHashing Scheme: handle key collisions after hashing\n\ntrade-off: allocating a large hash table vs. additional instructions for update/retrieval of keys\n\n\nBest currently as per Andy: xxHash (used in a lot of DBs)\n\n\nHash Functions\n\nfor any input key, return an integer representation of that key\nwhy not use SHA or other crypto hash functions? we don’t have security concerns since these are keys in an internal data structure, the extra overhead &amp; loss of performance isn’t worth it\n\n\n\nStatic Hashing Schemes §\n\nLinear Probe / Open Address\n\ngiant table of slots\nresolves collisions by linearly searching for the next free slot in the table\nInsertion\n\njump to next free slot, loop around if at end\nnote: lookup stop when you either find the key, loop back to start of search, or find out the location and you know it can’t be there (how??)\n\n\nDeletes (handling what happens when you delete a key from a lot)\n\nMovement: slide keys up after rehashing (costly)\nTombstone: set a marker to indicate that the entry in the slot is logically deleted\n\nslot can be re-used for new keys\nmay need periodic garbage collection\n\n\n\n\nNon Unique Keys (eg. usage: using hashtable for joins)\n\nSeparate Linked List : store values in separate storage area for each key\nRedundant Keys: store duplicate keys entries together in the hash table\n\neasier to implement so this is what most systems do\ncheck is key exists: find first key\ndeletion: delete all keys\n\n\n\n\nSidenote: name of column and record id together can be used to guarantee uniqueness of keys\n\n\nRobin Hood Hashing\n\nstealing slots from “rich” keys and giving them to “poor keys”\ninsertion: a key takes slot from another if the first is farther from its optimal position than the latter\n\neach key tracks the no. of positions they’re from their optimal position in the table\n\n\nthe number of “shifts” from ideal position determines how poor the key is\n\na key inserted at ideal position has a counter of 0\n\nsimilarly a key that had to shift twice has the counter of 2\nmore the counter, poorer the key is\nif the key being inserted has a larger counter than key already in place then the key already in place is shifted\ntradeoff: lots of shuffling can occur on insertion but this can make reads slightly faster | linear hashing is faster and easier for most cases\n\n\n\n\n\n\nCuckoo Hashing\n\nused in a lot of systems\nuse multiple hash tables with different hash function seeds\ninsertion: check every table &amp; pick anyone that has a free slot\n\nif no table has a free slot, evict element from one of them and then re-hash it find a new location\nyou need to keep track that you’re not in a cycle\n\n\nGood OSS implementation: https://github.com/efficient/libcuckoo\nWhat happens if you can’t find a new slot using any of the available hash functions? Allocate a new table 2x the size of previous. Re-insert all keys in new hash table.\n(A new hash function might work too but implementations don’t do this)\nWhy is it not considered dynamic if it can allocate a new table?\n\nThe process is not incremental (eg: think of re-allocating an array in C to insert more elements)\n\n\n\n\n\nDynamic Hash Tables §\n\nStatic hashing schemes require the DBMS to know the number of elements it wants to store\n\nTable needs to be rebuilt on growth/shrinkage in size\n\n\nResize on demand without complete re-allocation\nChained Hashing\n\nlinked list of buckets for each slot in hash table\nresolve collisions by placing all elements with the same hash key into the same bucket\nnew bucket is allocated when a bucket is full\n\n\nExtendible Hashing\n\nsplit buckets instead of letting the linked list grow forever\nmultiple slot locations can point to the same location\nreshuffle bucket entries on split and increase the no. of bits to examine\neg: no. of bits increasing : 00, 01, 10, 11 -&gt; 000, 010, 100, 110, 001, 011, 101, 111\n\nexample of hashed keys: 011100, 10111, 10110\n\n\n\n\nLinear Hashing (slightly confusing, read about it somewhere else)\n\nhash table maintains a pointer that tracks the next bucket to split\n\nwhen any bucket overflows, split the bucket at the pointer location (not the bucket that overflowed) (in preparation for future)\n\nthe overflowed bucket is extended\n\n\n\n\nuse multiple hashes to find the right bucket for a given key\nnew hash function is used at/below the split pointer\nsplitting buckets based on the split pointer will eventually get to all overflowed buckets\nwhen the ptr reaches the last slot, delete the first hash function and move back to beginning\neg:\n\nhash functions: key % n, key % 2n, key % 4n\nif you started out with 4 bucket lists, you’d have 8 bucket lists after splitting (slide has 5 because of no space)\n\n\nNote: Deletion was skipped in lecture. See slides or notes.\n\n\nSidenote: Hash table isn’t what you want to use for a table index. Why?\n\n8 : B+ Tree Index §\n\nTable indexes mainly use B+ trees\nTable index: replica of a subset of a table’s attributes that are organized and/or sorted for efficient access\nTrade-off for no. of indexes to create per DB: storage &amp; maintenance overhead\nIn OLAP DBs: indexes are primarily used for speeding up joins in smaller tables\nB-tree family: B-Tree, B+ Tree, B* Tree, Blink-Tree\npg uses B+ trees, much like other DBs\nNote: A slightly modified kind of B+ tree structure is discussed. (Not the original one from the paper)\nB+ tree is self-balancing tree, keeps data sorted\nOperations: Search, Sequential Access, Insertion, Deletion in O(logn)\nOptimized for systems that read &amp; write large blocks of data\nNode can have &gt; 2 children\nM-way search tree??\nProperties\n\nPerfectly balanced (every lead node is at the same depth in a tree)\nEvery node other than the root is at least half-full\n\nM/2 - 1 &lt;= #keys &lt;= M-1\n\n\nEvery inner node with k keys has k+1 non-null children\nSome implementations can be slightly different. Doesn’t matter as long as the operations are O(logn)\n\n\nB+ tree node comprises of an array of key/value pairs.\n\nKeys are derived from attribute(s) that the index is based on.\nThe values will differ based on whether the node is classified as “inner node” or “leaf node”.\nThe arrays are usually? kept in sorted order.\nInner node can have a key that’s deleted since they act like guide posts. Leat nodes can’t have deleted keys.\n\n\nLeaf Node Values\n\nApproach 1 (Record IDs) : a ptr. to the location of the tuple to which the index entry corresponds\n\nextra lookup\ndone by: pg, sql server, db2, oracle\n\n\nApproach 2 (Tuple Data): leaf nodes store actual content of the tuple\n\nextra overhead: secondary indexes must store the record IDs as their values\ndone by: sqlite, sql server, mysql, oracle\n\n\n\n\nB-tree vs. B+ tree\n\nB-tree stores keys &amp; values in all nodes in tree\n\nspace efficient (since each key only appears once)\nrange scans are expensive compared to B+ tree since leaf nodes are sequential\n\n\n\n\nB+ Tree\n\nSee https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html\nMax Degree = Max Keys Per Node + 1\nOperations: Insert, Delete, Lookup\nDBMS can use a B+ tree index if the query provides any of the attributes of the search key\n\nEg: Index on &lt;a,b,c&gt;\n\nConjunction: (a = 1 AND b = 2 AND c = 3)\nPartial: (a = 1 AND b = 2)\nSuffix: (b = 2), (c = 3)\nFind (A, B) ; (A, *), (*, A)\n\n\nEg: Index on &lt;col1, col2, col3&gt;\n\nColumn Values: {A, B, C, D}\nSearch: col2 = B;\n\n\n\n\nDuplicate Key Handling\n\nApproach 1 : Append Record ID\n\nAdd tuple’s unique Record ID as part of the key to ensure that all keys are unique\nDBMS can use partial keys to find tuples.\npg does this\n\n\nApproach 2 : Overflow Leaf Nodes\n\nLet leaf nodes to spill into overflow nodes that contain the duplicate keys.\nComplex to maintain.\n\n\n\n\nClustered Indexes\n\ntable is stored in order specified by the primary key\nsome DBs always use a clustered index while others can’t use them at all\npg doesn’t maintain the clustered index once done\nmysql maintains clustered indexes automatically for primary keys (why can it?)\n\n\nYou can specify BTREE or HASH in pg while creating the index\n\n\nRecommended Book on BTrees: Modern BTree Techniques (Goetz Graefe)\nDesign Decisions while Implementing BTrees\n\nNode Size\n\nas per research: slower the storage device, larger the optimal node size for B+ Tree\n\nHDD: ~1MB, SSD: ~10KB, In-Memory: ~512B\nwe want to maximize the sequential IO we’re doing\n\n\ncan also vary per workload\n\nroot to leaf traversal : small node sizes\nrange scans : large node sizes\n\n\n\n\nMerge Threshold\n\nsome dbms don’t always merge nodes when they’re half full &amp; delay it\n\nto reduce the amount of re-organization\n\n\nit might be better to let smaller nodes exists &amp; periodically rebuild the entire tree\n\n\nVariable Length Keys\n\nPointers : store key as pointers to the tuple’s attribute\n\nbad in a disk based system since we’d need to fetch each key using the ptr for comparison while traversing\nin memory based systems (esp. when there’s less memory), this approach prevents duplication\n\na variant of b-tree called t-tree does this\n\n\n\n\nVariable Length Nodes\n\nrequires careful memory management\n\n\nPadding : pad the key to be max length of the key type\n\nmysql does this\n\n\nKey Map / Indirection\n\nembed an array of pointers that map to the key + value list within the node\nstore an fixed length int instead of the key &amp; map it to the key\nkinda like dictionary compression\n\n\n\n\n\n\nIntra Node Search\n\nLinear\n\nSIMD (Single Instruction/Multiple Data) can be used to speed this up\n\n\nBinary\n\ndata needs to be sorted for this\n\n\nInterpolation (fastest)\n\napprox. location of desired key based on known distribution of keys\ndata should be sorted\nnote: this hasn’t been widely implemented outside academia\n\n\n\n\nOptimizations\n\nBuffer Updates : instead of applying the change right way, keep a log and apply the changes after a while (fractal tree index)\nPrefix Compression (like incremental compression in Lecture 5)\nDeduplication : avoid storing multiple copies of same key in leaf nodes; store key once &amp; maintain a list of tuples with that key (similar approach as hash tables)\nSuffix Truncation: we don’t need entire keys in inner nodes, just need a minimum prefix to correctly route probes into the index\nPointer Swizzling\n\nstore ptrs instead of page id to avoid the address lookup from the page table if a page is pinned in the buffer pool\nsome extra metadata needs to be tracked so that we know when a page is unpinned &amp; we can’t use the ptr anymore\n\n\nBulk Insert\n\nfastest way to build a new B+ tree for an existing table is to first sort the keys &amp; then build the index from bottom up\n\n\n\n\n\n9 : Index Concurrency Control §\n\nSingle Threaded Engines: VoltDB (by Andy), Redis\nConcurrency Protocol : method that DBMS uses to ensure “correct” results for concurrent operations on a shared object\nCorrectness Criteria\n\nLogical Correctness : can a thread see the data it’s supposed to see?\n\nif I insert a key, can I read it back?\n\n\nPhysical Correctness : is the internal representation of the object sound?\n\npointers aren’t invalid, data is correct etc.\n\n\n\n\nFocusing on Physical Correctness in this lecture.\n\nLocks vs. Latches §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_LocksLatchSeparateUser TransactionsThreadsProtectDB ContentsIn-Memory Data StructuresDuringEntire TransactionsCritical SectionsModesShared, Exclusive, Update, IntentionRead, WriteDeadlockDetection &amp; ResolutionAvoidanceDeadlock ByWaits-for, Timeout, AbortsCoding DisciplineKept InLock ManagerProtected Data Structures\n\n\n\nLatch Modes\n\nRead : multiple threads can read the same object at the same time\nWrite : only 1 thread can access the object\n\nif a thread a acquired a write latch, other threads can’t read or write\n\n\n\n\n\nLatch Implementations\n\nBlocking OS Mutex\n\nnon-scalable: about 25ns per lock/unlock invocation\neg: std::mutex in C++ -&gt; pthread_mutex -&gt; futex\n\nfutex : fast user mutex (user space spinlock)\nif fail to acquire then fall back to heavyweight mutex\n\n\n\n\nReader-Writer Latches\n\nallows for concurrent readers\nmust manage r-w queues to avoid starvation\ncan be implemented on top of spinlocks\neg: std::shared_mutex in C++ -&gt; pthread_rwlock\n\n\n\n\nSidenote: A lot of production systems have their own latches instead of using the one provided by the OS.\n\n\n\nHash Table Latching\n\nall threads move in same direction &amp; only access single page/slot at a time\ndeadlocks aren’t possible\nsidenote: take a global write latch on the entire table while re-sizing\nApproaches (note: pages have slots in them)\n\nPage Latches\n\neach page has its own r-w latch that protects its entire contents\nless metadata storage overhead but reduced amount of parallelism\n\n\nSlot Latches\n\neach slot has its own latch\n\n\n\n\n\n\n\nB+ Tree Concurrency Control §\n\nLatch Crabbing / Coupling\n\nProtocol to allow multiple threads to access / modify B+ Tree at the same time.\n\nGet latch for parent\nGet latch for child\nRelease latch for parent if “safe”\n\n\nSafe node: won’t split or merge when updated i.e.\n\nnot full (for insertion)\nmore than half-full (for deletion)\n\n\nReleasing latches from top-to-bottom allows other threads to proceed with their request faster compared to doing to bottom-to-top.\n\n\nAcquiring a write latch on root node everytime is a major bottleneck.\n\nOptimization\n\nassume that most modifications to a B+ Tree won’t require a split or merge\ntake read latches from top to bottom &amp; when you reach the leaf node then determine if your assumption is correct or not\n\nif correct: take write latch on leaf node, perform operation\nif not: release all latches, start search from start again but use the pessimistic algorithm (discussed above) this time\n\n\n\n\nSidenote: What about lock-free data structures?\n\nDon’t work that well under high contention.\nMentioned: Trying out an implementation of Microsoft’s B-W tree against a B+ Tree. B+ Tree was faster. Also mentioned that skip-list (which is latch free) is also slower than a good B+ Tree.\nB-W Tree has a separate lookup table with tree node ids &amp; that extra lookup reduces performance.\n\n\n\n\nWhat if threads want to move from one leaf node to another? (Leaf Node Scan)\n\nThere’s potential for deadlocks now compared to when we were just moving top to bottom.\nFor a scan, once we’ve reached the leaf node level, we can acquire read latches in the same way on siblings (and release them) as we were doing in top-&gt;bottom latch crabbing.\nConsider 2 parallel readers in opposite directions, if they intersect while scanning, it’s not an issue because a node can have multiple read latches &amp; once they move across each adjacent node will only have a single read latch.\nNow consider one scan &amp; another delete operation. The reader can’t acquire a read latch on the leaf node with the key that’s going to be deleted if the deleter acquired a write latch on it first.\n\nWe’d wait for very little time and then abort the read.\nNote: If the reader acquired the read latch earlier on the “to-be-deleted” node then we won’t have an issue. After the read, the deleter will delete the node.\n\n\nSuggestion from students: Jitter in timeouts, Drop readers since they’re less costly\nLatches don’t support deadlock detection or avoidance.\nThe leaf node sibling latch acquisition protocol must support a “no-wait” mode.\nDBMS’s data structures should cope with failed latch acquisitions.\n\n\n\n10 : Sorting &amp; Aggregations Algorithms §\n\nDisk Manager -&gt; Buffer Pool Manager -&gt; Access Methods (scanning indexes, scanning the table itself) -&gt; Operator Execution -&gt; Query Planning\nFocus on executing queries using the DBMS components built so far.\nOperator Algorithms, Query Processing Models, Runtime Architectures\nSQL Statement -&gt; AST -&gt; DAG\nWe can’t assume that query results necessarily fit in memory. Buffer Pool will be used to implement algorithms that need to spill to disk.\n\nAgain, prefer algorithms that maximize sequential I/O.\n\n\nSidenote: Mentioned using “kernel bypass” instead of OS APIs to do I/O.\n\nSorting §\n\nWhy do we need to sort?\n\nRelational model is unsorted. Queries can request sorted data.\nFor operations like duplicate elimination (DISTINCT), aggregations (GROUP BY), bulk loading sorted tuples into B+ Tree index.\n\n\nIn-Memory Sorting: Any standard sorting algorithm like quicksort.\n\nQuicksort would do random I/O on pivots which isn’t preferable for sorting involving disk.\n\n\nTop-N Heap Sort (Special case)\n\nIf a query contains an ORDER BY with a LIMIT, then the DBMS only needs to scan the data once to find the top-N elements.\nIdeal scenario for heapsort if the top-N elements fit in memory\n\nScan data once, maintain an in-memory sorted priority queue\n\n\npg does this\n\n\nExternal Merge Sort (Common)\n\nDivide and conquer algorithm.\nSplit data into runs (chunks of data that fit in-memory) -&gt; Sort each run individually &amp; write it back to disk -&gt; Combine into longer sorted chunks\nSorted Run\n\nA run is a list of key-value pairs.\nKey : Attribute(s) to compare to decide the sort order.\nValue: Early vs Late Materialisation\nEg: If you’re sorting an Employee table by Salary then “Salary” would be the key &amp; the Record ID or the corresponding Tuple would be the value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarly MaterialisationLate MaterialisationStore the tupleStore record IDNeed to copy the values multiple times during the sort-Data can be readily sent back to the clientData needs to be fetched using the record IDs when sending back-If a lot of data is filtered out then we didn’t waste space or memory storing unnecessary values?Common in column stores\n\n2-Way External Merge Sort\n\n“2” is the no. of runs to be merged into a new run for each pass.\nData is broken up into N pages.\nDBMS has a finite no. of B buffer pool pages to hold input and output data.\nNote: sortmemory is configurable in databases\nIn each pass, we read &amp; write every page in the file\n\nNumber of passes: 1 + ⌈log2N⌉\nTotal I/O cost: 2N * (# of passes)\n\n\nThis algorithm requires only 3 buffer pool pages (2 input, 1 output)\n\nIt doesn’t effectively utilise additional buffer space (if available).\nDouble Buffering Optimization : Prefetch the next run in the background &amp; store it in a 2nd buffer while the system is processing the current run.\n\nReduces wait-time for I/O requests at each step by continuously utilizing the disk.\n\n\n\n\n\n\nGeneral External Merge Sort\n\nPass #0\n\nUse B buffer pages\nProduce ⌈N / B⌉ sorted runs of size B\n\n\nPass #1,2,3\n\nMerge B-1 runs\n\n\nNo. of passes = 1 + ⌈ logB-1⌈N / B⌉⌉\nTotal I/O Cost = 2N * (# of passes)\nNote: We were ignoring the constant factor B in the 2-way external merge sort\n\n\nComparison Optimizations\n\nCode Specialization: instead of providing a comparison function as a pointer to the sorting algorithm, create a hardcoded version of sort that is specific to a key type.\n\nIf everything fits in memory then following the ptr to the function is costly.\nIn some systems, you can build out specialized version of the sorting algorithm where comparison functions are hard-coded in the sorting algorithm itself.\nMentions Just-In-Time compilation\npg does this\n\n\nSuffix Truncation: Compare a binary prefix of long VARCHAR keys instead of slower string comparison. Fallback to slower version if prefixes are equal.\n\n\nUsing B+ Trees for Sorting\n\nIf the table already has an index on the sort attribute(s), we can use it to accelerate sorting by retrieving tuples in desired sort order by traversing the leaf pages of the tree.\nCases\n\nClustered B+ Tree\n\nTraverse to the left-most leaf page and then retrieve tuples from all leaf pages.\n\n\nUnclustered B+ Tree\n\nChase each pointer to the page that contains the data.\nWould require random I/O for each data record.\n\n\n\n\nFrom Clustered and nonclustered indexes\n\nClustered indexes sort and store the data rows in the table or view based on their key values. These key values are the columns included in the index definition. There can be only one clustered index per table, because the data rows themselves can be stored in only one order.\nNonclustered indexes have a structure separate from the data rows. A nonclustered index contains the nonclustered index key values and each key value entry has a pointer to the data row that contains the key value.\n\n\n\n\n\nAggregations §\n\n\nCollapsing values for a single attribute from multiple tuples into a single scalar value.\n\n\nImplementation choices:\n\nSorting\nHashing\n\n\n\nHashing will almost always be better than sorting (esp. if your disk is slow)\n\n\nGoal is to avoid having random I/O going to disk &amp; maximizing sequential access.\n\n\nIf the SQL statement has a ORDER BY clause then it makes more sense to use sorting.\n\n\nIf the data doesn’t need to be sorted then hashing is usually better since it’s computationally cheaper.\n\n\nHashing Aggregate\n\nPopulate an ephemeral hash table as the DBMS scans the table.\nIf everything doesn’t fit in memory then we’ll need to perform an “External Hashing Aggregate”\nAssumption: We only need a single pass (of both phases) for now.\nPhase #1 - Partition\n\nDivide tuples into buckets/partitions based on hash key.\n\nA partition is one or more pages that contain the set of keys with same hash value.\n\n\nWrite them out to disk when they get full.\n\nPartitions are spilled to disk via output buffers.\n\n\nLet’s say we have B buffers.\n\nB-1 buffers for the partition\n1 buffer for the input data.\nPossible Optimization: If we have a  DISTINCT clause, we can avoid duplicate keys when building the initial partitions.\n\n\n\n\nPhase #2 - ReHash\n\nBuild in-memory hash table for each partition &amp; compute the aggregation.\n\nFor each partition on disk\n\nAssumption: The hash table &amp; each partition fits in memory.\n\nHash tables would be comparatively smaller than the entire dataset so they should ideally fit in memory. Even if they do spill-over, it can be dealt with easily.\n\n\nRead it in memory &amp; build an in-memory hash table based of a different hash function.\nGo through each bucket of this hash table to bring together matching tuples.\nDuring the Rehash phase, we want to store pairs in the form:\n\n(GroupKey-&gt;RunningVal)\nThing of the AVG clause using COUNT &amp; SUM\nHashTable would have a key &amp; then value as (count, sum) when could be further aggregated to produce the final result.\n\n\n\n\n\n\n\n\n\n\n\nDiscussed Optimizations for Sorting\n\nChunk I/O into large blocks to amortize costs\nDouble-buffering to overlap CPU &amp; I/O\n\n\n\nMentions: in-memory DBMS that assume everything can fit in memory\n\npg, MySQL are built from the ground-up to assume that things won’t fit in memory\n\n\n\n11 : Joins Algorithms §\n\nSince tables in a relational DB are normalised to avoid unnecessary repetition of information, we need to perform joins (using the join operator) to re-construct the “original tuples” without any loss of information.\nSidenote: For OLAP systems, a/c research 15-50% of the time for a query is spent in joins.\n⋈ is the symbol for the join operator.\nIn this lecture: Focusing on performing binary joins (involving 2 tables) using “inner equijoin” algorithms\n\ninner equijoin: match b/w left and right relation; equality operator used\n\ninner equijoin are the most common joins\nleft outer &amp; right outer joins can be understood as an extension to “inner equijoin”\n\n\nOther than this, there are anti-joins, inequality joins etc.\n\n\nSidenote: Multi-way (involving multiple tables at once) joins exist mostly in research for now.\nGenerally: smaller tables are used as outer (on left side), larger table (with more rows) are used as inner (on right side)\n\nSidenote: Query optimizer can consider these facts to optimize the query if the ordering was incorrect in initial query.\nNumber of tuples is more important in deciding which table should be considered the inner table.\n\n\nDecisions to make for Join operators\n\nOutput (data emitted by join operator to its parent operator in the query plan tree)\nCost Analysis Criteria (how to decide if one algorithm is better than other)\n\n\nSidenote: Query optimisations involves multiple stages: Logical Plan -&gt; Physical Plan\nOperator Output\n\nQuery optimizer can see the output required (say it needs to be sorted) &amp; decide what algorithm to use while doing the join (like merge sort).\nEarly Materialisation : copy the values for the attributes in outer &amp; inner tuples into a new output tuple (so that subsequent operators don’t need to go back to the base tables to get more data)\n\nProjection in the join operator\n\n\nLater Materialisation: only copy the join keys along with the record IDs of the matching tuple\n\nIdeal for column stores since unnecessary data isn’t copied.\nSidenote: Vertica (a column store) decided to stop late materialisation since they wanted stability in the query performance &amp; it’s easier to know the cost with early materialisation. (Some businesses like things to be stable rather than some queries be fast &amp; some slow)\n\n\n\n\nCost Analysis\n\nIO is the most important cost measure since disk is the bottleneck.\nOutput costs are ignored since that depends on the data &amp; we can’t compute that yet.\nComputation cost (of things in memory) is also ignored.\nJoin (R ⋈ S ) vs. Cross Product (R ⨯ S)\n\nCross Product involves just iterating &amp; matching all rows from both tables involved &amp; then filtering them. It’s obviously inefficient because of the large cross product.\n\nCROSS JOIN in SQL does this.\n\n\n\n\n\n\nJoin Algorithms\n\nNested\n\nSimple / Stupid\n\nNot really used in any system in production.\nhttps://dev.mysql.com/doc/refman/8.0/en/nested-loop-joins.html#nested-loop-join-algorithm\n\n\nBlock\n\nCommonly in a lot of systems.\nhttps://dev.mysql.com/doc/refman/8.0/en/nested-loop-joins.html#block-nested-loop-join-algorithm\n\n\nIndex\n\nFor looking up a query that’ll result in a few records in OLTP &amp; already have an index.\n\n\n\n\nSort Merge Join\n\nIf the output needs to be sorted &amp; your sort, join key are the same.\n\n\nHash Join\n\nGenerally the fastest one.\n\n\n\n\n\n\nR ⋈ S\n- R is the outer table\n- S is the inner table\n- M pages in table R, mtuples in R\n- N pages in table S, n tuples in S\n\nNested Loop Joins §\n\nSimple Nested Loop Join\n\nCost: M + (m . N)\nFor each tuple in the outer table, we must do a sequential scan to check for a match in the inner table.\nAlgorithm\n\n\n\nforeach tuple r ∈ R: // Outer\n\tforeach tuple s ∈ S: // Inner\n\t\temit, if r and s match\n\n\nBlock Nested Loop Join\n\nCost\n\nfor 3 buffer pages: M + (M.N)\nfor B buffer pages: M + (⌈M / (B-2)⌉ ∙ N)\n\n\nAlgorithm\n\nPerforms less disk accesses.\n\n\n\n\n\n// Consider 1 buffer = 1 page\n\n// 3 buffer pages: 2 input + 1 output\nforeach block Br ∈ R:\n\tforeach block Bs ∈ S:\n\t\tfor each tuple r ∈ Br:\n\t\t\tfor each tuple s ∈ Bs:\n\t\t\t\temit, if r and s match\n\n\n// B buffers available\n// 1 buffer for inner table, 1 for storing output taken\nforeach B-2 pages pr ∈ R:\n\tforeach page ps ∈ S:\n\t\tforeach tuple r ∈ B - 2 pages:\n\t\t\tforeach tuple s ∈ ps:\n\t\t\t\temit, if r and s match\n\n\nIndex Nested Loop Join\n\nWe can avoid sequential scans by using an index to find inner table matches.\nCost: M + (m.C)\n\nAssume the cost of each index probe is some constant C per tuple.\nThe constant C is hard to quantify without knowing what the index is since it’s different for different indexes. A unique index returns 1 tuple but a non-unique index can return a very large number of tuples.\n\n\nAlgorithm\n\n\n\nforeach tuple r ∈ R:\n\tforeach tuple s ∈ Index(ri = sj):\n\t\temit, if r and s match\n\nSidenote: You can specify in some systems how much memory you want to use in a query for a join (or set a per query limit). Beyond that, things will spill to disk.\nTakeaways\n\nPick the smaller table as the outer table.\nBuffer as much of the outer table in memory as possible.\nLoop over the inner table (or use an index)\n\nSort-Merge Join §\n\nWorking\n\nPhase 1 : Sort\n\nSort both tables on the join key(s).\n\n\nPhase 2: Merge\n\nStep through the 2 sorted tables with cursors &amp; emit matching tuples.\nMay need to backtrack depending on join type (and whether there are duplicates)\n\nThis isn’t that costly since the data is already sorted.\n\n\n\n\n\n\nCost: Sort R + Sort S + Merge\n\nSort R: 2M . (1 + ⌈ logB-1 ⌈M / B⌉ ⌉)\nSort S: 2N . (1 + ⌈ logB-1 ⌈N / B⌉ ⌉)\nMerge: M + N\nAssumption: No backtracking during merge\n\n\nWhen is sort-merge useful?\n\nOne or both tables are already sorted on join key.\nOutput must be sorted on join key.\nInput relations may be sorted either by an explicit sort operator or by scanning the relation using an index on the join key.\nSidenote: For a join producing small number of tuples, the Hash Join or Index Nested Loop Join could probably be better.\n\n\n\nHash Join §\n\nSame idea as index nested loop join (except you’ve to build a HashTable compared to a BTree index already existing)\nWorking\n\nPhase 1 : Build\n\nScan the outer relation &amp; populate a hash table. using the hash function “h1” on the join attribute.\nIn practice, linear probing works the best. (Rest discussed in Lecture 7)\n\n\nPhase 2 : Probe\n\nScan the inner relation &amp; use “h1” on each tuple to jump to a location in the hash table and find a matching tuple.\n\n\n\n\nContents\n\nKey: attribute(s) query will be joining tables on\n\nWe always need the original key (before hashing) to verify correct match in case of hash collisions.\n\n\nValue : depends on implementation (Early vs Late Materialization)\n\n\nCost: B . (B-1)\n\nAssumptions:\n\nB-1 partitions in Phase 1 (each no more than B blocks big):\nA table on N pages needs about sqrt(N) buffers.\nHash distributes records evenly.\n“If the hash function is not uniform, a fudge factor f &gt; 1 can be introduced, therefore the largest such table is B . sqrt(f)”\n\n\n\n\n\nOptimizations on Hash Join §\n\nBloom Filters\n\nProbing the hash table is expensive (esp. if the collision factor is high)\nCreate a Bloom Filter during the build phase to check if the key doesn’t exist.\n\nBloom Filter is much more compact than the actual hash-table &amp; fits in CPU cache or memory.\nProbabilistic data structure (bitmap)\nFalse negatives will never occur.\nFalse positives can happen sometimes.\nOperations\n\nInsert(x) : Use k hash functions to set bits in the filter to 1.\nLookup(x): Check whether the bits are 1 for each hash function.\n\n\nSee Bloom Filter Calculator\nGives 2x speedup over hash-joins in practice.\n\n\n\n\nPartitioned Hash Join / Grace Hash Join\n\nWe don’t want to let the buffer pool manager swap out hash table pages at random when we don’t have enough memory to fit the entire hash table.\nWorking\n\nBuild Phase: Hash both tables on the join attribute into partitions.\nProbe Phase: Compares tuples in corresponding partitions for each table.\n\n\nIf the buckets don’t fit in memory, perform the partitioning again.\n\nIn practice, 2 passes are enough.\n\n\nCost: 3(M + N)\n\nBuild Phase: Read+Write both tables\n\nCost: 2(M + N) (1 pass to read in, 1 to write to buckets)\n\n\nProbe Phase: Read both tables\n\nCost: M+N\n\n\nAssumptions:\n\nWe’ve enough buffer.\nNo recursive partitioning.\n\n\n\n\n\n\nHybrid Hash Join\n\nIf the keys are skewed, DBMS keeps the hot partition in-memory &amp; immediately perform the comparison instead of spilling to disk.\n\nSkewed: unevenly balanced among partitions\n\n\n\n\n\nCost Analysis §\n\nExample Database\n→ Table R: M = 1000, m = 100,000\n→ Table S: N = 500, n = 40,000\nConsider 4KB pages.\nNote: Technically, the DB is 6MB &amp; could all just fit in cache but we’re considering I/O use here.\nAssume 0.1ms / IO\n\nSimple Nested Loop Join\n\nM + (m . N) = 1000 + (100000 ∙ 500) = 50,001,000 IOs ≈ 1.3 hours\nN + (n ∙ M) = 500 + (40000 ∙ 1000) = 40,000,500 IOs ≈ 1.1 hours\n\nBlock Nested Loop Join\n\n3 Buffers (1 for outer, 1 for inner, 1 for output)\n\nM + (M ∙ N) = 1000 + (1000 ∙ 500) = 501,000 IOs\nAt 0.1 ms/IO, Total time ≈ 50 seconds\n\n\nB Buffers (and the outer relation fits completely in memory so B &gt; M + 2)\n\nM + N = 1000 + 500 = 1500 IOs ≈ 0.15 seconds\n\n\n\nSort-Merge Join\n\nWith B = 100\n\nSort Cost (R) = 2000 ∙ (1 + ⌈log99 (1000 /100)⌉) = 4000 IOs\nSort Cost (S) = 1000 ∙ (1 + ⌈ log99 (500 / 100)⌉) = 2000 IOs\nMerge Cost = (1000 + 500) = 1500 IOs\nTotal Cost = 4000 + 2000 + 1500 = 7500 IOs ≈ 0.75 seconds\n\n\n\nPartitioned Hash Join\n\n3 ∙ (M + N) = 3 ∙(1000 + 500) = 4,500 IOs ≈  0.45s\n\n\nHashing is almost always better than sorting for operator execution (except for non-uniform data or when the result needs to be sorted)\n\n12 : Query Execution 1 §\n\nAgenda: Processing Models -&gt; Access Methods -&gt; Modification Queries -&gt; Expression Evaluation\n\nProcessing Models §\n\nDefines how system executes a query plan.\nApproaches\n\nIterator / Volcano / Pipeline Model\nMaterialization Model\nVectorized / Batch Model\n\n\nIterator / Volcano / Pipeline Model\n\nEach operator implements a Next() function &amp; calls Next() on its children to get tuples &amp; process them. Each invocation returns “either a single tuple or a null marker if there are no more tuples”.\nUsed in almost every DBMS. (Exception: Data warehousing, OLAP workloads)\n\nEg: MongoDB, Cassandra, pg, sqlite, MySQL etc.\n\n\nSome operators are blocked until their children emit all tuples. Eg: Joins, Subqueries, Order By\nAdvantages: Easy to Implement, Output control (eg: limiting a query to 10 records etc.) works easily, Don’t need to write to disk a lot in-process since the pipeline just passes data from one operator to another in-memory (unless the dataset is too big &amp; spills out to disk)\nDisadvantage: Deals with only 1 tuple at a time. Sometimes batching might be better.\n\n\nMaterialization Model\n\nEach operator processes its input all at once and then emits its output all at once.\nDBMS can push down hints (like LIMIT) to avoid scanning too many tuples.\nOutput can be whole tuples (NSM) or subset of columns (DSM)\n\nNSM : N-Ary Storage Model\nDSM : Decomposed Storage Model\n\n\nBetter for OLTP workloads because queries only access a small number of tuples at a time.\n\nEg: VoltDB, RavenDB (DocumentDB), MonetDB (ColumnStore)\n\n\n\n\nVectorization Model\n\nEach operator implements a Next() function (like the Iterator model) &amp; emits a batch of tuples (instead of single tuple in Iterator model).\n\nCan do “vectorized operations” like SIMD\n\n\nSize of batch depends on: hardware, query properties.\nIdeal for OLAP queries.\n\nEg: Databricks, Snowflake, ClickHouse, DuckDB, CockroachDB\n\n\n\n\nConclusion\n\nFor general purpose systems: Iterator model\nFor specific OLTP workloads: Materialization model\nFor OLAP: Vectorization model\n\n\nSidenote: GPU-accelerated DBs is a bad idea because data exists on disk or memory. Data has to go from disk -&gt; memory -&gt; GPU. Overhead for transfer to CPU cache is relatively low compared to GPU.\nSidenote: Plan Processing Direction\n\nTop-to-Bottom: Start with root &amp; pull data up from its children. Tuples are passed w/ function calls.\nBottom-to-Top: Start with leaf nodes and push data to their parents. Allows for tighter control of caches / registers in pipelines.\nMost systems go from top-to-bottom.\n\n\n\nAccess Methods §\n\nWay that DBMS accesses data stored in table.\nBasically scanning a table or an index.\n\nTable also includes the temporary tables created for a query.\n\n\nApproaches:\n\nSequential Scan\nIndex Scan\nMulti-Index Scan\n\n\nSequential Scan\n\nFor each page in table, retrieve from buffer pool &amp; iterate over each tuple &amp; check whether to include it.\nDB maintains an internal cursor that tracks the last page / slot examined.\nOptimisations\n\nPrefetching\nBuffer Pool Bypass (avoids ruining the buffer pool for a single query)\nParallelization\nHeap Clustering (data is pre-sorted based on index)\nLate Materilization\nData Skipping\n\n\nData Skipping\n\nApproach 1 : Approximate Queries (Lossy)\n\nExecute queries on a samples subset of the entire table to produce approximate results.\nExample Query: Visitors for a site\nDBs using this: Redshift, Snowflake, DataBricks, BigQuery, Oracle\n\n\nApproach 2 : Zone Maps (Lossless)\n\nPre-compute columnar aggregations per page. DBMS can use the calculated zone map to decide whether to access the page or not.\n\nEg: Say if you’re looking for data with value above a threshold then pages that have a max below it don’t need to be accessed.\n\n\nDBs using this: Vertica, Snowflake, BigQuery, Oracle\n\n\n\n\n\n\nIndex Scan\n\nDBMS picks an index to find the tuples a query needs.\nSome factors used in deciding which index to use: attributes in index, attributes in query, attribute’s value domains (?), predicate composition, index has unique keys or not\n\n\nMulti-Index Scan\n\nIf no single index fits particularly well but combination of multiple indexes can be used.\nGet Record IDs for each index -&gt; Combine (Union or Intersect depending upon query) -&gt; Retrieve the records and process the rest of the query\nEg: pg Bitmap Scan, MySQL Index Merge\n\n\n\nModification Queries §\n\nOperators like INSERT, UPDATE, DELETE that modify the DB are responsible for modifying the target table and its indexes too.\nOutput of operators can be Record IDs or Tuple Data.\nUPDATE/DELETE\n\nChild operators pass RIDs for target tuples.\nKeeps track of previously seen tuples to avoid modifying a single tuple multiple times.\n\nSay you’re processing a query that increases a number for multiple records. If you’re using an index on the column storing the number, you’d encounter it again after increasing it since the new index entry would be added with the “new number”.\nHalloween Problem: “Anomaly where an update operation changes the physical location of a tuple, which causes a scan operator to visit the tuple multiple times.”\n\n\n\n\nINSERT\n\nA: Materialize tuples inside of the operator OR\nB: Operator inserts any tuple passed in from child operators.\n\nBetter approach since Insert operator doesn’t need to handle additional materialisation logic.\n\n\n\n\n\nExpression Evaluation §\n\n\nDBMS represents a WHERE clause as an expression tree.\n\n\nNodes in the tree can be:\n\nComparisons\nConjunction (AND) or Disjunction (OR)\nArithmetic Operators\nConstant Values\nTuple Attribute References (column name)\n\n\n\nEvaluating the expression tree for every record is slow.\n\n\nOptimisation:\n\nUse JIT compilation so that instead of the expression tree traversal, you can just call a function.\n\n\n\nSidenote: You can use JIT compilation on the entire query plan. Here we’re using it only for Expression Evaluation.\n\n\npg supports this. A lot of other DBs do too.\n\nYou can use SET jit = &#039;off&#039; to turn this off for perf comparison.\npg also decides if JIT is worth doing for a query because compiling the function isn’t worth it for a small amount of rows.\n\n\n\nSidenote: Some OLAP DBs (like Vertica) don’t support any indexes at all. They just pre-sort &amp; perform sequential scans.\n\n\n13 : Query Execution 2 §\n\nAgenda: Executing queries using multiple workers (on a single instance)\n\nProcess Models, Execution Parallelism, I/O Parallelism\n\n\nWhy care about parallel execution?\n\nIncreased performance for the same hardware: Higher Throughput, Lower Latency\nIncreased responsiveness\nPotentially lower total cost of ownership (TCP)\n\n\nSidenote: Intel processors have homogenous cores while Apple M1 has cores with different capacities.\nParallel vs Distributed DBMSs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-ParallelDistributedLocalityClose(can be) FarCommunication OverHigh Speed InterconnectSlower interconnectCommunicationCheap &amp; ReliableCostly &amp; Problematic\nProcess Model §\n\nDefines how the system is architected to support concurrent requests from a multi-user application.\nWorker: DBMS component responsible for execution tasks on behalf of client and returning the results.\nApproaches:\n\nProcess per DBMS worker\nThread per DBMS worker\nEmbedded DBMS\n\n\nProcess per DBMS worker\n\nEach worker is a separate OS process. (fork call in Linux)\nDisadvantages:\n\nRelies on OS scheduler\nUses shared-memory for global data structures (OR can use some kind of message passing b/w procs to keep track of global state)\n\n\nAdvantages:\n\nProcess crash doesn’t take down entire DBMS. (Andy: Be good. Don’t code things that’ll take down the DBMS)\n\n\nEg: pg, Oracle (old version) (these use shared memory)\nSidenote: Reason for OS threads being used in older DBs is variance in thread implementation b/w different OSs.\nFlow: Application -&gt; Dispatcher -&gt; Worker Processes -&gt; |\nDispatcher keeps tracks of forked processes &amp; their liveliness. It sends back the reference to a worker process to the client which the client can then use to run the SQL query.\nSidenote: In pg, postmaster is the dispatcher.\n\n\nThread per DBMS worker\n\nSingle process with multiple worker threads. Usually implemented using pre-emptive native OS thread (pthread call in Linux)\n\npreemptive thread: thread can be interrupted to let other threads run depending upon priority\n\n\nAdvantage:\n\nDBMS manages its own scheduling.\nLess context switch overhead.\n\n\nEg: MS-SQL, MySQL, Oracle (2014)\nSidenote:  DB2 supports both process &amp; thread per worker.\n\n\nScheduling\n\nDecisions:\n\nnumber of tasks to use\nCPU cores to use\nwhat CPU core should a task execute on\n\nonly for systems that care about NUMA; MySQL &amp; pg don’t do this, SQL Server does)\nNUMA: Non-uniform memory access\n\n\nwhere should a task store its output\n\nwrite to memory local to the core used by the next query operator\n\n\n\n\nCase Study : SQL Server : SQLOS\n\nSQLOS is a user-level OS layer that runs inside of the DBMS and manages provisioned hardware resources.\n\nDetermines which tasks are scheduled onto which threads.\nManages I/O scheduling and higher-level concepts (like logical database locks)\nDBMS makes no direct calls to the OS and uses the SQLOS abstraction.\nDoes non-preemptive thread scheduling. SQLOS decides what to do when thread “yields” to it.\nSQLOS uses a “quantum” of 4ms but since the threads are non-preemptive, this can’t be imposed. DBMS devs must add explicit yield calls in source code.\n\nQuantum: unit of time used by scheduler for allocation (time slice)\n\n\n\n\nSidenote: SQLOS was created during re-write in 2005-06.\n\n\n\n\nEmbedded DBMS\n\nDBMS runs in the same address space as the application.\nApplication is (mostly) responsible for threads and scheduling.\n\nDuckDB can spawn additional execution threads on its own.\n\n\nEg: BerkeleyDB, SQLite, RocksDB, LevelDB\n\n\n\nExecution Parallelism §\nInter-Query : Execute multiple disparate queries simultaneously.\nIntra-Query : Execute the operations of a single query in parallel.\nIntra-Query Parallelism\n\nMultiple workers for each query operator OR Multiple workers for different query operators\n“Think of organization of operators in terms of a producer/consumer paradigm.”\nCan either have multiple threads access centralized data structures or use partitioning to divide work up and write to a unique output (which can be then combined)\nWorkers may or may not require co-ordination among each other. (eg: Scanning tables won’t require coordination, Finding distinct keys will though)\nSidenote: Parallel Grace Hash Join (see Lecture 11 for Grace Hash Join)\n\nUse separate worker to perform the join (building and probing the hashtable) for each level of buckets for both tables after partitioning\n\n\nApproaches\n\nIntra-Operator (Horizontal)\nInter-Operator (Vertical)(Pipeline Parallelism)\nBushy (combination of Intra + Inter)\n\n\nIntra-Operator\n\nDecompose operators into independent fragments that perform the same function on different (disjoint) subsets of data.\nData could be coming from multiple child or be passed to multiple parent operators.\nMost common in DBMSs.\nDBMS inserts an “exchange” operator into the query plan to coalesce / split results from multiple child / parent operators.\nSidenote:\n\nThis is what pg supports &amp; calls “gather”. ? (confirm)\nSQL Server supports all 3 kinds of approaches.\n\n\nTypes of Exchange Operator\n\nGather : multiple workers -&gt; single output stream\n\nMost common.\n\n\nDistribute: single input stream -&gt; multiple output streams\nRepartition: multiple input streams -&gt; multiple output streams\n\nNote: Number of input and output streams doesn’t need to be the same.\n\n\n\n\n\n\nInter-Operator\n\nWorkers execute operators from different segments of a query plan at the same time.\n\nThink this way: A worker assigned to each operator. The operators runs all the time and processes any tuple that arrives &amp; emits it the the next worker.\n\n\nMore common in streaming / continuous query systems. Eg: Kafka, Flink, Spark\n\n\nBushy\n\nWorkers execute multiple operators from different segments of a query plan at the same time.\nNeeds Exchange operators.\n\n\n\nI/O Parallelism §\n\nAdditional threads won’t help if disk is the main bottleneck.\n\nWorkers accessing different segments of the same disk will slow it down.\n\n\nSolution: Split DBMS across multiple storage devices\nApproaches\n\nMultiple Disks / DB\n1 DB / Disk\n1 Relation / Disk\nMultiple Disks / Relation\n\n\npg, MySQL require configuration outside DBMS for this. Some DBMS support this natively.\n\nDB not supporting this natively prevents the DB scheduler from utilising the separation of storage devices in its decisions.\n\n\nMulti-Disk Parallelism\n\nRAID 0 (Striping) : Data can be split across multiple disks improving write latency.\nRAID 1 (Mirroring): Data replicated on multiple disks for redundancy. Also improves read latency.\nDB Partitioning\n\nSome DBMS allow specifying the disk location of each individual DB.\nRecovery log might be shared if transactions can update multiple DBs.\n\n\nTable-level Partitioning\n\nSplit single logical table into disjoint physical segments that are stored/managed separately.\nCommon in distributed DBs.\n\n\n\n\n\n14 : Query Planning &amp; Optimization §\n\nAgenda: Heuristic/Rule-Based Optimization, Query Cost Models, Cost-based Optimization\nQuery Optimization: find a correct execution plan that has the lowest “cost”\n\nCost can be speed, less network calls, energy efficiency or any other factor.\n\n\nLogical Plan (high-level description eg: Scan Table “foo”) Vs. Physical Plan (execution method eg. using index scan on “bar” index for “foo” table)\nOptimizer: Logical Algebra Expression =&gt; Physical Algebra Expression\nPhysical operators define a specific execution strategy using an access path.\n\nCan depend on physical format of data they process (eg. sorting , compression)\n\n\n\nApplication (SQL Query) -&gt; SQL Rewriter (SQL Query) -&gt; Parser (AST)\n\n-&gt; Binder (Logical Plan) -&gt; Tree Rewriter (Logical Plan)\n\n-&gt; Optimizer (Physical Plan) -&gt; |\n\n\nApplication (SQL Query)-&gt; SQL Rewriter (SQL Query)-&gt; Parser (AST)-&gt; Binder\n\nSQL Rewriting is rare. It involves rewriting the original SQL query.\n\nYouTube did this with their MySQL queries.\n\n\nBinder maps values in the AST to database objects (eg. Internal ID of a table or row) using the System Catalog.\n\nThis is the part that throws errors for invalid table, columns etc.\n\n\nTree Rewriter is optional but common. It gets schema info from the “System Catalog” for things like primary keys, foreign keys, type information on query plan.\n\nSidenote: Bustub goes from binder directly to physical plan.\n\n\nOptimizer / Compiler / Planner get additional information from the System Catalog and uses the “cost model” to predict the estimated cost of a query plan &amp; select the one with the lowest cost.\n\nThere are also “rule-based optimizers” which don’t need a cost model.\n\n\n\n\nQuery Optimization\n\nHeuristics / Rules | Logical Query Optimization\n\nRewrite query to remove inefficient things. Doesn’t need to examine data.\n\n\nCost-based Search | Cost Estimation\n\nCreate multiple equivalent plans for a query and pick the one with the lowest cost.\n\n\n\n\nNote: Most systems will use a mix of both approaches.\n\nLogical Plan Optimization §\n\nSplit Conjunctive Predicates\n\nDecompose predicates into their simplest forms to make it easier for the optimizer to move them around.\nEg: Separating multiple filter conditions in a WHERE clause.\n\n\nPredicate Pushdown:\n\nMove the predicate to the lowest applicable point in the plan.\nEg: Moving filters that remove large amount of data down in the query plan to reduce the amount of data that the query will operate on.\n\n\nReplace Cartesian Products\n\nReplace all Cartesian Products with inner joins using the join predicates.\n\n\nProjection Pushdown\n\nEliminate redundant attributes before pipeline breakers to reduce materialization cost.\nAssumption: We’re doing late materialisation.\nEg: You might need an attribute to filter the data but not really need it in the output of the query so it can be removed once the data is filtered.\n\n\nNested Sub-Queries\n\nApproaches\n\nRewrite to de-correlate and/or flatten them\nDecompose nested query and store result to temp. table &amp; then inject the value\n\nBreak queries into blocks and focus on one block at a time.\n\n\n\n\nCorrelated subquery: Inner queries reference something from the outer query\n\n\n\n/* Rewrite */\n \nSELECT name FROM sailors AS S\nWHERE EXISTS (\n SELECT * FROM reserves AS R\n  WHERE S.sid = R.sid\n  AND R.day = &#039;2022-10-25&#039;\n)\n \n/* After Re-write */\nSELECT name FROM sailors AS S, reserves AS R\nWHERE S.sid = R.sid\nAND R.day = &#039;2022-10-25&#039;\n/* Decomposing Queries */\nSELECT S.sid, MIN(R.day)\n\tFROM sailors S, reserves R, boats B\nWHERE S.sid = R.sid\n\tAND R.bid = B.bid\n\tAND B.color = &#039;red&#039;\n\tAND S.rating = (SELECT MAX(S2.rating) FROM sailors S2) /* Running it multiple times for the outer query would be wasteful */\nGROUP BY S.sid\nHAVING COUNT(*) &gt; 1\n \n/* After decomposing */\nSELECT MAX(rating) FROM sailors;\n \nSELECT S.sid, MIN(R.day)\n\tFROM sailors S, reserves R, boats B\nWHERE S.sid = R.sid\n\tAND R.bid = B.bid\n\tAND B.color = &#039;red&#039;\n\tAND S.rating = ### /* gets value from the separate query above */\nGROUP BY S.sid\nHAVING COUNT(*) &gt; 1;\nSidenote:\n\n\nIn some systems, the nested query is extracted &amp; run first and then go back to the optimizer with the value. (MySQL does this). Other systems extract the subquery, assign it to a variable and use that instead of running the query before-hand.\n\n\nExpression Rewriting\n\nQuery’s expressions (like the WHERE predicate) are turned into minimal set of expressions using if/then/else clauses or pattern-matching rule engine.\nImpossible / Unnecessary Predicates\n\nSELECT * FROM A WHERE 1 = 0\nSELECT * FROM A WHERE NOW() IS NULL\nSELECT * FROM A WHERE RANDOM() IS NULL\n\npg doesn’t re-write it, mysql does\n\n\nAfter rewriting: SELECT * FROM A WHERE false\n\n\nMerging Predicates\n\nSELECT * FROM A WHERE val BETWEEN 1 AND 100 OR val BETWEEN 50 AND 150\nAfter rewriting: SELECT * FROM A WHERE val BETWEEN 1 AND 150\n\n\n\n\n\nCost Estimation §\n\nCost Model Components\n\nPhysical Cost\n\nFactors: CPU cycles, I/O, cache misses, RAM consumption, network messages\nSidenote: Think about the. variance in physical cost while using a local SSD vs. sending messages over a network for writes.\n\n\nLogical Cost\n\nFactor: Output size (of no. of tuples) per operator\n\n\nAlgorithmic Cost\n\nFactor: Complexity of the operator algorithm implementation\n\n\n\n\nSidenote:\n\nAlgorithmic cost isn’t really used yet by anyone outside academia.\npg uses CPU &amp; IO costs weighted by constant factors (that can be configured by admin).\nDB2 runs some benchmarks on first start to analyze the disk it’s running on to set these constants.\nMongoDB runs all possible query plans &amp; selects the one that returns first.\n\nAndy doesn’t recommend doing this for queries with complex joins.\n\n\n\n\nStatistics\n\nDBMS stores internal statistics on tables, attributes, indexes &amp; updates them (regularly and/or as needed)\n\nEg: Table size changing by a large margin can be used to trigger this.\n\n\nCan be invoked manually too. ANALYZE in pg\nSelection Cardinality\n\n“The selectivity (sel) of a predicate P is the fraction of tuples that qualify.”\nAssumptions:\n\nUniform Data\n\nCan maintain a separate list for most-used values in a column and treat the rest as having the same distribution.\n\n\nIndependent Predicates\n\nSplitting up the conditions on different attributes in a WHERE clause, evaluating them &amp; multiplying them will lead to the correct selectivity.\nWhen is this assumption wrong?\n\nConsider a db of automobiles with company &amp; model name. A car model is produced by a specific company so it’s correlated entirely. There’d be a major difference in the actual &amp; calculated (with the assumption) selectivity value.\n\n\nSidenote: Correlations in different attributes can be specified in higher-end systems.\n\n\nInclusion Principles\n\nWhen we do a join, for every tuple in the inner table, there’ll be a tuple on the outer table.\n\n\n\n\n\n\nHistograms\n\nFor large number of rows with high cardinality, we can’t store stats on all the values for a column due to size constraints. We can bucket values.\nEqui-Width Histograms : Buckets have the same width (no. of values).\nEqui-Depth Histograms : Buckets have similar (not same) height (no. of occurrences).\n\n\nSketches\n\nProbabilistic data structures with approximate stats. Improves selectivity estimate accuracy over histograms in some cases.\nEg:\n\nCount Min Sketch: Approx. frequency\nHyperLogLog: Approx. number of distinct elements\n\n\n\n\nSampling\n\nCollect samples from tables to estimate selectivity. Update samples on significant change in tables.\nSQL Server does this. Most systems don’t.\n\n\n\n\n\nQuery Optimization §\n\nChoose the best plan for the query after evaluating all plans or on being timed out.\nNote: Aggregations are usually heuristic based so we’re focusing mainly on Joins here.\nSingle Relation Query Planning\n\nPick the best access method\n\nSequential Scan\nBinary Search (clustered indexes)\nIndex Scan\n\n\nPredicate Evaluation Ordering\nOLTP Query Planning\n\nEasy because “sargable” (Search Argument Able) because it’s usually just picking the best index &amp; joins are usually on foreign key relationships w/ small cardinality.\n\n\n\n\nMulti Relation Query Planning\n\nBottom-up\n\n“Start with nothing and then build up the plan to get to the outcome that you want.”\nStatic Rules (initial optimisations) -&gt; Dynamic Programming (determine the best join order)\nMore commonly implemented in DBs.\n\nEg: System R, MySQL, pg, DB2 etc.\n\n\nCase Study: System R Optimizer\n\nBreak query into blocks\nGenerate logical operators for each block\nGenerate physical operators for each logical operator\nGet all combinations of join algorithms &amp; access paths\nConstruct “left-deep” join tree with lowest cost. (Join tables going up one side of the tree)\n\nhttps://www.cs.emory.edu/~cheung/Courses/554/Syllabus/5-query-opt/left-deep-trees1.html\nhttps://iggyfernandez.wordpress.com/2010/11/27/sql-101-deep-left-trees-deep-right-trees-and-bushy-trees-oh-my/\n\n\nNote: System R didn’t consider things like how a sort merge join would be better than hash join if we want sorted data but newer systems use some metadata to keep track of these things too.\n\n\n\n\nTop-down\n\n“Start with the outcome that you want, and then work down the tree to find the optimal plan that gets you to that goal.”\nStart with a logical plan of what we want the query to be.\nDo branch-and-bound search to traverse the plan tree\n\nKeep track of global best plan during search.\nTreat physical properties of data as first-class entities during planning.\nBounding condition: Cost of the best plan seen so far.\n\n\nCan also create “enforcer rules” (eg: use sort merge if sorted data is needed in output)\nEg: MS-SQL, CockroachDB etc.\n\n\nSidenote: Talk on MS-SQL Server’s Optimizer: https://www.youtube.com/watch?v=pQe1LQJiXN0 (Best Optimizer as per Andy)\n\n\n\n15 : Concurrency Control Theory §\n\nAgenda: ACID (Mostly the Isolation part though)\nMotivation\n\nAvoiding race conditions\nDetermining correct database state\n\n\nTransaction : Execution of a sequence of one or more operations on a DB to perform some higher-level function. Basic unit of change in DB. Partial transaction not allowed.\nStrawman Example\n\nExecute each txn one-by-one (serial order). Only one txn can be running at a time in the DBMS.\nCopy the database file to a new file &amp; make all the changes to that file.\n\nIf the txn completes successfully, overwrite the original.\nIf the txn fails, just remove the dirty copy.\n\n\nWhy is this a bad idea?\n\nNo concurrency.\nCopying the entire DB file is inefficient.\n\n\nSidenote: Redis does things serially &amp; is still fast because it makes the changes in-memory.\n\n\nWhat do we want?\n\nConcurrent execution of independent transactions.\n\nWhy do we want that?\n\nBetter utilisation / throughput\nLower response times\n\n\n\n\nOther things to care about\n\nCorrectness\nFairness\n\n\nArbitrary interleaving of operations can lead to:\n\nTemporary inconsistency (unavoidable)\n\nThis isn’t exposed to the user.\nEg: There’s some state when say money was deducted from one account but hasn’t been added to the other account during a transfer.\n\n\nPermanent inconsistency\n\n\n\n\nDefinitions\n\nDatabase: Fixed set of named data objects.\n\nNo inserts/deletes. Only reads &amp; updates on already existing data objects.\n\n\nTransaction: Sequence of read and write operations\n\n\nTransactions in SQL\n\nStarts with BEGIN\nStops with either COMMIT or ABORT\n\nIf commit, the DBMS saves OR aborts all the txn’s changes.\nIf abort, all changes are undone. Abort can be done by user or the DB.\n\n\nSidenote: There are partial rollbacks.\n\n\nCorrectness Criteria : ACID\n\nAtomicity : All actions in txn happen, or none happen.\nConsistency : If each txn is consistent and the DB starts consistent, then it ends up consistent.\n\nAndy mentioned that this explanation is vague.\n\n\nIsolation : Execution of one txn is isolated from that of other txns.\nDurability : If a txn commits, its effects persist.\n\n\n\nAtomicity §\n\nScenarios : We take $100 out of Andy’s account but then\n\nThe DBMS aborts the txn before we transfer it.\nThere is a power failure before we transfer it.\n\n\nApproaches\n\nLogging (WAL)\n\nLog all actions. Maintain undo records both in memory &amp; on disk.\nSystem needs to figure out what happened using the log.\nMost systems use this.\n\n\nShadow Paging\n\nMake copies of pages that txns can make changes to. Page is visible when txn commits.\nInstantaneous recovery w/ nothing to analyse since the DB is always pointing to the correct version.\nSystems using this: CouchDB, IBM System R, LMDB\nSidenote\n\nWas removed from IBM System R because of fragmentation (caused by invalidating multiple pages at once), difficulty in determining the latest version for some data at given time, garbage collecting the page copies, random reads compared to sequential access in WAL.\nSome DB in Puerto Rico implemented this back in 1970s since they had frequent power cuts &amp; the DB was easy to recover using shadow paging.\n\n\n\n\n\n\n\nConsistency §\nKinds\n\nDatabase Consistency\n\nDB accurately models real world &amp; follows integrity constraints (set by developer eg: unique constraint).\nTxns in future see effects of txns committed in the past inside of the DB.\n\nObvious for single node DBs. Instead think about different nodes of a distributed DB being consistent w/ each other once a txn is committed.\n\n\n\n\nTransaction Consistency\n\n“If the database is consistent before the transaction starts (running alone), it will also be consistent after.”\nSidenote: Will be revisited when teaching distributed DBs. Ignore for now.\n\n\n\nIsolation §\n\nUsers submit txns &amp; each txn should execute as if it was running by itself.\nDBMS achieves concurrency by interleaving the actions (read/writes) of txns.\nConcurrency Control Protocol : How the DBMS decided the proper interleaving of operations from multiple transactions.\nTwo categories: Pessimistic &amp; Optimistic\nAssume two a/c A &amp; B (with $1000 each initially)\n\nT1 : A transfers $100 to B.\nT2 : Both a/c get 6% interest credited.\n\n\n\nT1\nA = A - 100\nB = B + 100\n\nT2\nA = A * 1.06\nB = B * 1.06\n\nLegal Outcome\n\n(A+B) * 1.06 -&gt; $2120\nNet effect must be equivalent to T1 &amp; T2 running serially in some order.\nNo guarantee that T1 executes before T2 or vice versa if both are submitted together.\nSerial execution scenarios\n\nT1 happens first: A = 954, B = 1166\nT2 happens first: A = 960, B = 1160\nFrom DB’s perspective, both orders are correct.\n\n\nWhy interleave transactions?\n\nMaximize concurrency.\nAnother txn can continue executing if one txn stalls because of something like a page fault.\nSlow disk or network I/O isn’t necessarily of concern now (unlike the 1970s) but we want to take advantage of multi-core CPUs.\n\n\n\nInterleaving Example (Good)\n| T1          | T2           |\n|-------------|--------------|\n| BEGIN       |              |\n| A = A - 100 |              |\n|             | BEGIN        |\n|             | A = A * 1.06 |\n| B = B + 100 |              |\n| COMMIT      |              |\n|             | B = B * 1.06 |\n|             | COMMIT       |\n\n---\nA = 954, B = 1166\nA + B = 2120\n\nInterleaving Example (Bad)\n| T1          | T2           |\n|-------------|--------------|\n| BEGIN       |              |\n| A = A - 100 |              |\n|             | BEGIN        |\n|             | A = A * 1.06 |\n|             | B = B * 1.06 |\n|             | COMMIT       |\n| B = B + 100 |              |\n| COMMIT      |              |\n\n---\nA = 954, B = 1160\nA + B = 2114\nBank is missing $6 in this case.\n\n\nWe judge whether a schedule is correct by checking if it’s equivalent to some serial execution.\n\nFormal Properties of Schedules §\nSerial Schedule : Doesn’t interleave the actions of different txns.\nEquivalent Schedule : The effect of executing the first schedule is identical to the effect of executing the second schedule regardless of the arithmetic operations.\nSerializable Schedule : Equivalent to some serial execution of the transaction.\n\nSerializability is less intuitive notion of correctness but it provides more flexibility in scheduling operations which leads to better parallelism.\nSidenote:\n\nTxns can be executed in order their initiation time or commit order &amp; they’d be correct but this is obviously less flexible.\nStrong Serializability / Consistency (Arrival Order Execution) : Commit Order = Write Order\n\nEg: Google Spanner, FaunaDB. Most systems don’t do this though.\n\n\n\n\n\nTODO: Look up how things like RANDOM() work with transactions.\nConflicting Operations §\nWe need to identify if a schedule is serializable.\nTwo operations conflict if:\n\nThey’re by different transactions.\nThey’re on the same object &amp; one of them is a write.\n\nInterleaved Execution Anomalies\n\nRead-Write Conflicts (R-W)\n\nUnrepeatable Read : Txn gets different values when reading the same object multiple times.\n\n\nWrite-Read Conflicts (W-R)\n\nDirty Read: One txn reads data written by another txn that has not committed yet.\n\n\nWrite-Write Conflicts (W-W)\n\nLost Update: One txn overwrites uncommitted data from another uncommitted txn\n\n\nSidenote: Phantom Reads &amp; Write Skew are some other anomalies.\n\nNone of the below scenarios is equivalent to a serial order.\n\n// Unrepeatable Read\n\n| T1         | T2         |\n|------------|------------|\n| BEGIN      |            |\n| 10&lt;-R(A)   |            |\n|            | BEGIN      |\n|            | R(A)-&gt;10   |\n|            | W(A)&lt;-19   |\n|            | COMMIT     |\n| 19&lt;-R(A)   |            |\n| COMMIT     |            |\n\n\n// Dirty Read\n\n| T1         | T2              |\n|------------|-----------------|\n| BEGIN      |                 |\n| 10&lt;-R(A)   |                 |\n| 12-&gt;W(A)   | BEGIN           |\n|            | R(A)-&gt;12        |\n|            | W(A)&lt;-14 (add 2)|\n|            | COMMIT          |\n| ABORT      |                 |\n\n\n// Lost Update\n\n| T1         | T2         |\n|------------|------------|\n| BEGIN      |            |\n| 10-&gt;W(A)   |            |\n|            | BEGIN      |\n|            | W(A)&lt;-19   |\n|            | W(B)&lt;-YY   |\n|            | COMMIT     |\n| ZZ-&gt;W(B)   |            |\n| COMMIT     |            |\n\nLevels of Serializability\n\nConflict Serializability\n\nSupported by most DBMSs.\n\n\nView Serializability\n\nNo DBMS actually does this since it’s difficult to implement as it requires understanding of what the application actually wants to do with the data.\nAllows for more schedules than conflict serializability.\n\n\nNote: Definition of both levels don’t allow all serializable schedules.\nThere are other levels but mostly in academia.\n\nConflict Serializable Schedule §\n\nTwo schedules are conflict equivalent iff\n\nThey involve the same actions of the same transactions.\nEvery pair of conflicting actions is ordered the same way.\n\n\nA schedule S is conflict serializable if\n\nS is conflict equivalent to some serial schedule.\nIntuition: Transform S into a serial schedule by swapping consecutive non-conflicting operations of different txns.\n\n\nSidenote: Some systems like DynamoDB, FaunaDB require you to send the entire transaction at once but most systems don’t work that way. Here we’re considering the situation where we don’t know what comes after an operation in a particular txn.\nTODO: Add link with timestamp to this portion of the lecture.\n\nDependency Graphs / Precedence Graphs\n\nSwapping operations is cumbersome for more than 2 txns.\nCreating\n\nOne node per txn.\nEdge from Ti to Tj if:\n\nOperation Oi of Ti conflicts w/ Oj of Tj\nAND Oi appears earlier in the schedule than Oj.\n\n\n\n\nA schedule is conflict serializable iff its dependency graph is acyclic.\n\nDurability §\n\nAll changes of committed txns. should be persistent\n\nNo torn updates\nNo changes from failed txn.\n\n\nDBMS can use logging or shadow paging to ensure durability.\nWill be discussed later on.\n\n16 : Two-Phase Locking Concurrency Control §\n\nAgenda: Lock Types, Two-Phase Locking, Deadlock Detection + Prevention, Hierarchial Locking\nMotivation: Need a way to guarantee that all execution schedules are correct (i.e. serializable) without knowing the entire schedule ahead of time.\nLocks vs. Latches\nSolution: Use locks to protect database objects (eg: rows, tables, pages, attributes etc.).\nNote: We didn’t use a global lock manager for access in the BTree since it’s comparatively slower compared to using latches for that purpose.\n\nThe lock manager needs to have a global view to take corrective action on deadlocks &amp; decide which txn. to give the lock to basis their priority.\nIt also needs to make scheduling decisions for better resource usage.\nCost of updating a database object is way more than the cost of updating a BTree node.\n\n\nBasic Lock Types\n\nS-LOCK: shared for reads\nX-LOCK: exclusive for writes\n\n\nSidenote: Databases have more different lock types. See https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-TABLES for an example.\nProcess\n\nTransactions request locks (or upgrades).\nLock manager grants or blocks requests.\nTransactions release locks.\nLock manager updates its internal lock-table.\n\nLock-table keeps track on txns. holding locks &amp; txns. waiting to acquire any locks.\nDepending on how deadlocks are handles, there can be a background thread that can look for deadlocks &amp; pick one to kill OR there can be some ordering protocol that can help decide at the moment lock is acquired if there’ll be a deadlock &amp; kill the txn. there. (How does the ordering protocol work?)\n\n\n\n\nJust using locks doesn’t guarantee serializability. We need a way to specify how txn are going to acquire locks.\n\nTwo Phase Locking (2PL) §\n\nConcurrency Control Protocol that determines whether a txn can access an object in the database at runtime.\nPessimistic Protocol\n\nAssumption is that txns are going to have anomalies so txns. need to acquire locks on any object they want to use.\n\n\nDoesn’t need to know all the queries that a txn will execute ahead of time.\nPhases\n\n1 : Growing : Each txn. requests the lock it needs from the DBMS’s lock manager (which grants/denies lock requests).\n2 : Shrinking : The txn. is only allowed to release/downgrade locks that it previously acquired. It can’t acquire new locks. Txn. will be aborted &amp; rolled back if this happens.\n\nThe txn enters the shrinking phase when it decides to release a lock.\n\n\n\n\nNote: Typically you don’t write lock requests manually. The DBMS will handle it.\nIf the system doesn’t know the full schedule, how does the shrinking phase works?\n\nSystems can just release the locks at the end of the txn. but shrinking can be done during  the txn. too.\n\n\nSidenote\n\nMost systems run on a low isolation level, not serializable.\nYou can provide hints in SQL (like SELECT FOR UPDATE) that tell the DB that you’re going to write the data so it acquires an exclusive lock instead of the shared one. This saves an upgrade request.\n\n\n2PL is sufficient to guarantee conflict serializability but it’s subject to cascading aborts.\n\n// Cascading Abort\n\n\n| T1          | T2           |\n|-------------|--------------|\n| BEGIN       |              |\n| X-LOCK(A)   |              |\n| X-LOCK(B)   |              |\n| R(A)        |              |\n| W(A)        |              |\n| UNLOCK(A)   | BEGIN        |\n|             | X-LOCK(A)    |\n|             | R(A)         | \n|             | W(A)         |\n| R(B)        | .            |\n| W(B)        | .            |\n| .           | .            |\n| .           |              |\n| .           |              |\n| ABORT       |              |\n\n// Reading the write from T1 in T2 is okay since its equivalent to serial ordering.\n// But T2 will need to rolled back if T1 aborts.\n\n// Since T2 read data that was modified in T1, it&#039;ll need to wait until T1 commits or aborts.\n\n\nSome observations about 2PL\n\nThere are schedules that are serializable but wouldn’t be allowed by 2PL because locking limits concurrency.\nMay still have “dirty reads” if not using Strong Strict 2PL (aka Rigorous 2PL)\nMay lead to deadlocks (Solution: Detection or Prevention)\n\nNote:\n\nA schedule is strict if a value written by a txn. is not read or over-written by other txns until that txn finishes.\n\nDoesn’t incur cascading aborts.\nAborting txns just involves restoring original value of modified tuples.\n\n\n\nStrong Strict Two-Phase Locking §\n\nThe txn. is only allowed to release locks after it has ended.\nAllows only Conflict Serializable Schedule.\nStronger guarantee than needed for most apps.\nSidenote: There’s a version called Strict 2PL where you’re allowed to release the exclusive locks but not the shared locks.\n\n\nSerial ⊂ Strong Strict 2PL ⊂ Conflict Serializable ⊂ View Serializable ⊂ All Schedules\n\n2PL Deadlocks §\n\n\nA deadlock is a cycle of transactions waiting for locks to be released by each other.\n\n\nDetection\n\nLock managers creates a “waits-for graph” to keep track of what locks each txn is waiting to acquire.\n\nNodes are transactions.\nEdge from Ti to Tj if Ti is waiting for Tj to release a lock.\n\n\nThe system periodically checks for cycles in the graph &amp; decides how to break it.\nWhen the DBMS detects a deadlock, it’ll select a “victim” txn to rollback &amp; break the cycle.\n\nThe victim txn will either restart (rare, with stored procedures) or abort (more common) depending on how it was invoked.\nSome factors for victim selection\n\nBy age\nBy progress\nBy the # of items already locked\nBy the # of txns that we have to rollback with it\nThe # of times a txn has been restarted in the past\n\n\nRollback Length (how far to rollback the txn’s changes)\n\nCompletely : Rollback entire txn &amp; tell the application it was aborted.\nPartial (Savepoints) : Rollback a portion of a txn (to break deadlock) &amp; then re-attempt the undone queries.\n\n\n\n\n\n\n\nPrevention\n\nWhen a txn tries to acquire a lock that is held by another txn, the DBMS kills one of them to prevent a deadlock.\nDoesn’t require a “waits-for graph” or detection algorithm.\nPriorities are assigned based on (logical) timestamp (older timestamp = higher priority)\nWait-Die (“old waits for young”)\n\nIf requesting txn has higher priority than holding txn, then requesting txn waits for holding txn. Otherwise requesting txn aborts.\n\n\nWound-Wait (“young waits for old”)\n\nIf requesting txn has higher priority than holding txn, then holding txn aborts and releases lock. Otherwise requesting txn waits.\n\n\nWhen a txn restarts, it priority is its original timestamp to avoid it from getting starved.\n\n\n\nDeadlock Detection is more common than Deadlock Prevention.\n\n\nWhen a txn wants to acquire a lock, the DBMS can decide the granularity (/scope) (eg. Attribute, Tuple, Page, Table) of that lock.\n\n\nTrade-off: Parallelism vs Overhead of acquiring too many locks\n\nSay you want to change a lot of rows, acquiring a lock for each row would not be efficient.\n\n\n\nDatabase Lock Hierarchy Tree: Database (rare) -&gt; Table (very common) -&gt; Page (common) -&gt; Tuple (very common) -&gt; Attribute (rare)\n\nAttribute here isn’t a whole column but the specific column attribute for a tuple. (Yugabyte provides this functionality)\n\n\n\nIntention Locks\n\nAllow higher-level node to be locked in shared or exclusive mode without having to check all descendant nodes. If a node is locked in an intention mode, then some txn is doing explicit locking at a lower level in the hierarchy.\nTypes\n\nIntention Shared (IS) : indicates explicit locking at lower level with shared locks\nIntention Exclusive (IX) : indicates explicit locking at lower level with exclusive locks\nShared+Intention Exclusive (SIX) : subtree rooted by that node is locked explicitly in shared mode &amp; explicit locking is being done at a lower level with exclusive-mode locks.\n\n\n\n\n\n\nLocking Protocol\n\nEach txn obtains appropriate lock at highest level of the database hierarchy.\nTo get S or IS lock on a node, the txn must hold at least IS on parent node.\nTo get X, IX, or SIX on a node, the txn must hold at least IX on parent node.\n\n\n\nLock Escalation\n\nDBMS can switch to coarser-grained locks when a txn acquires too many low level locks.\nReduces the no. of requests that the lock manager must process.\nFor eg:\n\nA IS lock at parent table can be upgraded to a S (shared) lock if a txn is trying to read a lot of tuples.\n\n\n\n\n\n17 : Timestamp Ordering Concurrency Control §\n\n\nAgenda: Basic T/O Protocol, Optimistic Concurrency Control, Isolation Levels\n\n\n2PL (Pessimistic) -&gt; Determine serializability order of conflicting operations at runtime while txns execute.\n\n\nTimestamp Ordering (Optimistic) -&gt; Determine serializability order of txns before they execute.\n\nOptimistic : Not acquiring locks for txns.\n\n\n\nT/O Concurrency Control uses timestamps to determine the serializability order of txns.\n\nTransactions are assigned unique timestamps used to determine serial ordering.\n\n\n\nTransaction -&gt; Ti\n\n\nTimestamp for Ti -&gt; TS(Ti)\n\n\nTimestamps are assigned at different times during the txn. in different schemes:\n\nSome at start, others at end of txn. MVCC assigns one at start and another at end.\n\n\n\nImplementation\n\nSystem Clock\nLogical Counter\nHybrid (common in distributed systems)\n\n\n\nIf TS(Ti) &lt; TS(Tj) : DBMS must ensure that the execution schedule is equivalent to a serial schedule where Ti appears before Tj\n\n\nTimestamp Ordering Protocols\n\nBasic Timestamp Ordering (T/O) Protocol\n\nThis is also optimistic.\n\n\nOptimistic Concurrency Control\n\nBasic Timestamp Ordering (Basic T/O) §\n\nEvery txn. has a single timestamp assigned when in starts.\nTxns read and write objects without locks.\nEvery object X is tagged w/ timestamp of the last txn. that successfully did read/write:\n\nW-TS(X) - Write timestamp on X\nR-TS(X) - Read timestamp on X\n\n\nIf a txn. tries to access an object “from the future” (tagged w/ a later timestamp than itself), it aborts and restarts.\n\nPhysical ordering can be out of order. Logical ordering must be correct.\n\n\nReads\n\nif TS(Ti) &lt; W-TS(X) :\n\nviolates T/O of Ti w/ regard to the writer of X, abort and restart\n\n\nelse:\n\nallow Ti to read X &amp; update R-TS(X) to max(R-TS(X), TS(Ti))\n(Optional) Make a local copy of X to ensure repeatable reads for Ti\n\n\n\n\nWrites\n\nif TS(Ti) &lt; R-TS(X) or TS(Ti) &lt; W-TS(X):\n\nabort and restart Ti\n\n\nelse:\n\nallow Ti to write X &amp; update W-TS(X)\n(Optional) Make a local copy of X to ensure repeatable reads for Ti\n\n\n\n\nThomas Write Rule\n\nIf TS(Ti) &lt; R-TS(X)\n\nabort &amp; restart Ti\n\n\nIf TS(Ti) &lt; W-TS(X):\n\nignore the write to allow the txn to continue executing w/o aborting\nviolates T/O of Ti\n\n\nElse:\n\nallow Ti to write X &amp; update W-TS(X)\n\n\nEg: Tj (where Tj &gt; Ti) wrote on some object A then the write from Ti doesn’t really matter since the object was going to be written by Tj anyway.\n\nNote: Ti would still be reading its own write (if there’s a read after this write) but not aborting or updating the W-TS(X) timestamp.\n\n\n\n\nBasic T/O generates a schedule that is conflict serializable if you don’t use the Thomas Write Rule.\n\nNo deadlocks because no txn. ever waits.\nIssues\n\nHigh overhead from copying data to txn’s workspace &amp; from updating timestamps.\n\nEvery read requires the txn to write to the database.\n\n\nLong running txns can get started. Favors newer txn. since older txn. would violate the checks more frequently.\n\n\n\n\n\nOptimistic Concurrency Control (OCC) §\n\nAssumption: Conflicts b/w txns are rare &amp; most txns are short-lived.\n\nForcing txns to acquire locks or update timestamps adds unnecessary overhead.\nIt’d be better to optimize for the no-conflict case.\n\n\nDBMS creates a private workspace for each txn.\n\nObject read is copied into workspace.\nModifications are applied to workspace.\n\n\nWhen a txn commits, DBMS compares workspace write set to check conflicts with other txns.\n\nIf there aren’t any conflicts, the write set is installed into the global database.\n\n\nPhases\n\nRead Phase : track read/write sets of txns and store in private workspace\nValidation Phase : on txn commit, check for conflict w/ other txns\n\nYou get a timestamp for the txn at commit\nChecks other txns. for RW &amp; WW conflicts &amp; ensure that conflicts are in one direction\napproaches:\n\nBackward Validation\n\ncheck whether the committing txn intersects its read/write sets w/ those of any txns. in the schedule that’ve already committed\n\n\nForward Validation\n\ncheck whether the committing txn intersects its read/write sets w/ any active txns. haven’t yet committed\n\n\n\n\n\n\nWrite Phase : if validation succeeds, apply private changes to database, otherwise abort and restart\n\n\n\nValidation Phase §\n\nYou get a timestamp for the txn at commit\nChecks other txns. for RW &amp; WW conflicts &amp; ensure that conflicts are in one direction\nApproaches:\n\nBackward Validation : check whether the committing txn intersects its read/write sets w/ those of any txns. in the schedule that’ve already committed\nForward Validation : check whether the committing txn intersects its read/write sets w/ any active txns. haven’t yet committed\n\n\nForward Validation\n\nIf TS(Ti) &lt; TS(Tj), then one of the following must hold:\n\nTi completes all 3 phases before Tj begins execution\nTi completes before Tj starts its Write phase &amp; Ti doesn’t write to any object read by Tj\n\nWriteSet(Ti) ∩ ReadSet(Tj) = Ø\n\n\nTi completes its Read phase before Tj completes its Read phase &amp; Ti doesn’t write to any object that is either read or written by Tj\n\nWriteSet(Ti) ∩ ReadSet(Tj)  = Ø\nWriteSet(Ti) ∩ WriteSet(Tj) = Ø\n\n\n\n\n\n\n\nWrite Phase §\n\n\nPropagate changes in the txn’s write set to database to make them visible to other txns.\n\n\nSerial Commits\n\nUse a global latch to limit a single txn. to be in the Validation/Write phases at a time.\n\n\n\nParallel Commits\n\nUse fine-grained write latches to support parallel Validation/Write phases.\nAvoiding deadlocks:  Acquire latches in primary key order so all threads are acquiring latches in the same direction.\n\n\n\nOCC works well when the no. of conflicts is low.\n\nMost txns. are read-only.\nTxns. access disjoint subsets of data.\n\n\n\nIssues\n\nHigh overhead for copying data locally.\nValidation/Write phase bottlenecks.\nAborts are more wasteful than in 2PL because they only occur after a txn has already executed.\n\nFor eg: if a txn. updates a large no. of tuples and then finds out that it has a conflict then it has to rollback all its changes\n\n\n\n\n\nDynamic Databases §\nTill now, we’ve only dealt with txns that read and update existing objects in the database. We didn’t consider insertion and deletion which brings up new problems.\nThe Phantom Problem §\nTuples can appear or disappear while a txn. is running.\n    T1                |     T2\nBEGIN                 | BEGIN\n                      |\nSELECT COUNT(age)     |\n FROM people          | -&gt; 99\n WHERE status = &#039;lit&#039; |\n                      | INSERT INTO people\n\t\t\t\t\t  | (age = 30, status = &#039;lit&#039;)\n                      | COMMIT\nSELECT COUNT(age)     |\n FROM people          | -&gt; 100\n WHERE status = &#039;lit&#039; |\n                      |\nCOMMIT                |\n\n\nT1 locked only existing records and not ones under way.\nConflict serializability on reads and writes of individual items guarantees serializability only if the set of objects is fixed.\n\nApproaches to fix:\n\nRe-Execute Scans\n\nRun queries again at commit to see whether they produce a different result to identify missed changes.\nSimplest. Mostly implemented in in-memory systems since we don’t want to read from disk again. Most systems don’t do this because the scan set could be really big.\nWorking\n\nDBMS tracks the “WHERE” clause for all queries that the txn. executes.\n\nRetain the scan set\n\n\nUpon commit, re-execute the scan portion for each query and check whether it generates the same result.\n\nEg: For an “UPDATE” query, don’t modify matching tuples but just find them again.\n\n\n\n\nEg: DynamoDB, FaunaDB, Hekaton\n\n\nPredicate Locking\n\nLogically determine the overlap of predicates before queries start running.\nRarely implemented (see HyPer precision locking). Mostly exists in literature.\nWorking\n\nIn a “WHERE” clause of\n\n“SELECT” query -&gt; shared lock on predicate\n“UPDATE”, “INSERT”, “DELETE” query -&gt; exclusive lock on predicate\n\n\n\n\nEg:\nSELECT COUNT(age) FROM people WHERE status = &#039;lit&#039; &amp; INSERT INTO people VALUES(age = 30, status = &#039;lit&#039;)\n\nRecords in table “people”: age = 30 ^ status = &#039;lit&#039; ⊆ status = &#039;lit&#039;\n\n\n\n\nIndex Locking\n\nUse keys in indexes to protect ranges.\nMost commonly implemented.\nSchemes\n\n? Key-Value Locks\n\nLocks that cover a single key-value in an index.\nNeed “virtual keys” for non-existent values.\nSee https://www.ibm.com/docs/en/informix-servers/12.10?topic=locks-key-value\n\n\nGap Locks\n\nEach txn. acquires a key-value lock on the single key it wants to access. &amp; then gets a gap lock on next key gap.\n\n\nKey-Range Locks\n\nTxn takes locks on ranges in the key space.\n\nEach range is from one key that appears in the relation to the next that appears.\n\n\nLocks that cover a key value and the gap to the next key value in a single index.\nNeed “virtual keys” for artificial values (infinity)\n\n\nHierarchical Locking\n\nSimilar to 2PL, take intention locks at broader ranges and narrow down from there.\nAllow for txn. to hold wider key-range locks with different locking modes.\n\n\n\n\n\n\n\nIsolation Levels §\n\nMost systems won’t have serializability by default but use a lower level of isolation for better performance.\n\nIsolation levels control the extent that a txn. is exposed to the action of other concurrent txns.\nWeaker isolation levels provide greater concurrency at the cost of exposing txns. to uncommitted changes:\n\nDirty Reads\nUnrepeatable Reads\nPhantom Reads\n\nIsolation Levels as per ANSI SQL standard:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNamePhantom ReadsUnrepeatable ReadsDirty ReadsSERIALIZABLE✗✗✗REPEATABLE READS✓✗✗READ COMMITTED✓✓✗READ UNCOMMITTED✓✓✓\nImplementation\nSERIALIZABLE: Obtain all locks + index locks + strict 2PL\nREPEATABLE READS : Same as above but no index locks\nREAD COMMITTED : Same as above but S locks are released immediately\nREAD UNCOMMITTED : Same as above but no S locks (dirty reads)\nThere are some other txn. isolation levels too (not in the standard):\n\nSTRONG SERIALIZABLE (Google Spanner)\nSNAPSHOT ISOLATION (Yugabyte, Oracle)\nCURSOR STABILITY (IBM DB2)\n\n18 : Multi-Version Concurrency Control §\n\n\nDBMS maintains multiple physical versions of a single logical object in the database. When a txn:\n\nwrites to an object: the DBMS creates a new version of that object\nreads an object: it reads the newest version that existed when the txn started or just the latest version depending upon isolation level\n\n\n\nSidenote:\n\nMVCC protocol was first proposed in 1978.\nFirst implementation was Rdb/VMS (now Oracle Rdb) &amp; Interbase (open sourced as Firebird) in early 1980s by Jim Starkey.\n\n\n\nWriters don’t block readers.\n\n\nReaders don’t block writers.\n\n\nRead-only txns can read a consistent snapshot without acquiring locks.\n\n\nEasily support time-travel queries.\n\n\nSimilar to OCC but has a global workspace instead of the private workspace for each txn. in OCC.\n\n\nYou can keep track of additional metadata that tracks the txn status to keep track of txns. that’ve committed. This can be tracked separately in a txn status table or in the tuple itself.\n\npg stores the status in the tuple itself.\n\n\n\nThe version a txn. is going to read will depend upon the isolation level.\n\n\nSnapshot Isolation (SI)\n\nGuaranteed repeatable reads, Phantom reads possible, Won’t read uncommitted data\nWhen a txn. starts, it sees a consistent snapshot of the database that existed when that txn. started.\n\nNo torn writes from active txns (i.e. you see all or nothing)\n\ntorn writes : partial writes\n\n\nFirst writer winds if 2 txn. update the same object.\n\n\nSI can have Write Skew Anomaly.\n\nWrite Skew Anomaly\n\nEg:\n\nThere’s a set of white &amp; black marbles.\nT1: Change white marbles to black.\nT2: Change black marbles to white.\nIn serial order, the marbles should be all white or black.\nBut in SI, since we didn’t take any write locks there wasn’t any conflict b/w the txns. we’d end up with an incorrect state.\n\n\n\nApproaches for implementing MVCC\n\nTimestamp Ordering\n\nAssign txns. timestamp that determine serial order.\n\n\nOptimistic Concurrency Control\n\nUse private workspace for new versions.\n\n\nTwo-Phase Locking (2PL)\n\nTxns. acquire appropriate lock on physical version before they can read/write a logical tuple.\n\n\n\nVersion Storage §\n\nDBMS uses the tuples’ pointer field to create a version chain per logical tuple.\n\nAllows DBMS to find the version that is visible to a particular txn. at runtime.\nIndexes always point to the head of the version chain.\n\n\nStorage Schemes\n\nAppend-Only Storage\n\nNew version appended to the same table space.\nUsed by: Postgres\nVersion Chain Ordering:\n\nOldest-to-Newest (O2N) : If the head points to the older version, you need to traverse the “chain” all the way to get to the latest version.\nNewest-to-Oldest (N2O) : If the head points to the latest version, you’ll need to update all the indexes with that tuple to point to the latest version for every new version.\n\n\n\n\nTime-Travel Storage\n\nOld version copied to separate table space on an update.\nUsed by: SQL Server\n\n\nDelta Storage\n\nOriginal values of the modified attributes are copied into a separate delta record space &amp; “master” version in over-written.\nTxns can recreate old versions by applying the delta in reverse order.\n(Kinda like git diff)\nRecommended scheme by Andy\nUsed by: MySQL, Oracle and a lot of other DBs.\n\n\n\n\nGarbage Collection\n\nDBMS needs to remove reclaimable physical versions from the DB over time.\n\nreclaimable : no active txn in the DBMS can see that version OR version created by aborted txn.\n\n\nDesign Decisions\n\nHow to look for expired versions?\nHow to decide when its safe to reclaim memory?\n\n\nApproaches\n\nTuple-level\n\nFind old versions by examining tuples directly\nSidenote: We can have some hints to point to places that can be looked up for reclaimable versions.\nBackground Vacuuming\n\nSeparate threads periodically scan the table and look for reclaimable versions.\nThreads get their threshold from the smallest timestamp of any active txn.\nA bitmap can be maintained of “dirty” blocks for faster &amp; smaller lookups. (not completely accurate)\n\n\nCooperative Cleaning\n\nWorker threads find reclaimable versions as they traverse the version chain. Only works with O2N.\nTrade-off: If I never read a particular set tuple then their older versions are not going to be reclaimed.\n\n\n\n\nTransaction-level\n\nTxns keep track of their old versions.\nTrade-off: For a large no. of tuples updated, a lot of data needs to be maintained and passed on to some background thread for recovery.\n\n\n\n\n\n\nIndex Management\n\nPrimary key points to head of version chain.\n\nIf a txn updates a tuple’s pkey attribute(s), then this is treated as a DELETE followed by an INSERT.\n\n\nSecondary index updates are more complicated. Approaches:\n\nLogical Pointers\n\nuse a fixed id (like primary key or tuple id) per tuple that doesn’t change\nrequires extra indirection layer\nfor eg: mysql has the primary key in the secondary indexes which can be used to get the latest record id using the primary key index\n\n\nPhysical Pointers\n\nuse the physical address to the version chain head\n\n\n\n\n\n\nSkipped in Lecture\n\nMVCC Indexes\n\nMVCC Duplicate Key Problems\n\n\nMVCC Deletes\n\n\n\n19 : Database Logging §\nAgenda: Failure Classification, Buffer Pool Policies, Shadow Paging, Write-Ahead Log, Logging Schemes, Checkpoints\nParts of recovery algorithms:\n\nActions during normal txn processing to ensure. DBMS can recover from a failure.\nActions after a failure to recover the database to a state that ensures atomicity, consistency, and durability.\n\nStorage Types\n\nVolatile Storage (eg. DRAM, SRAM)\nNon-volatile storage (egL HDD, SDD)\nStable storage\n\ntheoretical non-volatile storage that survives all possible failure scenarios\n\n\n\nFailure Classification\n\nTransaction Failures\n\nLogical Errors\n\ntxn can’t complete due to some internal error condition like integrity constraint violation\neg: inserting duplicate into a unique value column\n\n\nInternal State Errors\n\nactive txn. needs to be terminated due to an error condition like deadlock in 2PL\n\n\n\n\nSystem Failures\n\nSoftware Failure\n\nissue w/ OS or DBMS implementation (eg. uncaught div-by-0 exception)\n\n\nHardware Failure\n\nhost computer crashes (eg. power get cut abruptly)\nassumption: non-volatile storage contents are assumed to not be corrupted by system crash\n\n\n\n\nStorage Media Failures\n\nNon-Repairable Hardware Failure\n\nDestruction is assumed to be detectable (eg. through checksums)\nDBMS can’t recover from this.\n\n\n\n\n\nDBMS needs to ensure\n\nChanges for txn are durable once the DBMS has reported the txn. as committed\nNo partial changes are durable if the txn. aborted.\n\nUndo vs. Redo\nUndo: removing the effects of incomplete or aborted txn.\nRedo: re-applying the effects of committed txn. for durability\nImplementation depends on buffer pool manager.\nSituation:\n2 txns. T1 &amp; T2 modify values in the same page. T2 completes successfully while T1 aborts.\nDo we force T2’s changes to be written to disk?\nIs T1 allowed to over-write the in-memory page even though it hasn’t committed?\nWhat happens when we need to rollback T1?\nSteal Policy : DBMS allows an uncommitted txn to overwrite the most recent committed value of an object in non-volatile storage.\nForce Policy :  DBMS requires that all updates made by a txn are reflected on non-volatile storage before the txn can commit.\nNO-STEAL + FORCE\n\nMake a copy of the page on commit in buffer pool to only write the committed values to non-volatile storage.\nIssue: Txn. can’t modify a portion of DB that exceeds the memory available in the buffer pool.\nEasiest to implement.\n\nDon’t need to undo changes of aborted txn because they weren’t written to disk.\nDon’t need to redo changes of a committed txn because all the changes are written at commit time (assuming atomic hardware writes)\n\n\n\nShadow Paging (an implementation of NO-STEAL + FORCE)\n\nInstead of copying the entire DB, the DBMS copies pages on write to create 2 versions:\n\nMaster : only contains changes from committed txns.\nShadow : temp. DB w/ changes made from uncommitted txns.\n\n\nThe root points to the master version.\nTo install updates, overwrite the root to point to the shadow copy (once it’s durable)\n\nThis fixes the problem w/ torn writes in case multiple pages are updated in a txn.\n\n\nSidenote: In MVCC we made copies at the tuple level. Here we’re making them at the page level.\nUndo: Remove the shadow pages. Leave the master &amp; the DB root pointer alone.\nRedo: Not needed.\nIssues\n\nCopying the entire page table is expensive.\n\nCan avoid this by using a page table structured like a B+Tree like LMDB does. Only need to copy paths in the tree that lead to updated leaf nodes.\n\n\nCommit overhead is high.\n\nFlushing every updated page, page table, root.\n\n\nPerforming writes to random non-contiguous pages on disk leads to data fragmentation (i.e. pages won’t be sequential after updates so sequential scans would be slow)\nNeeds garbage collection\nOnly supports 1 writer txn at a time or txns in a batch (no way to separate changes of 1 txn from other which can lead to issue when a txn. commits but other one aborts)\n\n\n\nSQLite (Pre-2010)\n\nRollback Mode: When a txn modifies a page, the DBMS copies the original page to a separate journal file before over-writing master version.\nAfter restarting, if a journal file exists, then the DBMS restores it to undo changes from uncommitted txns.\nJournal files are deleted only after a txn is committed OR the changes of an uncommitted txn have been reversed.\n\nWrite-Ahead Log §\n\nMaintain a log file separate from data files that contains the changes that txns make to DB.\n\nAssumption: Log is on stable storage.\nLog contains enough info. to perform the undo, redo actions to restore the DB.\n\n\nDBMS must write to disk the log file records that correspond to changes made to a DB object before it can flush that object to disk.\n\nSidenote: Some DBs don’t wait for the WAL to be fsynced.\n\n\nBuffer Pool Policy: STEAL + NO-FORCE\n\nWAL Protocol\n\nWrite a BEGIN record to the log for each txn to mark its starting point.\n\nMost systems write a BEGIN record when a query that modifies the DB is issued.\n\n\nWhen a txn finishes:\n\nWrite a COMMIT record on the log.\nMake sure that all log records are flushed before it returns an acknowledgement to application.\n\n\nEach log entry contains info. about the change to a single object. Some fields:\n\nTxn. Id\nObject Id\nBefore Value (for Undo)\n\nnot needed if doing append-only MVCC like pg\n\n\nAfter Value (for Redo)\n\n\n\nWAL Implementation\n\nFlushing the log buffer to disk every time a txn commits will become a bottleneck.\nDBMS can use the group commit optimization to batch multiple log flushes together to amortize overhead.\n\nFlush on full buffer or timeout.\n\n\n\n\nAlmost every DBMS uses NO-FORCE + STEAL\n\nLogging Schemes\n\nPhysical Logging\n\nrecord byte level changes made to specific page\n\n\nLogical Logging\n\nrecord high-level operations executed by txn\nrequires less data written in each record compared to physical logging\ndifficult to implement recovery w/ logical logging if you’ve concurrent txns running at lower isolation levels since its hard to determine what parts of the DB may have been modified before crash\ntakes longer to recover since queries need to be re-executed\nvery few systems do this\n\n\nPhysiological Logging\n\nrecord hybrid approach w/ byte-level changes for a single tuple identified by page id + slot no\ndoesn’t specify organization of page\n\n\n\n// UPDATE foo SET val = XYZ WHERE id = 1;\n\n// Physical\n&lt;T1,\nTable=X,\nPage=99,\nOffset=1024,\nBefore=ABC,\nAfter=XYZ&gt;\n&lt;T1,\nIndex=X_PKEY,\nPage=45,\nOffset=9,\nKey=(1,Record1)&gt;\n\n// Logical\n&lt;T1,\nQuery=&quot;UPDATE foo\nSET val=XYZ\nWHERE id=1&quot;&gt;\n\n// Physiological\n&lt;T1,\nTable=X,\nPage=99,\nSlot=1,\nBefore=ABC,\nAfter=XYZ&gt;\n&lt;T1,\nIndex=X_PKEY,\nIndexPage=45,\nKey=(1,Record1)&gt;\n\nLog Structured Systems\n\nDon’t have dirty pages. Any page retrieved from disk is immutable.\nDBMS buffers log records in in-memory pages (MemTable).  Must be flushed if full. May contain changes from uncommitted txns.\nMaintain a separate WAL to recreate the MemTable on crash.\n\nCheckpoints\n\nFlush all buffers (even with uncommitted changes) to disk periodically so that you don’t need to replay the entire log on recovery.\nAdd a checkpoint to indicate that anything modified before has been written to disk.\n\nNaive Blocking / Consistent Checkpoint Protocol\n\nProtocol\n\nPause all queries\nFlush all WAL records in memory to disk\nFlush all modified pages in the buffer pool to disk\nWrite a CHECKPOINT entry to WAL &amp; flush to disk\nResume queries\n\n\nIssues\n\nDBMS must stall txns when it takes a checkpoint to ensure a consistent snapshot.\nScanning the log to find uncommitted txns can take a long time.\n\n\n\nFrequency of Checkpoints\n\nToo often cause runtime performance to degrade.\nBut waiting a long time will make recovery time long.\n\nConclusion\n\nWAL is almost always the best approach to handle loss of volatile storage.\nUse incremental updates (STEAL + NO-FORCE) w/ checkpoints.\nOn recovery: undo uncommitted txns + redo committed txns.\n\n20 : Database Recovery §\nAgenda: Log Sequence Numbers, Normal Commit &amp; Abort Operations, Fuzzy Checkpointing, Recovery Algorithm\nActions after a failure to recover the database to a state that ensures atomicity, consistency, and durability.\nARIES (Algorithms for Recovery and Isolation Exploiting Semantics)\n\nDeveloped at IBM Research in early 1990s for DB2\nPaper: https://dl.acm.org/doi/pdf/10.1145/128765.128770\nMost DBMS use some variant of this.\n\nARIES - Main Ideas\n\nWrite-Ahead Log\n\nAny change is recorded in log on stable storage before the DB change is written to disk.\nMust use STEAL + NO-FORCE buffer pool policies\n\nSTEAL: allowed to write dirty pages from the buffer pool before txn. is allowed to commit (but we’ve to write the log entry first)\nNO-FORCE: we don’t need the DB to flush out all the dirty pages for a txn. when it commits (but log records need to be flushed out)\n\n\nSidenote: Why can’t we use STEAL + FORCE? You can. You’d just need to flush out the dirty pages too which can be slow. But it’s correct.\n\n\nRepeating History During Redo\nLogging Changes During Undo\n\nWe’ll add new log entries to list the changes we’re reversing during recovery to ensure that we can stay in a consistent state even if we repeatedly crash during recovery.\n\n\n\nWAL Records\n\nEvery log record now includes a globally unique log sequence number (LSN).\n\nLSNs represent the physical order that txns make changes to the DB.\n\n\nDifferent parts in the system keep track of LSNs related to them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameLocationDefinitionflushedLSNMemoryLast LSN in log on diskUpdated when WAL buffer is written to diskpageLSNpageNewest update to pageUpdated every time a txn modifies a record in the pagerecLSNpageOldest update to page since it was last flushedlastLSNTiLatest record of txn TiMasterRecordDiskLSN of latest checkpoint\n\npageLSN, recLSN are stored in page header.\nBefore the DBMS can write page x to disk, it must flush the log at least to the point where: pageLSNx &lt;= flushedLSN\n\nNormal Execution\n\nEach txn invokes a sequence of reads and writes followed by commit or abort\nAssumptions:\n\nAll log records fit within single page\nDisk writes are atomic\nSingle versioned tuples w/ Strong Strict 2PL\nSTEAL + NO-FORCE buffer management w/ WAL\n\n\n\nTxn Commit §\n\nWhen a txn commits, DBMS writes a COMMIT record to log &amp; guarantees that all log records upto txn’s COMMIT record are flushed to disk.\n\nLog flushes are sequential sync. writes to disk.\nMultiple log records per page.\n\n\nWhen the commit succeeds, write a special TXN-END record to log.\n\nIndicates that no new log record for a txn. will appear in the log.\nDoesn’t need to be flushed immediately.\nWhen you add this record depends on how you’re doing concurrency control, validation etc. &amp; you might need to wait for other txns.\n\n\nSidenote: Some systems do async commit &amp; just report that txn have been committed &amp; flush the log records at a later point.\n\nTxn Abort §\n\nSpecial case of the ARIES undo operation applied to only one txn.\nAdd another field to log records:\n\nprevLSN : previous LSN for the txn.\nThis helps maintain a (logical) linked-list for each txn making it easy to walk through its records &amp; find the list of operations applied for a txn to determine what needs to be reversed.\n\n\nWhen a txn aborts, write an ABORT record to log.\nThen, analyze the txn’s update in reverse order &amp; for each update record:\n\nWrite a CLR entry to the log.\nRestore old value.\n\n\nFinally, write a TXN-END record &amp; release locks.\n\nCompensation Log Records (CLR)\n\nA CLR describes the actions taken to undo the actions of a previous update record.\nHas all the fields of an update log record + the undoNext pointer\n\nundoNext -&gt; next to be undone LSN\n\n\nDBMS doesn’t wait for CLRs to be flushed to report that txn aborted.\n\nEven though the changes might not have been reversed yet, concurrency control schemes like 2PL or MVCC will ensure that reads aren’t incorrect for other txns.\n\n\n\nThe Log Record looks somewhat like this now:\n&lt;LSN | prevLSN | TxnId | Type | Object | Before | After | UndoNext&gt;\nEg:\n&lt;001 | nil | T1 | BEGIN   | - | -  | -  |  -  &gt;\n&lt;002 | 001 | T1 | UPDATE  | A | 30 | 40 |  -  &gt;\n&lt;026 | 011 | T1 | CLR-002 | A | 40 | 30 | 001 &gt;\n\nSidenote:\n\nCLR makes things more efficient. It isn’t necessary for recovery. It prevents us from doing things repeatedly.\npg writes the entire page to WAL on modification by the first txn. that brought the page to memory. This can help in recovery if table data gets corrupted in DB.\nCLRs never need to be undone.\n\nCheckpointing §\nNon-Fuzzy Checkpoints\n\nDBMS halts everything when it takes a checkpoint to ensure a consistent snapshot.\n\nHalt the start of any new txns.\nWait until all active txns finish executing.\nFlushed dirty pages on disk.\n\n\nBad for runtime performance but makes recovery easy.\n\nEg: If a txn is long running, we’ll need to wait for the txn to finish to take the checkpoint.\nSome systems (like embedded devices) work this way since the assumption is that txn is short.\n\n\n\nBetter Checkpoints\n\nPause modifying txns while the DBMS takes the checkpoint.\n\nPrevent queries from acquiring write latches on table/index pages.\n\n\nWe’ll have inconsistent checkpoints (eg. w/ a torn update). To prevent this, we must record internal state of the beginning of the checkpoint:\n\nActive Transaction Table (ATT)\nDirty Page Table (DPT)\n\n\nActive Transaction Table\n\nOne entry per currently active txn with:\n\ntxnId\nstatus: current “mode” of the txn. can be:\n\nR -&gt; Running\nC -&gt; Committing\nU -&gt; Candidate for undo (default when doing recovery)\n\n\nlastLSN: most recent LSN created by txn\n\n\nRemove entry after the TXN-END record\n\n\nDirty Page Table\n\nOne entry per dirty page in the buffer pool:\n\nrecLSN: LSN of log record that first caused the page to be dirty\n\n\n\n\nIssue\n\nWe’re still stalling txn when we take the checkpoint.\nWe’re assuming the flushing out pages is atomic. (We might have a lot of data to be flushed out. We know that most hardware only support atomic writes upto 4KB)\n\n\n\nFuzzy Checkpoints §\n\nDBMS allows active txns to continue the run while the system writes the log records for the checkpoint.\n\nNo attempt to force dirty pages to disk.\n\n\nIdea: Tables are only accurate at the moment the checkpoint started but we can’t guarantee that dirty pages modified after have been written.\nNew log records to track checkpoint boundaries:\n\nCHECKPOINT-BEGIN: indicates start of checkpoint\nCHECKPOINT-END: contains ATT + DPT\n\nany txn that begins after the checkpoint starts is excluded from ATT\n\n\n\n\nLSN of CHECKPOINT-BEGIN record is written to the MasterRecord when it completes.\n\nARIES - Recovery Phase §\n\n\nPhase 1 : Analysis\n\nExamine the WAL in forward direction starting from last BEGIN-CHECKPOINT found via MasterRecord till the end to identify dirty pages &amp; active txns at the time of the crash.\nProcess\n\nScan log fwd. from last successful checkpoint.\nIf the DBMS find a TXN-END record, remove its corresponding txn from ATT.\nAll other records:\n\nIf txn not in ATT, add it with default status UNDO\nOn coming across commit for a txn, change txn status to COMMIT\nSidenote: During recovery txn is not going to be in Running mode\n\n\nFor update log records:\n\nIf page P not in DPT, add P to DPT, set its recLSN = LSN\n\n\n\n\n\n\n\nPhase 2 : Redo\n\nRepeat all actions starting from an appropriate point in the log.\n\nall includes txns that will abort\n\n\nJump to smallest recLSN in DPT identified after Analysis\nProcess\n\nScan fwd. from the log record containing recLSN in DPT.\nFor each “update log record” or CLR with a given LSN, redo the action unless:\n\nAffected page is not in DPT (it got flushed to disk already)\nAffected page is in DPT but hat record’s LSN &lt; page’s recLSN (changes have been applied already )\n\n\nTo redo an action\n\nReapply logged update\nSet pageLSN to log record’s LSN\nNo additional logging, no forced flushes. Dirty pages in buffer pool can be flushed at end.\n\n\nAt end of Redo Phase, write TXN-END log records for all txns with status C &amp; remove them from the ATT\n\n\n\n\n\nPhase 3 : Undo\n\nReverse the actions of txns that didn’t commit before the crash. (All the txns with U status in the ATT after the Analysis Phase)\nGo back in the log record to the oldest log record of oldest txn active at crash.\nProcess them in reverse LSN order using lastLSN to speed up traversal.\nWrite a CLR for every modification.\n\n\n\nIf we crash during Analysis or Redo phase, we just need to run them again.\n\n\nPerformance Optimizations\n\nRedo Phase\n\nAssume that DBMS isn’t going to crash again &amp; flush all changes to disk async. in background\n\n\nUndo Phase\n\nMaintain log of changes that we need to reverse for pages in memory. We can apply the changes lazily before a txn tries to read that page.\n\n\n\n\n\nConclusion §\nMain Ideas in ARIES\n\nWAL w/ STEAL + NO-FORCE\nFuzzy Checkpoints (snapshot of dirty page ids)\nRedo everything since the earliest dirty page\nUndo txns that never commit\nWrite CLRs when undoing, to survive failures during restarts\n\n21: Introduction to Distributed Databases §\nAgenda: System Architecture, Design Issues, Partitioning Schemes, Distributed Concurrency Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel DBMSsDistributed DBMSsNodes are physically closeNodes can be farNodes connected w/ high-speed LANNodes connected using public networkCommunication cost is assumed to be smallCommunication cost and problems can’t be ignored\nSystem Architecture\nA distributed DBMS’s system architecture specified what shared resources are directly accessible to CPUs.\nKinds\n\nShared Everything\n\nDisk, Memory, CPU are local to each other.\nMessages b/w two queries are passed in memory.\nNote: There seems to be some difference in the terminology in the lecture &amp; what’s available online. A “shared everything” architecture in some places online is servers accessing common resources while in the lecture, shared everything refers to a non-distributed system.\n\nSee https://blogs.sap.com/2013/02/21/shared-everything-vs-shared-nothing-get-the-best-of-both-with-sap-sybase-iq-16/\n\n\n\n\nShared Memory\n\nCPUs aren’t co-located but Memory &amp; Disk are shared.\nCPUs have access to common memory address via a fast interconnect.\n\nNote: The interconnect is not guaranteed to be reliable.\n\n\nEach processor has a global view of all the in-memory data structures.\nEach DBMS instance on a processor must “know” about other instances.\nNo distributed DB implements this. HPC (High Performance Computing) applications that operate on TBs on data (stored locally) implement this.\nSidenote: see RDMA (remote direct memory access)\n\n\nShared Disk\n\nSeparate nodes with local CPU &amp; Memory but Disk is shared.\nAll CPUs can access a single logical disk directly via an interconnect but each have their own private memory.\nMessages are passed b/w nodes over network to learn about their current state.\nExecution layer can be scaled independently from the storage layer.\nEg: Snowflake, Druid, Spark, Dremio, Yugabyte, Spanner, Amazon Aurora\nSidenotes\n\nNodes can have locally attached storage but as a cache instead of the final state storage.\nSome systems split workload by key so that a specific node handles a particular record. Others just send messages to nodes whenever a change occurs so that their cache can be updated or invalidated.\nAmazon Aurora uses an interface over EBS that’s aware of replication, transaction etc. &amp; they can send messages at storage layer b/w different nodes.\n\n\n\n\nShared Nothing\n\nEvery node has its local CPU, Memory &amp; Disk.\nNodes only communicate via network.\nBetter performance &amp; efficiency\nIssues: Harder to scale, ensure consistency\n\nAdding new nodes can require physically migrating data.\n\n\nSidenote: Older systems used this. It’s easier to build Shared Disk systems w/ the cloud now using something like S3 (for analytical workloads) or EBS (for transactional workloads).\nEg: Cassandra, Redis, TiDB, SingleStore, MongoDB, CockroachLabs, Clickhouse, Citus, etcd\n\n\n\nSidenote: Early Distributed DBMS\n\nMuffin (1979) by Stonebraker (multiple Ingress nodes w/ a processing layer on top)\nSDD-1 by Bernstein\nSystem R by IBM (1984)\nGamma (1986)\nNonStop SQL (1987) - still running &amp; widely used (banks use it)\n\nDesign Issues\n\nHow does the application find data?\nWhere does the application send queries?\n\nSituation: Query goes to one node, data is is another.\n\n\nQuery Execution on Distributed Data\n\nPush query to data (Shared disk always uses this)\nPull data to query\n\n\nCorrectness\nDivision of DB across resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomogenous NodesHeterogenous NodesEvery node in the cluster can perform the same set of tasksNodes are assigned specific tasksMake provisioning &amp; failover easyCan allow a single physical node to host multiple virtual node types for dedicated tasks\nData Transparency : Applications shouldn’t be required to know where data is physically located in a distributed DBMS. In practice, developers need to be aware of the communication costs of queries to avoid expensive data movement.\nDatabase Partitioning\n\n\nSplit DB across multiple resources (disks, nodes, processors)\n\n\nDB executes query fragments on each partition &amp; then combines the results to produce a single answer.\n\n\nDBMS can partition DB physically (shared nothing) or logically (shared disk)\n\n\nNote: In general terminology online, partitioning is splitting data on a single server while sharding is splitting data across multiple servers.\n\n\nPartitioning Schemes\n\nNaive Table Partitioning\n\nAssign an entire table to a single node. Assume that each node has enough space for an entire table.\nIdeal if queries never join data across tables stored on different nodes &amp; access patterns are uniform.\nEg: MongoDB can do this. Most systems don’t.\n\n\nVertical Partitioning\n\nSplit a table’s attributes into separate partitions.\nMust store tuple info. to reconstruct the original record.\nSay we have a column w/ really large text fields occupying the space. We can separate it out &amp; keep it in another partition.\n\n\nHorizontal Partitioning\n\nSplit a table’s tuple into disjoint subsets based on some partitioning key and scheme.\nChoose column(s) that divide the DB equally in terms of size, load or usage.\nSchemes\n\nHashing\nRange\nPredicate\n\n\n\n\n\n\n\nKinds\n\nLogical Partitioning\n\nA node is responsible for a set of keys, but it doesn’t actually store those keys.\nThis is commonly used in a shared disk architecture.\n\n\nPhysical Partitioning\n\nA node is responsible for a set of keys, and it physically stores those keys.\nThis is commonly used in a shared nothing architecture.\n\n\n\n\n\nNote: If a node gets a request from the application requesting different keys than the ones its responsible for, then it makes a request to the peer node (instead of doing the round trip w/ the application) that’s responsible for those keys &amp; gets the result from there. It can also store a local copy.\n\n\nYou can use consistent hashing with a fixed replication factor so that each insert goes to multiple fixed number of adjacent nodes &amp; is replicated.\n\nTechnique was introduced by this paper: https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf\nDynamoDB, Memcached, Riak, Cassandra use this.\n\n\n\nScope of Txn.\n\nA single node txn only accesses data that is contained on 1 partition.\nA distributed txn accesses data at 1 or more partitions. (Requires expensive coordination)\n\n\n\nTransaction Coordination\n\nCentralized\n\nSidenote:\n\nTP Monitors\n\nCentralized coordinator for distributed DBMSs developed in 1970-80s to provide txns b/w terminals &amp; mainframe DBs. (Used by ATMs, airlines)\nCoordinator would hold locks similar to a Lock Manager in a DB.\nProcess\n\nApp sends Lock Request to Coordinator\nCoordinator grants the Lock Request\nApp sends requests to Partitions\nWhen app wants to commit, it sends a Commit Request to Coordinator\nCoordinator checks w/ the partition nodes &amp; makes them commit\n\n\n\n\nNowadays\n\nThere’s usually a middleware component is b/w which receives the request from the application &amp; executes it on behalf of the application. The application doesn’t need to know of the partitions in this case since the middleware manages everything (like query routing, txn).\nEg: MongoDB (kinda), Vitess\n\n\n\n\n\n\nDecentralized\n\n\n\nDistributed Concurrency Control\n\nNeed to allow multiple txns. to execute simul. across multiple nodes.\nChallenges: Replication, Network Communications Overhead, Node Failures, Clock Skew\n\n\n\n22 : Distributed OLTP Database Systems §\n\nAgenda: Atomic Commit Protocols, Replication, Consistency Issues (CAP / PACELC), Google Spanner\nOLTP\n\nShort-lived read/write txns.\nSmall footprint\nRepetitive operations\n\n\nOLAP\n\nLong running, read-only queries\nComplex joins\nExploratory queries\n\n\nDecentralized Coordinator\n\nApp sends request to the primary node\nQueries are sent to different partitions\nOnce changes are done, request primary node to commit txn.\nPrimary node checks with other nodes to confirm if txn is safe to commit &amp; commits it\n\n\nAssumption: All nodes in a distributed DBMS are well-behaved &amp; under the same administrative domain i.e. if we tell a node to commit a txn, then it’ll commit the txn (if there isn’t a failure).\n\nSidenote: Byzantine Fault Tolerant protocol is used if you don’t trust other nodes (like in blockchains).\n\n\n\nAtomic Commit Protocol §\n\nWhen a multi-node txn finishes, the DBMS needs to ask all the nodes involved whether it is safe to commit.\nExamples:\n\nTwo Phase Commit\nPaxos\nRaft\nZAB (used in Apache Zookeeper)\nViewstamped Replication\n\n\nTwo Phase Commit (2PC)\n\nPhase 1 : Prepare\n\nApp sends request to coordinator node to commit.\nCoordinator node asks other nodes if txn is safe to commit.\nEach node sends back its vote.\n\n\nPhase 2 : Commit or Abort\n\nCommit Scenario (each participant agrees)\n\nCoordinator node asks the other nodes to commit, changes are applied &amp; an ack is sent back to the coordinator.\nCoordinator sends an acknowledgement to the app.\n\n\nAbort Scenario (one participant disagrees)\n\nCoordinator notifies the app that the txn is aborted.\nCoordinator sends an abort message to the participants &amp; gets an ack from them.\n\n\n\n\nSee https://en.wikipedia.org/wiki/Two-phase_commit_protocol\nEach node records the inbound/outbound messages &amp; outcome for each phase in a non-volatile storage log.\nOn recovery, examine the log &amp; if local txn:\n\nIn prepared state -&gt; Contact coordinator\nNot in prepared state -&gt; Abort it\nWas committing &amp; node is the coordinator -&gt; Send COMMIT message to nodes\n\n\nIf coordinator crashes\n\nParticipants must decide what to do after a timeout.\n\nEasiest thing to do is to abort the txn. A participant has to become the new coordinator.\n\n\nSystem isn’t available during this time.\n\n\nIf participant crashes\n\nCoordinator assumes that it responded with an abort if it hasn’t send an acknowledgement yet.\nNodes use a timeout to determine that participant is dead.\n\n\nOptimizations\n\nEarly Prepare Voting (Rare)\n\nIf you send a query to a remote node that you know will be the last one you execute there then that node will also return their vote for the prepare phase with the query result.\nRare because people don’t write compatible code in applications. This shows up in systems where txns are running as stored procedures.\n\n\nEarly Ack After Prepare (Common)\n\nIf all nodes vote to commit a txn, the coordinator can send the client an acknowledgement that their txn was successful before the commit phase finishes.\n\n\n\n\n\n\n\nPaxos\n\nCoordinator (Proposer) proposes an outcome &amp; then the participant (acceptor) vote on whether the outcome should succeed.\nDoesn’t block if a majority of participants are available &amp; has provably minimal message delays in best case.\nParticipants decline txn if they receive another with a higher logical counter from another proposer (doesn’t have to commit, only needs to receive the “higher” txn)\nRead https://15799.courses.cs.cmu.edu/fall2013/static/slides/paxos_made_live.pdf\n\nSidenote: The original paper was released in 90s &amp; is much harder to read so it’s better to read the paper published by Google.\n\n\n\n\nMulti Paxos\n\nIf a single leader that oversees proposing changes is elected for some period then it can skip the Propose phase.\n\nFall back to full Paxos whenever there is a failure.\n\n\nThe system periodically renews the leader (known as lease) using another Paxos round.\n\nNodes must exchange log entires during leader election so that everyone is up to date.\n\n\n\n\n2PC vs Paxos\n\n2PC blocks if coordinator fails after the prepare message is sent until coordinator recovers.\nPaxos is non-blocking if majority of participants are alive.\n\n\n\nReplication §\n\nDBMS can replicate data across redundant nodes to increase availability.\nDesign Decisions\n\nReplica Configuration\nPropagation Scheme\nPropagation Timing\nUpdate Method\n\n\nReplica Configurations\n\nPrimary-Replica (old term : Master-Slave)\n\nAll updates go to a designated primary for each object.\nPrimary propagates updates to its replicas without an atomic commit protocol.\nRead-only txns may be allowed to access replicas.\nIf the primary goes down, then hold an election to select a new primary.\n\n\nMulti-Primary (Multi-Home)\n\nTxns can update data objects at any replica.\nReplicas must sync with each other using an atomic commit protocol.\n\n\n\n\nK-safety\n\nThreshold for determining the fault tolerance of the replicated database.\nThe value K represents the no. of replicas per data object that must always be available.\nIf the no. of replicas goes below this threshold, DBMS halts and goes offline.\n\n\nPropagation Scheme\n\nWhen a txn commits on a replicated DB, the DBMS decides whether it must wait for that txn’s changes to propagate to other nodes before it can send the acknowledgement to application.\nPropagation Levels\n\nSynchronous (Strong Consistency)\nAsynchronous (Eventual Consistency)\n\nNote: There could be a small window when you’d lose the data in case the primary goes down after sending the commit ack to app without syncing the commit to the other nodes.\n\n\n\n\nPropagation Timing Approaches\n\nContinuous\n\nDBMS sends log messages immediately as it generates them.\nAlso need to send a commit/abort message.\n\n\nOn Commit\n\nDBMS sends the log messages for a txn to the replicas once the txn commits.\nDoesn’t waste time sending log records for aborted txns.\nAssumption: Txn’s log records fits entirely in memory.\n\n\n\n\nActive vs. Passive Approaches\n\nActive-Active\n\nA txn executes at each replica independently.\nNeed to check if the txn ends up with the same result at each replica.\n\n\nActive-Passive (more common)\n\nEach txn executes at a single location &amp; propagates the changes to the replica.\nCan either do physical or logical replication.\nNot the same as Primary-Replica vs Multi-Primary. How????\n\n\n\n\n\n\n\nGoogle Spanner §\n\nGeo-replicated\nSchematized, semi-relational? data model\nDecentralized shared-disk architecture\nLog-structured on-disk storage\nConcurrency Control\n\nMVCC + Strict 2PL w/ Wound-Wait Deadlock Prevention + Multi-Paxos + 2PC\nExternally consistent global write-transactions with synchronous replication\n? Lock-free read-only transactions\nDBMS ensures ordering through globally unique timestamps generated from atomic clocks &amp; GPS devices.\n\n\nDB is broken up into tablets (partitions):\n\nUse Paxos to elect leader in tablet group\nUse 2PC for txns that span tablets\n\n\n\nCAP Theorem §\n\nIt’s not possible for a distributed system to always be:\n\nConsistent (Linerazibility)\nAvailable (All up nodes can satisfy all requests)\nNetwork Partition Tolerant (Operate correctly despite message loss)\n\n\nSee the PACELC theorem which is a newer extension to the CAP theorem.\nTraditional/Distributed Relational DBMS would choose consistency &amp; availability over network partition tolerance.\nNoSQL DBMS chose high availability over consistency &amp; then resolved conflicts after nodes are reconnected.\n\n23 : Distributed OLAP Database Systems §\n\nAgenda: Execution Models for OLAP Systems, Query Planning, Distributed Join Algorithms, Cloud Systems\nStar Schema\n\nInvolves a Fact table and many Dimension tables.\nThe Fact table holds references (as foreign keys) to other Dimension tables.\n\n\nSnowflake: More normalised than a Star schema. Tables can hold references to multiple levels of lookup tables for better normalisation.\nStar vs Snowflake Schema\n\nIssue 1 : Normalization\n\nSnowflake schema take up less storage space.\nDenormalised data models may incur integrity &amp; consistency violations.\n\n\nIssue 2 : Query Complexity\n\nSnowflake schemas require more joins to get the data needed for query.\nQueries on star schemas will usually be faster.\nStar schemas have more accurate statistics &amp; get better query plans.\n\n\n\n\nDesign Decision : Push vs Pull\n\nPush Query to Data\n\nSend the query (or a portion of it) to the node that contains the data.\nPerform as much filtering, processing as possible where data resides before transmitting over network.\n\n\nPull Data to Query\n\nBring data to node that’s executing a query that needs it for processing.\n\n\n\n\nObservation\n\nData received in node from remote sources are cached in the buffer pool which allows the DBMS to support intermediate results that are larger than amount of available memory. But these ephemeral pages are persisted on restarts.\nWhat happens to a long-running OLAP query if a node crashes during execution?\n\n\nQuery Fault Tolerance\n\nMost shared-nothing distributed OLAP DBMSs are designed to assume that nodes do not fail during query execution. If 1 node fails during query execution then the whole query fails.\n\nTraditionally, systems made the trade-off to not store the intermediate results since writing the intermediate results to disk impacted performance significantly for small clusters.\n\n\nThe DBMS could take a snapshot of intermediate results fro a query during execution to allow it to recover if nodes fail.\n\nNewer cloud systems allow writing the intermediate results to a shared disk which can be used by some other node to continue the query if the current one fails.\n\n\nSidenote: Hadoop stores the intermediate result after every step in the query pipeline. Most modern systems decide when checkpointing results is required &amp; do it rather than performing it after each step.\n\n\nQuery Planning\n\nWe need to consider the physical location of data &amp; network transfer costs.\nSidenote: DB2 runs a benchmark on startup to determine the network latency to other nodes &amp; factor that in query costs.\nQuery Plan Fragments\n\nPhysical Operators\n\nGenerate a query plan &amp; then break it up into partition specific fragments.\nMost systems implement this approach.\n\n\nSQL\n\nRewrite original query into partition-specific queries.\n\nSQL -&gt; Query Plan -&gt; Physical Operators -&gt; SQL for partitions\n\n\nAllows for local optimization at each node.\nEg: SingleStore, Vitess do this.\n\n\n\n\nObservation\n\nEfficiency of a distributed join depends on the target tables’ partitioning schemes.\nAn approach is putting entire tables on a single node &amp; then doing the join. But,\n\nParallelism of a distributed DBMS is lost.\nData transfer over network is costly.\n\n\n\n\n\n\nDistributed Join Algorithms\n\nTo join tables R &amp; S, the DBMS needs to get the proper tuples on the same node. Once data is at the node, the DBMS executes the same join algorithms as single node systems.\nSELECT * FROM R JOIN S ON R.id = S.id\nScenarios\n\n1 table is replicated at every node.\n\nEach node joins its local data in parallel &amp; then sends their results to a coordinating node.\n\n\nTables are partitioned on the join attribute.\n\nEach node performs the join on local data &amp; then sends to a coordinator node for coalescing.\n\n\nBoth tables are partitioned on different keys.\n\nIf 1 of the tables is small, then the DBMS “broadcasts” that table to all nodes.\n“Broadcast Hash Join”\n\n\nBoth tables are not partitioned on the join key.\n\nDBMS copies the tables by “shuffling” them across nodes.\nEg: One node would end up with all records for R &amp; S w/ id 1 to 100 &amp; another node with 101-200. They can then perform the join on the id (which isn’t the join key for these tables) on each node.\n\n\n\n\nOptimisation : Semi Join\n\nJoin type where result only contains columns from the left table.\nSELECT R.id FROM R JOIN S ON R.id = S.id WHERE R.id IS NOT NULL\nUsed by distributed DBMSs to minimize the amount of data sent during joins.\nSidenote: If SEMI JOIN isn’t explicitly supported, you can fake it with EXISTS.\n\n\n\n\nCloud Systems\n\nManaged DBMS\n\nNo significant modification to the DBMS to be aware that it’s running in a cloud environment.\n\n\nCloud-Native DBMS\n\nDesigned explicitly to run in a cloud environment.\nUsually based on a shared-disk architecture.\nEg: Snowflake, Google BigQuery, Amazon, Redshift, MS SQL Azure\n\n\n\n\nServerless Databases\n\nEvicts “tenants” when they become idle to save compute resources.\nEg: Neon, CockroachDB, Planetscale, MS SQL Azure, Fauna\nYou’d have a cold start for the first query since there’s no buffer pool keeping the pages in cache once a “tenant” is evicted.\n\n\nData Lakes\n\nRepository for storing large amounts of structured, semi-structured &amp; unstructured data without having to define a schema or ingest the data into proprietary internal formats.\nEg: Trino, Presto, Amazon Redshift, Google BigQuery, Hive, Databricks, Snowflake\n\n\nUniversal Formats\n\nMost systems use a proprietary on-disk binary file format. The only way to share data b/w these systems is to convert data to some text based format like CSV, XML, JSON.\nThere are new open-source binary file formats for easy access to data across systems.\n\nEg: Apache Parquet (from Twitter), Apache ORC, Apache Arrow (from Pandas/Dremio, in-mem compressed columnar storage), Apache Iceberg (from Netflix), HDF5 (multi-dimensional arrays used in scientific workloads)\n\n\n\n\nDisaggregated Components\n\nSystem Catalogs: HCatalog, Google Data Catalog\nNode Management: Kubernetes, Apache Yarn\nQuery Optimizers: Apache Calcite, Greenplum Orca\n\n\n\n24 : Embedded Database Logic §\n\nAgenda: User-defined Functions, Stored Procedures, Triggers, Change Notifications, User-defined Types, Views\n(Potential) Benefits of moving application logic\n\nLess network round-trips\nImmediate notification of changes\nDBMS spends less time waiting during transactions\nDevelopers don’t have to reimplement functionality (if we end we rewriting the application in some other language)\n\n\nUser-Defined Functions (UDF)\n\nFunction written by application developer that extends the system’s functionality beyond its built-in operations.\nTakes in input args (scalars) -&gt; Perform computation -&gt; Return a results (scalars, tables)\nUDFs can either be written using SQL Functions or an External Programming Language (depending upon the DB)\n\nSQL Standard: SQL/PSM\nOracle/DB2: PL/SQL\nPostgres: PL/pgSQL (inspired by Ada)\n\n\nOther systems support more common PLs in sandbox or non-sandbox environments.\nDisadvantages\n\nQuery optimizers treat UDFs as black boxes &amp; are unable to estimate cost or optimize the queries.\nDifficult to parallelize UDFs due to correlated queries inside of them. Some DBMSs will only execute queries with a single thread if they contain a UDF.\nComplex UDFs in SELECT/WHERE clauses force the DBMS to execute iteratively.\nDBMS can’t perform cross-statement optimizations (?) since it executes the commands in UDF one by one.\nSidenote: Some DBMSs allow specifying hints to mark the UDF for parallelization. SQL Server has read only functions which don’t let you update the DB.\n\n\n\n\nStored Procedures\n\nSelf contained function that performs more complex logic inside of the DBMS.\nCan have many input/output parameters.\nCan modify the database table/structures.\nSidenote:\n\nSome DBMSs distinguish UDFs vs. stored procedures but not all.\nA UDF is meant to perform a subset of a read-only computation within a query. A stored procedure is meant to perform a complete computation that is independent of a query. (Not all DBs enforce this though.)\n\n\n\n\nTriggers\n\nA trigger instructs the DBMS to invoke a UDF when some even occurs in the DB.\nRequires developer to define:\n\ntype of event that’ll cause it to run (INSERT, DELETE, ALTER, DROP etc.)\nscope of event (TABLE, DATABASE, VIEW, SYSTEM)\nwhen it runs relative to the event (before query, after query, before each row, after each row, instead of the query)\n\n\n\n\nChange Notification\n\nLike a trigger except DBMS sends a message to an external entity that something has happened in the database.\n\nCan be chained w/ trigger to pass along whenever a change occurs.\n\n\nSQL Standard: LISTEN + NOTIFY\n\n\nUser-Defined Types / Structured Types\n\nSpecial data type that is defined by the application developer that the DBMS can store natively.\nEach DBMS exposes a different API that allows you to create a UDT.\n\nPostgres/DB2 : In-built support\nOracle: PL/SQL\nMSSQL: Using external languages (.Net, C)\n\n\n\n\nViews\n\nCreates a virtual table containing the output from a SELECT query which can be accessed as if it was a real table.\nAllows programmer to simplify a complex query that is executed often.\n\nDoesn’t make the DBMS run faster though.\n\n\nOften used a mechanism to hide a subset of table’s attributes from certain users.\nSidenote: With “VIEW”, dynamic results are only materialized when needed while “SELECT…INTO” creates static table that doesn’t get updated when student gets updated.\nThe SQL standard allow modifying a VIEW if it only contain 1 base table &amp; doesn’t contain grouping, distinction, union or aggregation.\nMaterialized Views\n\nCreates a view containing the output from a SELECT query that is retained (i.e. not recomputed each time it’s accessed)\nSome DBMS (like SQL Server) automatically updated matviews when underlying tables change.\nOther DBMSs (like Postgres) require manual refresh.\n\n\n\n\nConclusion\n\nMoving application logic into the DBMS has lots of benefits:\n\nBetter efficiency\nReusable across applications\n\n\nBut it has problems:\n\nDebugging is difficult.\nNot portable.\nPotentially need to maintain different versions.\nDBAs don’t like constant change.\nThings like PL/SQL are quite archaic.\n\n\n\n\n"},"notes/cockroachdb's-query-optimizer-2020":{"title":"CockroachDB's Query Optimizer (2020)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"CockroachDB: Postgres-compatible Geo-Distributed SQL database\nArchitecture\n\nShared-nothing\nConsists of distributed SQL layer on top of a distributed KV layer\n\nThis talk focuses on the SQL layer.\nQuery Optimization in CockroachDB §\n\nWhy not use Postgres (or some other OSS) optimizer?\n\nCockroachDB codebase is written in Go. pg’s optimizer is in C. Didn’t want the overhead of calling C from Go.\nExecution plans are very different in CockroachDB. A plan that performs very well on a single node may perform poorly when distributed across nodes at a large distance.\nOptimizer is key to DB performance and using other optimizer won’t let them maintain control.\n\n\nCDB’s First Optimizer\n\nNot really an optimizer. Used heuristics (rules) to choose execution plan.\n\nEg: “if an index is available, always use it”\n\n\nOvertime rules started looking like this:\n\n“always use the index, except when the table is very small or we expect to scan &gt;75% of the rows, or the index is located on a remote machine”\n\n\nThis became difficult to manage.\nThis kind of optimizer works for OLTP but customers were using CDB for OLAP queries too.\n\n\nCost-based optimizer\n\nInstead of applying rigid rules, considers multiple alternatives\nAssign a cost to each alternative and choose lowest cost option\nCascade-style optimization w/ unified search\nHow to generate alternatives?\n\nStart w/ default plan from SQL query\nPerform a series of transformations (what ?)\nStore alternatives in a compact data structure called memo\n\nhttps://www.querifylabs.com/blog/memoization-in-cost-based-optimizers\nhttps://sqlserverperformace.blogspot.com/2020/03/inside-sql-server-query-optimizer-part.html\n\n\n\n\n\n\n\nQ/A\n\nDo you do anything like what Postgres does where they’ve the initial cost that is a cheap approximation and then there’s a final cost if you decide to complete the rest of it OR is it a single cost model that produces a single value?\n\nJust a single value right now.\n\n\nCan you do cost model estimations on logical nodes or does it always has to be a physical node?\n\nWe’ve merged the idea of logical and physical nodes. For eg: we don’t have the concept of a logical join. A logical join is just a hash join for us.\n\n\n\nGenerating alternative plans §\n\nPhase of plan generation\n\nParse -&gt; Optbuild -&gt; Normalize -&gt; Explore -&gt; DistSQL planning\n\n\nSample Query\n\nCREATE TABLE ab (a INT PRIMARY KEY, b INT, INDEX(b));\nCREATE TABLE cd (c INT PRIMARY KEY, d INT);\nSELECT * FROM ab JOIN cd ON b=c WHERE b&gt;1;\nParsing §\n\nParse the SQL query. Uses a yak file similar to pg.\n\n\nOptbuild §\n\nTakes AST from parser &amp; produces the preliminary query plan.\n\nConstructSelect(\n    ConstructInnerJoin(\n        ConstructScan(),\n        ConstructScan(),\n        ConstructFiltersItem(\n            ConstructEq(\n                ConstructVariable(),\n                ConstructVariable(),\n            ),\n        ),\n    ),\n    ConstructFiltersItem(\n        ConstructGt(\n            ConstructVariable(),\n            ConstructConst(),\n        ),\n    ),\n)\n\n\nAlso does semantic analysis. For eg:\n\nDo tables in the query actually exists and does the current user have the permission to read them?\nDo columns exist in those specific table and are they unique?\nWhat columns are selected by *?\nDo types match for equality comparison?\n\n\nQ/A:\n\nAt what point do you try to bind a prepared statement value to a type?\n\nHappens during the optbuild phase.\n\n\n\n\n\nNormalization §\n\nHappens in parallel w/ the optbuild phase. The nested function calls are factory methods generated from a no. of defined normalization rules.\nEach of the factory functions have a bunch of normalization rules that’ll execute and modify the output and create the full normalization plan.\n\nIn the image, we’ve used the fact that b=c to infer that if b&gt;1 then c&gt;1 and pushed the filter down below the join.\nNormalization rules\n\nCreate a logically equivalent relation expression.\nNormalization (or “rewrite”) rules are almost always good to apply.\nEg:\n\nEliminate unnecessary operation: NOT (NOT) x -&gt; x\nCanonicalize expressions: 5 = x -&gt; x = 5\nConstant folding: length(&#039;abc&#039;) -&gt; 3\nPredicate push-down\nDe-correlation of subqueries\n\n\n\n\n\nDSL : Optgen §\n\nDSL for representing normalization and exploration rules.\nGets compiled to factory function in Go which are called in Optbuild.\nExamples\n\n\n\n\n# EliminateNot discards a doubled Not operator // Comment explaining the rule\n\n[EliminateNot, Normalize] // Header with rulename, tag\n(Not (Not $input:*)) // Match rule\n=&gt;\n$input // Replace expression\n\n// ConstructNot constructs an expression for the Not operator.\nfunc (_f *Factory) ConstructNot(input opt.ScalarExpr) opt.ScalarExpr {\n\t// [EliminateNot]\n\t{\n\t\t_not, _ := input.(*memo.NotExpr)\n\t\tif _not != nil {\n\t\t\tinput := _not.Input\n\t\t\tif _f.matchedRule == nil || _f.matchedRule(opt.EliminateNot) {\n\t\t\t\t_expr := input\n\t\t\t\treturn _expr\n\t\t\t}\n\t\t}\n\t}\n\t// ... other rules ...\n\te := _f.mem.MemoizeNot(input)\n\treturn _f.onConstructScalar(e)\n}\n\n\n\n# MergeSelects combines two nested Select operators into a single Select that\n# ANDs the filter conditions of the two Selects.\n\n[MergeSelects, Normalize]\n(Select (Select $input:* $innerFilters:*) $filters:*)\n=&gt;\n(Select $input (ConcatFilters $innerFilters $filters))\n\n// [MergeSelects]\n{\n\t_select, _ := input.(*memo.SelectExpr)\n\tif _select != nil {\n\t\tinput := _select.Input\n\t\tinnerFilters := _select.Filters\n\t\tif _f.matchedRule == nil || _f.matchedRule(opt.MergeSelects) {\n\t\t\t_expr := _f.ConstructSelect(\n\t\t\t\tinput,\n\t\t\t\t// DSL allows calling arbitrary Go functions (like ConcatFilters)\n\t\t\t\t// defined by them\n\t\t\t\t_f.funcs.ConcatFilters(innerFilters, filters),\n\t\t\t)\n\t\t\treturn _expr\n\t\t}\n\t}\n}\nQ/A\n\nHow Go specific is the DSL?\n\nWe don’t use any Go specific PL features. Can be rewritten for other languages.\n\n\n\nExploration §\n\nExploration rules may or may not produce a better plan so both alternatives are kept around unlike normalization where replacement is done.\nSame syntax as Normalize rules in the DSL with a different tag (Explore)\nEg:\n\nJoin reordering: A join (B join C) -&gt; (A join B) join C\nJoin algorithm (eg. hash join, merge join, lookup join)\nIndex selection\n\n\n\nMemo after normalization\n\nMemo stores query plan trees. It consists of a series of groups.\nThey also store scalar expressions in the Memo groups but only relational expressions are shown in the image below.\nGroups can refer to other groups. Eg. Group 1 does an inner join b/w Group 2 and 3.\n\n\nFor exploration, you iterate through the groups and see if any exploration rules match.\nGenerateIndexScans created an alternate scan in which we’re scanning the secondary index as opposed to the primary index.\n\n\nExample from https://www.querifylabs.com/blog/memoization-in-cost-based-optimizers\n\nThe best plan gets passed to the next phase after cost determination.\nDistSQL Planning §\nTakes the plan from the optimizer and extends them to whatever the cluster topology is.\n\n\nCurrently doesn’t take advantage of how the tables are laid out on the disk, optimizations that may involve broadcasting some data from a smaller table to another for joins etc. but planned in near future.\n\nChoosing a Plan §\n\nFactors that affect cost:\n\nHardware configuration\nData distribution\nType of operators\n\nPerformed benchmarking to understand the relative cost of different operators under the assumption that they won’t change much w/ different queries.\nThis relative cost is hard-coded in the cost model.\n\n\nNo. of rows processed by each operator\n\n\n\nNo. of rows processed by each operator\n\nCollect statistics: Row count, Distinct count, Null count, Histogram\nMulti-column stats are also collected in addition to single column stats\n\nUse indexes to determine what columns to collect multi-column stats on\n\nEg: for index on (a, b, c), collect multi-column stats on (a,b) and (a,b,c)\n\n\n\n\nStats Collection (CREATE STATISTICS): Full Table Scan -&gt; Perform Sampling (size: 10K rows) ; Insert each row into HyperLogLog sketch to calculate distinct count for each column -&gt; Aggregate Samples\nCREATE STATISTICS is automatically run when:\n\nA table is created\nA new column or index is added\n~20% of data in table has changed\n\n\nHow to determine when 20% of data has changed?\n\nAfter a mutation on some node, statistic collection will be triggered based on chance\nP(refresh) = no. of rows updated / (no. of rows in table * 0.20)\n\n\nAlways refresh if no stats yet OR a it’s been a while since last refresh\nEach create stats run takes minutes. Full table scan can impact performance. Many table scans at once can bring down the cluster.\nTo minimize performance impact\n\nRun CREATE STATISTICS as a job\n\nGuarantees only 1 stats job running at a time\nResilient to node failures\n\n\nUse throttling to limit CPU utilization by stats job\n\n\n\nLocality-Aware SQL Optimization §\n\nCustomers can optionally duplicate read-mostly data in each locality\n\nUse replication constraints to pin the copies to different geographic regions (eg. US-East, US-West, EU)\n\ni.e. duplicate index keys in the table similar to the primary key\n\n\nOptimizer includes locality in cost model &amp; automatically selects index from same locality: primary, idx_eu or idx_usw\n\n\n\nCREATE TABLE postal_code (\n\tid INT PRIMARY KEY,\n\tcode STRING,\n\tINDEX idx_eu (id) STORING (code),\n\tINDEX idx_usw (id) STORING (code),\n)\n\nPlan queries to use data from the same locality\nWhat’s next? (see attached slides for more info on this)\n\nReplace duplicated indexes w/ “global tables”\n\nhttps://www.cockroachlabs.com/blog/global-tables-in-cockroachdb/\n\n\nSupport geo-partitioned unique indexes\nMove DistSQL planning into optimizer\nIncorporate latency into cost model\n\n\n\nTheory vs. Practice §\nOptimizing for OLTP\n\nWhen they went from the simple heuristic planner to the cost based optimizer, they had to focus a lot of minimizing overhead for simple OLTP queries (eg. primary key lookup) because even though the heuristic planner had problems, it was really fast\n\nTook advantage of logical properties essential for optimization\n\nCardinality (different from stats)\nFunctional dependencies\nNon-null columns etc.\n\n\n\n\nNormalization rules are really important. As of this talk they’ve 242 normalization rules &amp; 29 exploration rules\nForeign key checks &amp; cascades optimized as “post queries”\n\nForeign key checks (eg: value inserted in a child table that references some other parent table and needs to check whether the value exists in parent table or not) use joins which can be optimized.\nDone after the statement is executed (but hasn’t returned value yet).\n\n\n\nJoin Ordering\n\nv1 was shipped w/o join ordering.\nInitially implemented w/ 2 rules: CommuteJoin, AssociateJoin\n\nWas really inefficient.\nReordered at most 4 tables by default.\n\n\nAn intern implemented DPSUBE from “Guido Moerkotte, Pit Fender, and Marius Eich. 2013. On the correct and complete enumeration of the core search space.” and made it more efficient.\n\nNow it orders up to 8 tables by default.\n\n\n\nQuery Cache\n\nLRU cache keyed on SQL string\nStores optimized memo\nFor prepared statements w. placeholders\n\nNormalized memo is stored\nPlaceholders are replaced during execution followed by additional normalization and exploration\n\n\n\nOther features\n\nOptimizer Hints\n\nCockroachDB allows forcing specific index and join types as hints\n\n\nDebugging tools EXPLAIN ANALYZE (DEBUG) ...\n\nGenerates a bundles with stats, schema, environment variables,  the query plan at various verbosities\n\n\n\nQ/A §\n\nHow do you test the optimizer? How do you test if the cost model is working correctly?\n\nWe don’t prove that the chosen plan is the best plan. We mostly focus on testing the correctness of the plan.\nWe run benchmarking regularly so that existing queries don’t regress.\n\n\nAre input parameters enough to debug everything you need?\n\nIt’s enough for now. Once we start adding more to the cost model to make it more aware of the cluster topology and data distribution then we’d need more info.\n\n\nDo you run any kind of SQL fuzzer?\n\nYes. SQLSmith. Mostly for checking if there’s any internal error.\nManuel Rigger was testing logical correctness using SQLLancer and opened a bunch of issues.\n\nhttps://www.manuelrigger.at/dbms-bugs/\n\n\n\n\nIn general, what’s the complexity of queries you’re seeing? Cockroach isn’t like Snowflake so complex queries might be limited since they’re pushed to those systems. Are things limited to TPC-H, TPC-DS?\n\nComplex queries are uncommon.\n\n\nIs there any restriction on rewrite rules on the kind of complexity they’re allowed to handle? You mentioned that they can call arbitrary (programmed by them) Go functions.\n\nWe don’t allow User-Defined functions. All functions are reviewed by some team member.\n\n\n\nAppendix §\n\nSlides: https://courses.cs.washington.edu/courses/csep590d/22sp/lectures/UW%20Talk%202022%20CockroachDB’s%20Optimizer.pdf (from another event)\nhttps://github.com/cockroachdb/cockroach\nSIGMOD’20 - CockroachDB: The Resilient Geo-Distributed SQL Database\nHow we built a cost-based SQL optimizer\n"},"notes/duckdb---lambda-functions-in-the-duck's-nest-2024":{"title":"DuckDB - Lambda functions in the duck's nest (2024)","links":[],"tags":["db","talks"],"content":"\nTania is a software engineer at DuckDB Labs. She previously interned at CWI.\nDuckDB in a nutshell: Analytical database, In-process, Written in C++\n\nNested Data §\n\nRelevance of nested data &amp; lambdas\n\nNested inputs like JSON and Lists are common\nAggregation aren’t intuitive on nested structures\nUn-nesting data\n\n\nRecent example: https://til.simonwillison.net/duckdb/remote-parquet\n\n-- reading data from multiple parquet files stored elsewhere using DuckDB\nSELECT SUM(size_total)\nFROM (\n    SELECT SUM(size) as size_total FROM &#039;https://huggingface.co/datasets/vivym/midjourney-messages/resolve/main/data/000000.parquet&#039;\n    UNION ALL\n    SELECT SUM(size) as size_total FROM &#039;https://huggingface.co/datasets/vivym/midjourney-messages/resolve/main/data/000001.parquet&#039;\n    UNION ALL\n    SELECT SUM(size) as size_total FROM &#039;https://huggingface.co/datasets/vivym/midjourney-messages/resolve/main/data/000002.parquet&#039;\n    UNION ALL\n    SELECT SUM(size) as size_total FROM &#039;https://huggingface.co/datasets/vivym/midjourney-messages/resolve/main/data/000003.parquet&#039;\n    UNION ALL\n    SELECT SUM(size) as size_total FROM &#039;https://huggingface.co/datasets/vivym/midjourney-messages/resolve/main/data/000004.parquet&#039;\n\t-- and so on until 000055.parquet\n);\n-- query using lambda function in DuckDB\nSELECT\n    SUM(size) AS size\nFROM read_parquet(\n    list_transform(\n        generate_series(0, 55),\n        n -&gt; &#039;https://huggingface.co/datasets/vivym/midjourney-messages/resolve/main/data/&#039; ||\n            format(&#039;{:06d}&#039;, n) || &#039;.parquet&#039;\n    )\n);\n\nNested data isn’t typical for relational DBs\n\nSchemas don’t support nesting and force users to normalize their data\nYou’d end up needing to buy/build a specialized analytical system that supports nested data\n\nNote: pg has nested data support but isn’t an analytical system.\n\n\n\n\n\nHow DuckDB Does It §\nExecution §\n\nThe internal layout for the list type is broken into a vector with offset and length of list and then the selection and child vector with the original data.\nBenefits\n\nVector-at-a-time (operation applied to entire vector leveraging SIMD)\nIdeally the vectors fit in L1 cache.\nCan use tight loops to run computations.\n\n\nSidenote:\nA tight loop is one which is CPU cache-friendly. It is a loop which fits in the instruction cache, which does no branching, and which effectively hides memory fetch latency for data being processed.\nSource: https://stackoverflow.com/a/26924484/12531621\n\n-- tbl:\n┌────────────┬───┐\n│ l          ┆ n │\n╞════════════╪═══╡\n│ [7, 12, 5] ┆ 2 │\n│ [1, 11]    ┆ 3 │\n└────────────┴───┘\n \n-- SELECT [x + n FOR x IN l IF x &lt; 10] AS result FROM tbl;\n┌────────┐\n│ result │\n╞════════╡\n│ [9, 7] │\n│ [4]    │\n└────────┘\n\nFirst list has length 2 since one element (12) got filtered out. Similarly, we filtered out the 2nd list and are left with 1 element.\nSelection vector has also changed. First element (7) is on index 0. Second element (5) is on index 2 and so on.\n\n\nParser, Transformer, Binder §\n\nParser / Transformer\n\nParse list comprehension syntax\nTransform into scalar function\nSELECT [x + n FOR x IN l IF x &lt; 10] AS result FROM tbl; -&gt;\nSELECT LIST_TRANSFORM(LIST_FILTER(l, y -&gt; y &lt; 10), x -&gt; x + n) AS result FROM tbl;\nBinder\n\nLambda parameters (x -&gt;)\n\nGet the logical type of each parameter\nCreate a dummy table containing 1 column for each parameter\n\n\nLambda expression (-&gt; x + n)\n\nBind the lambda expression like any other expression\nUse the dummy table to “pretend” that lambda parameters are columns\n\n\nIndex calculation\n\nDuring execution : expects vectors to execute on\nIndexes express vector location. In -&gt; x + n x has index: 0, n has index 1.\n\n\nExpressions can get more complex with recursion and nested lambda parameters.\n\nEg: log(n + x * 2) + y where y is an outer lambda parameter.\n\n\nCaptures\n\nThe lambda expression is removed from the scalar function LIST_FILTER and added to a global execution state.\nThe function parameters are updated. Eg: LIST_FILTER(l, y -&gt; y &lt; 10) -&gt; LIST_FILTER(l, 10)\n\n\n\n\n\n\n"},"notes/duckdb-internals-2023":{"title":"DuckDB Internals (2023)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"\nRemarks\nThe Unified Vector Format was really neat. It’s nice to see the adaptive string format from Umbra being used by DuckDB too.\n\n\nIn-process OLAP DBMS (SQLite for Analytics)\nSupports querying directly over many formats (like Parquet, CSV, JSON, Arrow, Pandas, SQLite, Postgres)\nMark is the Co-Founder &amp; CTO along w/ being a major contributor at DuckDB. PhD from CWI.\n\nSystem Overview §\n\n\nWritten in C++, Uses the pg parser\nVectorized push-based model (vector at a time for an operator instead of the usual row at a time model)\n\nVectors §\n\nDuckDB has a custom vector format similar to Arrow\n\nFocused more on execution. The Arrow format is optimized for other use-cases like streaming over a socket, serialization, on-disk storage etc.\nCo-designed w/ Velox (both DuckDB &amp; Velox have the same vector format)\n\n\nFor scalar types, vectors are logically arrays.\nTheir VectorType physically stores compressed data which can be pushed into the engine &amp; operated on directly without decompressing.\nSome examples: \nOperations involving different kind of vectors together would need to be handled case by case which would require a lot of code. DuckDB doesn’t use specialized execution for these cases &amp; uses generic operators.\n\nFlatten: Convert vector into Flat vector (i.e. decompress)\n\nDownside: need to move/copy data around\n? Do they actually use this\n\n\nToUnified - Convert vector to unified format\n\nUnified Format for Applicable Types\nDoesn’t require copying/moving data\n\n\n\n\nQ/A\n\nYou get a dictionary vector when reading a DuckDB file but does the same happen when reading Parquet files with their dictionary format?\n\nWe emit our dictionary vectors for Parquet too. Some values like NULL parameters in query are also converted to Constant vector if applicable.\n\n\nDo you have any idea of how much performance gain on removing the indirection layer when comparing a dictionary w/ 1 element &amp; a constant vector?\n\nWe don’t specialize a lot on dictionary vectors compared to constant vectors since the benefit isn’t always obvious for generic dictionaries compared to a constant vector.\n\n\nDo you read data before you choose which representation to store your vectors in?\n\nOur storage is compatible with vectors but it isn’t exactly that. We do things like bitpacking before storing data. Some of our storage formats are compatible w/ vectors but not all of them translate to different vector types.\n\n\n(Clarification) Say a user inserts values like 1, 2, 0 which would be apt for the dictionary format. How would you determine whether to use Flat or Dictionary Vector?\n\nThere’s a compression phase when we write to disk. When reading back from the storage format, we can say that our storage format is dictionary encoded so a Dictionary vector might be the optimal representation for this.\n\n\n\n\nStrings\n\nSame format as Umbra is used.\nSize: 16 bytes (B)\nShort Strings  (&lt;= 12 bytes)\n\nlength(4B) | string data (4B+8B)\nInlined (instead of storing a ptr to the string)\n\n\nLong String\n\nlength(4B) | prefix (4B) | offset or ptr(8B)\n\n\nRegardless of string length, the first 8 byte contains the size &amp; prefix for the string. This allows faster comparisons.\n\n\nNested Types (eg. structs, list)\n\nStoring as blobs or string is slow\nStored recursively using vectors (similar to Arrow)\n\nFast &amp; highly efficient processing\nAllows reuse of operators for nested types\n\n\nThe types are composable i.e. structs can have lists, lists can have structs &amp; so on.\nStructs store their child vectors &amp; a validity mask.\n\nLists are stored as combination of offset/lengths &amp; a child vector. The child vector can have a different length.\n\n\nQ/A: What happens when data types differ b/w struct fields for different entries?\n\nWe don’t support different datatypes for the same field when using structs. The schema for a struct is fixed. There’s a Map type (key-pair values) which is stored as a list of structs which can be used instead.\n\n\nQ/A: Can the struct contain itself as a child type?\n\nNo. Not supported.\n\n\n\n\n\nQuery Execution §\n\nPull-Based Model (Vector Volcano) (initial, not used now)\n\nEvery operator implements GetChunk.\nQuery starts by calling GetChunk on the root.\nNodes recursively call GetChunk on the children.\nGeared towards single threaded execution\n\n\nParallelism Model\n\nhttps://youtu.be/bZOvAKGkzpQ?si=cgTGLEY35DqLwrSC&amp;t=1937\nAdding multi-threading to a Volcano-based system\n\nExchange Operator\n\nIndividual operators don’t need to be parallelism aware\nThe optimizers splits the query plan into multiple partitions which are executed independently\n\nEg: HashTable being built over different ranges for a table &amp; then joined by the exchange operator\n\n\nNote for Self: See https://15445.courses.cs.cmu.edu/fall2022/slides/13-queryexecution2.pdf for more details\nIssues\n\nPlan explosion (lots of plan due to splitting &amp; re-joining, DBMSs often do quadratic or even exponential order operations with plans)\nLoad imbalance issues (all partitions may not have the same load &amp; lead to uneven distribution across threads)\nAdded materialization cost (communication b/w operators require materializing rows since the operators are parallelism unaware)\n\n\n\n\nMorsel driven parallelism\n\nIndividual operators are parallelism-aware\n\nEg: Hash Join knows how to obtain input in parallel. Same for Scan operator.\n\n\nParallelism isn’t integrated into the query plan\nMorsel drive parallelism was hard to integrate into a pull-based model where everything is entangled together. So, folks at DuckDB switched to a push-based model.\nPaper: Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age\n\n\n\n\n\n\nPush-based Model\n\nSeparate interfaces for sink, source &amp; operator.\nSource &amp; sink are parallelism aware.\nSource\n\nvoid GetData(DataChunk &amp;chunk, GlobalSourceState &amp;gstate, LocalSourceState &amp;lstate)\n\n\nOperator\n\nOperatorResulttype Execute(DataChunk &amp;input, DataChunk &amp;chunk, OperatorState &amp;state)\n\n\nSink\n\nvoid Sink(GlobalSinkState &amp;gstate, LocalSinkState &amp;lstate, DataChunk &amp;input)\nvoid Combine(GlobalSinkState &amp;gstate, LocalSinkState &amp;lstate)\nvoid Finalize(GlobalSinkState &amp;gstate)\n\n\n\n\nThe Hash Join on the right side doesn’t need to be parallelism aware since it’s just going to probe the HashTable built by the HashJoin on left.\n\n\nIn pull-based flow, control flow lives inside the operator which allows for great flexibility (eg. can call child, decide to get more data or not etc.) but that level of flexibility wasn’t needed in DuckDB. Call stack holds all state since everything is built on recursive calls.\nIn push-based flow, control flow happens in a central location. State is stored explicitly in a central location. This also enables optimizations like\n\nVector Cache: add small caches b/w operators\nScan Sharing: push results of 1 scan into multiple sinks (can be done in a pull-based system but easier to implement in a push-based one)\nQ/A: What happens if the different sinks consuming from the Scan operate at different rates?\n\nCurrently Sinks are blocking in the sense that they are executed one after the other. If we were doing async IO then we’d need to apply back-pressure somehow.\n\n\n\n\nExecution of a pipeline can be paused since state is stored centrally.\n\nEg: If you’re pushing data into a sink &amp; that sink has fixed sized buffer, if the buffer is full, you can interrupt the pipeline.\nSimilarly, you need to be able to pause your pipeline if you were doing async IO so that the pipeline waits for the IO to be done.\nHard to do this in a pull-based model.\nQ/A: Could you have synchronization bottlenecks if you’ve parallel queries? What about the global state? Can that become a bottleneck?\n\nDepends on the operators you’re using. Could definitely happen if you’ve things that do socket connections etc. Usually the only socket connections we’ve are on our sources because we don’t have a client consuming this over a socket.\nThe global state is where the communication b/w threads happen &amp; it can definitely become a bottleneck.\n\n\n\n\n\n\n\nTable Storage §\n\nSingle-file block-based storage format\nWAL is stored separately\nFile Strcuture\n\nInfo (4K) | HeaderOne (4K) | HeaderTwo(4K) | Block (BLOCK_SIZE) | ...\nBlocks are all the same fixed size: 256KB\n\n\nSupport for ACID using headers. How?? Didn’t get his explanation.\nTables are partitioned into row groups.\n\nEach row group has 120K~ rows.\nRow groups are the parallelism &amp; checkpoint unit.\n\n\nCompression works very well w/ columnar storage.\n\nGeneral purpose, heavyweight compression\n\nFinds patterns in bits\nEg: gzip, zstd, snappy, lz4\nSimple to apply\nHigher decompression speed slows down execution\nNeed to decompress in bulk - no random seeks or compressed execution\n\n\nSpecial purpose, lightweight compression\n\nFind specific patterns in data\nEg: RLE, bitpacking, dictionary\nFaster than general purpose compression\nDownside: No effect if the patterns aren’t there. So multiple algorithms need to be used.\nPatterns can be exploited during execution\n\n\n\n\nCompression works per-column per row-group\n\nPhase 1 : Analyze: Figure out which compression method(s) are best\nPhase 2 : Compress\n\n\n\nLightning Round : Misc Topics §\n\nBuffer Manager\n\nCustom lock-free buffer manager\n\nInspired by lean-store\nDoesn’t use LRU since it needs a centralized data structure but similar to it\n\n\nGranularity: 256KB\n\n\nOut-Of-Core\n\nSupports larger than memory execution using a streaming engine.\n\nSpecial join, sort &amp; window algorithms.\n\n\nGoal: Gracefully degrade performance.\n\nTraditionally, systems just switch over to the slow path (for on disk) once things don’t fit in-memory.\nDuckDB tries to use as much memory as possible &amp; tries to write as little to disk as possible.\nSee Paper: Efficient External Sorting in DuckDB\n\n\n\n\nTransactions\n\nACID txns.\nBased on Hyper MVCC model\n\nOptimized for vectorized processing\n\n\nSupports Snapshot Isolation\nOptimistic concurrency control\n\nChanges to the same rows -&gt; txn. abort\n\n\nPaper: Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems\n\n\nPluggable File System + HTTP/Object Store Reads\n\nCan be used for querying S3, other files online directly\n\n\n\nQ/A §\n\nDoes query cancellation work in DuckDB? For eg: Can you stop an ongoing hash table build?\n\nQuery cancellation is supported. Control is returned to the central location from the operator which can cancel the query.\n\n\nIs compaction done in DuckDB? If a version isn’t used, it it removed? There were users reporting that even after deleting certain tables, the file size wasn’t reduced.\n\nWe don’t do compaction yet. We do re-use the blocks (which are marked free whenever a row group is destroyed)\nMVCC is only done in-memory. We don’t write it out to storage like Postgres or other DBs. Once the system shuts down, the txns are cancelled anyway.\n\n\nIs checkpointing done in background (periodically) or when the WAL gets full?\n\nWe checkpoint when the WAL crosses a specific threshold.\nWe also do optimistic writing. If you’re loading a lot of data, we won’t write to the WAL, we just directly write to the DB file &amp; directly compress stuff which speeds things up.\n\n\nWhat do you think is the role of UDF in DuckDB?\n\nWe’re working on implementing UDFs in Python right now. We’ve some Javascript UDFs. We’ll probably implement WASM UDFs as well. You can also build extensions to implement UDFs in DuckDB.\nUDFs are less important in DuckDB compared to Postgres since pulling data out is so fast. Things you’d need a UDF for in Postgres, you can just do by getting the data out of DuckDB.\n\n\nIs the query optimizer top-down or bottom-up?\n\nWe’ve different query optimizers. We’ve a mix of rule &amp; cost based optimizer but it’s currently in a messy state.\n\n\nHow do you keep track of resources?\n\nFor threads, we’ve our own threadpool. We allow users to use their thread-pool or integrate w/ ours.\nFor memory, the buffer manager has fixed memory &amp; unpins pages (if possible) when that memory limit is exceeded.\n\n\n\nAppendix §\n\nhttps://duckdb.org/\nhttps://github.com/duckdb/duckdb\nSlides for the talk: https://15721.courses.cs.cmu.edu/spring2023/slides/22-duckdb.pdf\nInitial Paper: https://mytherin.github.io/papers/2019-duckdbdemo.pdf\nOlder Talk (2020): https://www.youtube.com/watch?v=PFUZlNQIndo\n"},"notes/efficient-csv-parsing---on-the-complexity-of-simple-things-2024":{"title":"Efficient CSV Parsing - On the Complexity of Simple Things (2024)","links":[],"tags":["talks"],"content":"\nPedro did his PhD and Post-doc from CWI. He works at DuckDB now. He’s worked on ART indexes, zonemaps, index joins, CSV reader etc. for DuckDB.\n\nEarly On (2018) §\n\nThe CSV parser was able to read simple well constructed CSV files.\nAccept main dialect options: Delimiter, Quote, Escape\nAccept schema definitions: Column Names / Types\nIssues\n\nBad Performance in Line Split and Sanitization\nRelied on std::ifstream\n\nNo control on buffer management or file access.\n\n\nDidn’t work for quoted new lines\n\n\n\nBuffered CSV Reader + GOTO (2019) §\n\nRelevant file in PR: https://github.com/duckdb/duckdb/pull/211/files#diff-ab757987c109f9f46a7cd0876d1971730997734da4abe4ba27402738f457771c\n\n// Pseudo-Code\n \nnormal:\n\t/* state: normal parsing state */\n\t// this state parses the remainder of a non-quoted value\n\t// until we reach a delimiter or newline\n\tdo {\n\t    for (; position &lt; buffer_size; position++) {\n\t        if (buffer[position] == options.delimiter[0]) {\n\t            // delimiter: end the value and add it to the chunk\n\t            goto add_value;\n\t        } else if (StringUtil::CharacterIsNewline(buffer[position])) {\n\t            // newline: add row\n\t            goto add_row;\n\t        }\n\t    }\n\t} while (ReadBuffer(start, line_start));\n\t// file ends during normal scan: go to end state\n\tgoto final_state;\n \nadd_value:\n\t...\nadd_row:\n\t...\nin_quotes:\n\t...\nunquote:\n\t...\nhandle_escape:\n\t...\ncarriage_return:\n\t...\nfinal_state:\n\t...\n\nInstead of ifstream, you’ve your own file handle which utilises 32MB buffers and provides control of what’s in memory or not for efficiency.\n~ 4x improvement TPCH-SF10 benchmark with 60M line-items. Went from ~220s to ~55s.\nIssues\n\nProblematic to debug (no stack information)\n\nEspecially due to the use of the goto statement\n\n\nBrittle (can’t handle CSV files that aren’t well constructed)\nSetting headers, types and dialect (delimiter, quotes, escapes) is difficult\n\n\n\nWoodstock Era (2020) §\n\nRelevant article: https://duckdb.org/2023/10/27/csv-sniffer.html\nFocus on automatically reading CSV files.\n\nHave types, names and dialects inferred from data\n\n\nIntroduction of options like:\n\nignore_errors: skip lines that don’t confirm w/ found rules\nnull_padding: mark columns as null in rows w/ missing columns\nskip_rows: skip dirty rows (comments etc.) (auto-detected)\n\n\nThings seen in CSV files\n\nDirty Lines: comments on top or interspersed in the line\nMissing Columns\nToo Many Columns (extra ; but no data)\nMissing Quotes (eg: In line - The Who; &#039;Who&quot;s Next; 1971)\nNon-UTF8 characters\nEmpty Lines\nExtra ; at the end of every line (with no data)\n\n\nOnly a 10% speed-up on TPCH-SF10 compared to previous version.\n\nThe Parallel State Machine Era (2023-now) §\n\nIn a (non well-structured) CSV file, you don’t know where a line starts or ends. Eg:\nCSV File:\n1;Tenacious D; &#039;The Pick Of Destiny&#039;; 2006\n2;&#039;Simon \\r\\n Garfunkel&#039;; &#039;Sounds of Silence&#039;; 1966\n3;Sir. Jon Lajoie; &#039;You want some of this?&#039;; 2009\n4;The Who; &#039;Who&#039;s Next&#039;; 1971\n\nWe’ve information about the schema of the CSV file from the CSV sniffer.\nBigint | Varchar | Varchar | Date\n\nBuffers:\n\nBuffer #1: 1;Tenacious D; &#039;The Pick Of Destiny&#039;; 2006\\r\\n2;&#039;Simon\\r\\n Garfunkel\nBuffer #2: &#039;; &#039;Sounds of Silence&#039;, 1966 \\r\\n 3;Sir. Jon Lajoie; &#039;You want some of\nBuffer #3: this?&#039;; 2009\\r\\n 4;The Who; &#039;Who&#039;s Next&#039;; 1971\n\nLet’s say 4 threads (Thread 0 - Thread 3) are allotted parts to read by the global scanner (split shown using  | ).\n1;Tenacious D; &#039;Th | e Pick Of Destiny | &#039;; 2006\\r\\n2;&#039;S | imon\\r\\n Garfunkel\n\nThreads can cross over boundaries (in and across buffers) if needed. They also use the schema to determine if the particular line they read is valid or not.\nState Machine §\nThe parser uses a state machines similar to this: (States x Transition Trigger)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatesA..Z, 0..9,”\\\\nStandardStandardDelimiterStandardStandardRecord SeparatorDelimiterStandardDelimiterQuoteStandardRecord SeparatorQuoteQuoteQuoteUnquoteEscapeQuoteRecord SeparatorStandardDelimiterStandardStandardRecord SeparatorEscapeINVALIDINVALIDQuoteINVALIDINVALIDUnquoteINVALIDDelimiterINVALIDINVALIDRecord SeparatorEg: If you’re currently in Standard state and come across ”,”, you’ll transition to Delimiter state.\n\nImproved ease of debugging.\nPerformance: takes 0.1s on the TPCH-SF10 benchmark\nSome other optimizations\n\nImplicit Casting (casting during read instead of in a separate step when possible)\nProjection Pushdown\n\n\n\nFuture §\n\nDetecting Structural Errors (Can be ignore or shown depending upon configuration)\n\nMissing Columns\nExtra Columns\nInvalid Unicode (&quot;pedro��&quot; -&gt; &quot;pedro&quot;)\nSidenote: Can also be configured to store rejected values (with file metadata, config and the error with exact row-col position)\n\n\nAsync IO\nCSV Fuzzing / Testing\nFaster Dialect Detection\nCaching / Partioning\n\nMultiple readers over the same file start from the same point.\nIO, Parsing must happens again. This can be avoided.\nAcademic work: NoDB: Efficient Query Execution on Raw Data Files\n\n\n"},"notes/fast,-lazy-container-loading-in-modal-2024":{"title":"Fast, lazy container loading in Modal (2024)","links":[],"tags":["talks","nyc-systems"],"content":"Modal is a serverless cloud GPU service.\nThe container loading problem §\n\nA usual setup on Modal for some standard ML prediction inference:\n\nRuntime: Python 3.11 (Fat container, 8 GiB uncompressed)\nModel: Bert (512 MiB .safetensors file)\n\n\nYou need the container file system for the container to start running. A container filesystem is an overlay mount on the host. An overlay FS mount allows reconcile mutations that happen against a read-only lower directory.\n\noverlay on /mnt/overlay type overlay (\n    rw,\n    relatime,\n    lowerdir=/tmp/tmph45cav46/lower, &lt;- 8GiB uncompressed. read-only\n    upperdir=/tmp/tmph45cav46/upper, &lt;- mutable, initally empty\n    workdir=/tmp/tmph45cav46/work\n)\n\nApproaches §\n\nEager\n\nUsed by Docker, K8s\nLoading approach\n\nContainer image is a fat stack of N tarballs (gzipped).\n\nEach layer is a separate tarball.\n\n\nDownload N tarballs over the network: concurrent, 2GiB/s. (If host doesn’t already have it )\nDecompress gzipped tarballs: single-threaded, 80MiB/s.\nUnpack into a rootfs directory on host.\nSidenote: See /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/ on a K8s node (with containerd runtime).\n\n\nWhile getting the tarballs over the network isn’t expensive, they’re usually compressed and decompressing them takes a lot of time. You also need to write the files against the host file system, setting metadata etc. This needs to be done before the container is even started.\n\nThis delay is fine when you’re running a typical K8s cluster with a few applications since the nodes chosen will usually have the container image cached but its quite slow for a server-less platform.\n\n\n\n\nLazy\n\nUsed by Modal, AWS Lambda1\nLoading approach\n\nContainer image is an index: ~5MiB, load in 1-100ms.\nIndex has only pointers to file contents.\nFUSE mount that index: ~2ms.\n\nlowerdir=/tmp/tmph45cav466/fuse\n\n\nFUSE : Filesystem in Userspace\n\nguest process in user space —filesystem calls like open()—&gt;\nFUSE module in kernel space —FUSE message—&gt;\nhost process —file data—&gt; kernel -&gt; guest process\n\n\n\n\n\n\n\nLazy loading performance problems §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystemRead LatencyRead ThroughputCost ($USD/GiB/month)Memory1-100 ns10-40 GiB/s$2.0SSD100 µs4 GiB/s$0.10AZ Cache server1 ms10 GiB/s$0.15^Regional CDN100 ms3-10 GiB/s-Blob storage (e.g. S3)200 ms3-10 GiB/s$0.015^ : significant operation costAZ -&gt; cloud provider availability zone\nThis is solvable\n\nLatency lags bandwidth: bandwidth improves much more quickly than latency.\nCaching works\n\nWhile Modal is a general container runtime, its customer base is mostly folks using Python for AI/ML applications.\nThere’s a lot of overlap in the files in the container used by different libraries (like pytorch, dreambooth etc.).\nA lot of stuff in a fat image is junk that won’t be used but an eager loading system will always load it.\n\n\n\nSystem Architecture §\n\nPerformance grind §\nLLM models have sizes in 10s of GiBs. Modal’s peak bandwidth throughput was 800 MiB/s which wasn’t sufficient. It’s now 2.5 GiB/s.\nHost\n\nAvoid small hosts since cloud providers won’t provide enough bandwidth for IO on SSDs and network.\nYou don’t want to be bottlenecked by the network. Network perf &gt;&gt; FUSE perf. Aim for &gt; 4 GiB/s download speeds.\n\nAvoid single-flow traffic limit &amp; throttling set by cloud provider. Make sure you get the advertised bandwidth.\n\nYou instance might have 20GiB down. but only 5GiB for a single-flow.\n\n\n\n\nStack SSDs together in RAID 0 config (data split across drives)\n\nRAID 0 improves IO performance since IO is split across those drives and can be done concurrently. A large file can be stored in multiple segments on different drives and read back much faster.\nWe don’t need durability here since the origin of the data is the blob store.\n\n\nModal uses AWS g5.12xlarge (5 GiB/s, 3.8TiB NVMe SSD)\n\nFUSE\n\nPeak read() throughput was around 800 MiB/s. Lags far behind NVMe and network throughput.\nNow at 2.5 GiB/s.\nFocus on large-file throughput, not op latency.\nIncreasing read_ahead\n\nread_ahead allows kernel to request future pages when it receives request for a particular page\nOriginally at 128KiB, now set at 32MiB.\nWe can’t keep increasing it forever. The kernel will end up reading a lot of data for a single read-request and in turn increase latency.\n\n\nIncreasing READ request size (FUSE max_pages)\n\nEach READ request from Kernel to the userspace FUSE server has a size. Larger request sizes, fewer trips into user space.\nOriginally at 128KiB, now at 32MiB.\nWithout direct IO, couldn’t go above 1MiB. ???\n\nNo page cache if using direct IO.\n\n\n\n\nTune FUSE’s congestion threshold and background threads\n\nThe FUSE system in the kernel is going to fulfill the IO requests either on an synchronous path or in background. Read requests are buffered. Modal is only using a read-only filesystem so they want to make the background queues as large as possible to avoid throttling the guest processes.\ncongestion_threshold -&gt; was 9, now 256.\nmax_background: was 12, now 256.\n\n\nOther suggestions\n\nDon’t blow up the heap.\n\nHaving a large heap was dropping their throughput and reducing the heap size while they were pulling through the large files was improving their throughput.\n\nThe large heap was leading to more page faults for their workload.\nWhen you issue a read to Modal and it goes over the network, they want to keep the returned data into disk and the page cache so that any subsequent reads are served from the disk or page cache.\nThey were keeping the data in memory in order to write to the disk and blowing up their memory usage.\n\n\nThey now defer writes, keep the heap small, stream data from network to guest process without holding it in memory.\nThis increased throughput by 100-200 MiB/s.\n\n\nBalance disk cache write priority against read priority.\n\n\nAlso see\n\nImproving readahead\nhttps://web.archive.org/web/20230408150313/http://www.fsl.cs.stonybrook.edu/docs/fuse/fuse-article-appendices.html\nTo FUSE or Not to FUSE: Performance of User-Space File Systems\n\n\n\n\n512 MiB .safetensors file loads in 200ms from disk cache, ~300ms from network.\n\nQ/A\n\nHow do you defer disk-writing without blowing up the heap or creating a bottleneck on IO write perf of the SSD?\n\nKeep a reference to data-chunk on the blob store and fetch it later. (Blob stores allow referring to ranges of data) and free the memory occupied by the data chunk.\n\n\nDo you have to worry about data corruption in the read requests or mis-directed reads?\n\nWe check a lot on the write path. We check length and digests on the read path. We’re able to detect corruption on the disks and fix it.\n\n\nDid you ever run into problems where a workload caused latency issues?\n\nLatency is still a bottleneck even for the usual workload. Cold start performance is the most important part.\nWe’re trying to avoid trips into the user-space from the kernel. We’re thinking of using things like EROFS which you load inside the kernel and avoids the trip to user-space.\nWe also run a user-space kernel called gVisor which adds additional slowness.\n\n\nThe Python interpreter requests files/modules one by one. How do you deal with that?\n\nEarlier, this was mitigated by the AZ cache which we used to be much more invested in.\nNow divested from the AZ cache. We focus more on regional and origin store.\nWe do a thing called task profiling where we see what the process has done, record that and then aggressively pre-load before the container even starts up. If we’ve never seen that container before, we rely on a certain heuristics (eg. what’s a python process going to want, what’s a ML python process going to want).\n\n\nCould you use io_uring to improve performance?\n\nWe haven’t explored using it yet but it sounds interesting.\n\n\n\nOther things\n\nFast container image distribution plugin with lazy pulling\nBlog: https://modal.com/blog/jono-containers-talk\n\nFootnotes §\n\n\nOn-demand Container Loading in AWS Lambda ↩\n\n\n"},"notes/litestream---making-single-node-deployments-cool-again-2022":{"title":"Litestream - Making Single-Node Deployments Cool Again (2022)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"\nBen is the author of Litestream &amp; the key-value store BoltDB.\nWhat Litestream Does\n\nStreaming replication for SQLite\nBacks up to S3, network disks, SFTP etc.\nAdds high durability to a single node deployment\n\n\n\nSQLite Review §\n\nSQLite Internal Structure\n\nPhysical Layout: Fixed size pages in a file (usually 4KiB but can be changed)\nLogical Layout: B+ Tree\n\n\nEarly SQLite\n\nHad a transaction mechanism called rollback journal. If you wanted to make a change in the DB transactionally, you’d copy out the older version of the page to a file called the rollback journal &amp; you’d make changes inside the DB. If anything fails, just copy back the file from the rollback journal.\nBad concurrency. Single writer OR multiple readers.\n\n\nModern SQLite\n\nWAL added in 2010.\n\nImproved read concurrency, performance.\nSingle writer AND multiple readers. Multiple versions of pages exist. Each reader gets its own snapshot at the point-in-time it started.\nSerializable isolation.\n\n\n\n\n\nHow Litestream Works §\n\nOnly works with SQLite WAL mode.\nSQLite’s WAL journaling mode writes new pages to a “-wal” file.\nIf a query starts after a page has been modified by some txn then that page will be read from the WAL &amp; the rest of the pages (which aren’t modified by the point the query started) will be read from the DB.\nSQLite Checkpointing : When WAL grows too large, latest versions of pages are written back to the DB.\n\nBy default the WAL gets upto 4MB before checkpointing.\n\n\nLitestream checks the WAL continuously (every second) &amp; copied out segments to a shadow WAL. Use some kind of checksum to verify consistency of data. Also detects when the WAL checkpoints &amp; truncates and then creates a new shadow WAL.\nShadow WAL acts as the staging area to push out later to durable storage periodically (default is 10s but can be configured).\nQ/A: Do you batch when copying to S3 or copying as is?\n\nWe take a chunk of the subset of WAL to copy to S3. We might concatenate some chunks but we’re not really batching.\n\n\nLitestream also pushes the snapshot of the whole DB separate from the WAL file everyday so that once you’ve a snapshot you only need to replay the WAL files that occurred after that.\n\nYou can do a point-in-time recovery for any period after the last snapshot until the next snapshot is created.\n\n\nData window loss is around 1s. (if you configure Litestream to copy to S3 every second)\nQ/A: Can pages be split across the WAL segments?\n\nNo. The segments always contain full txns. so you won’t have a txn. split across segments. All the pages are within those txns.\n\n\nQ/A: The logical tree contain ptrs or page addresses. When these pages are moved to S3, how is the SQLite DB restored from this backup?\n\nThe page ptrs within the BTree are ptrs to page number &amp; not to a specific version of a page. When you restore from S3, you copy the pages into the same position in the DB.\n\n\n\nFailure Modes §\n\nData Loss Window\n\nLitestream uses async replication &amp; will lose up to X seconds of data during catastrophic failure where X is the configured sync interval.\nMakes attempt to push to durable storage on clean shutdown.\n\n\nRestore\n\nWAL is replicated as-is w/ no compaction. More WAL segments means longer restore times.\nWAL downloaded in parallel from S3.\nLZ4 compressed.\nIncreasing snapshot frequency improves restore performance.\n\n\nMultiple instances can corrupt\n\nLitestream is single-instance only\nReplicating from multiple sources to a single path will corrupt backups because each instance will delete the other instance’s data.\nIn cases like blue-green deployment (where 2 versions of an app are running for sometime), data can get corrupted.\n\n\nRestarting your instance\n\nWill incur downtime\nRestarts are less common for older VPS models but more common for ephemeral models like FlyIO, K8s.\n\n\n\n\nSidenote: Costs (with S3)\n\nAWS S3 ingress is free.\nPUT requests are 0.005$ / 1000 req.\nReplicating every second costs $13 / month &amp; every 10 seconds costs $1.30/month.\n\n\nLiteFS §\n\nPeople wanted realtime read replication &amp; high availability. So, they made LiteFS.\nLiteFS is a Distributed File System for SQLite databases.\nFUSE-based.\nCluster of nodes share the same SQLite data.\nSingle-writer elected via Consul lease.\nInstant point-in-time restores.\nWhy FUSE?\n\nSQLite has a virtual file system but requires user intervention (load an extension every session before use).\nFUSE allows fine grained control which lets them do things like prevent writes of read replicas, redirect writes etc.\nMulti-process support.\n\n\nWhy cluster SQLite?\n\nImprove availability esp. during deploys\nReplicate to edge\n\n\nLeader Election via Lease (and not Raft)\n\nBen wrote the initial raft library for etcd but wanted to keep Litestream simple.\nRaft has strict distributed consensus guarantees which isn’t required for all applications.\nConsul Lease allows for loose membership.\nRolling DB checksum tracked at every txn.\n\nUses XOR CRC64 with the old checksum of the page &amp; add in the new one.\nFast to compute &amp; provided constant integrity checking.\nAllows for mostly monotonic txn ID.\nIf a primary (w/ a txn that hasn’t been pushed to replicas) goes down &amp; another node comes up then you’ve some divergence b/w nodes. The checksum helps realise the divergence &amp; reset state. (Some data is lost but the DB isn’t corrupted)\n\n\n\n\nInstant Point-inTime Restore\n\nIntroduces Lite Transaction (LTX) file format instead of raw WAL.\nLTX files are compactable from multiple streams.\n\nDurable storage can be rolled up into higher level LTX files.\n\n\nCan use this to have a staging area for every PR.\n\n\nAlternatives\n\nRegular Backups w/ cron (can be done in addition w/ Litestreams)\nRaft-based replication tools: rqlite, dqlite\n\nBetter consistency &amp; durability guarantees but more complex to set up &amp; maintain.\n\n\nVFS-based Replication: verneuil, LiteReplica,\n\nImplemented via virtual file system within SQLite\nMust be compiled/linked into the application\n\n\nPostgres &amp; MySQL\n\n\n\nQ/A §\n\nHow big can the DB be for these tools to work well?\n\nTargeting around 1-10GB DBs.\n\n\nLitestream is blindly copying pages w/o parsing them. Have you thought about some optimizations you could do?\n\nLitestream does parse the WAL file. There’s a header for the whole WAL &amp; one for each frame which is used to delineate txn.\nWe could do Change Data Capture (CDC).\n\n\nHow much load does running Litestream add onto a machine?\n\nNot much really. Most of the pages are recent &amp; will be in the page cache. Analysis is pretty minimal since the WAL is mostly copied so it doesn’t require much processing time.\n\n\n"},"notes/materialize---a-streaming-sql-database-powered-by-timely-dataflow-2020":{"title":"Materialize - A Streaming SQL Database Powered by Timely Dataflow (2020)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"Arjun is the Co-founder and CEO of Materialize.\nWhat is a streaming database? §\n\nOptimized for view-maintenance on an ongoing basis over streams of already processed txns.\nTraditionally, most systems were either OLTP or OLAP systems.\nOld Architecture:\n\nOLTP -&gt; Batch ETL -&gt; OLAP -&gt; SQL -&gt; BI Tools, Reports, Data Mining\n\n\nIdeal Architecture Today:\n\nDB -&gt;\n\n(Streaming Pipeline) Streaming CDC -&gt; Stream Processing -&gt; Microservices -&gt; Visualization, Alerting, Monitoring\n\nSome tools for stream processing: Apache Flink, Apache Druid, Kafka Streams\n\n\n(Batch Pipeline) Batch ELT -&gt; OLAP -&gt; SQL -&gt; BI Tools, Reports, Data Mining\n\n\n\n\nA lot of tooling is needed to get data from a stream reliably to dashboards, alerts and applications. In essence a lot of these tools are just calculating materialized views over the streamed data. With Materialize:\n\nDB -&gt; Streaming CDC -&gt; Stream Processing -&gt; Materialize -&gt; SQL -&gt; Visualization, Alerting, Monitoring\n\n\n\nThe Streaming Ecosystem §\n\nWhat’s different about online view maintenance (OLVM)?\n\nQueries run for a long time\n\nOLTP/OLAP queries are optimized at execution time. You can re-plan for each query.\nStreaming queries need to be optimal forever.\nOnce you create a view, query plan is fixed.\nQuery planning is much harder since an OLTP system will maintain an evaluation context and can decide to bail and re-run a query but that isn’t an option in dataflow engines. It needs to have a static query plan prepared.\nError handling is also more difficult since the DB needs to keep running and making progress.\n\n\nStatistics won’t save you\n\nPast performance is not indicative of future results.\nMost query optimization literature for OLAP is oriented around the idea of getting good cardinality estimates of your tables and using that to choose the best query plan.\nIn streaming, you’ve to be adaptable to fast and slow moving streams which can change the no. of events by a large amount. Sometimes, you might be using data combined from a slow and fast moving stream which further increases the complexity.\nQ/A: You’d have a dimension and a fact table where the dimension table is mostly static. So you could have some statistics about the fact table using the dimension table. Or are there no static tables in streaming?\n\nIn streaming, people reissue the dimension tables because of updates from batch ETL. This can be really bad for performance if your query plans assumed that the dimension tables are mostly going to be static.\nIn OLAP, you could just re-plan everything.\n\n\nQ/A: Do people actually drop the entire dimension table and then load it back that often?\n\nYeah. It’s pretty common.\n\n\n\n\nWrites are already ordered\n\nNo concurrency control (similar to OLAP) is needed.\n\n\nQuery patterns are known and (mostly) repeated\n\nA lot of things can be done ahead of time.\n\n\n\n\n\nA Streaming Manifesto §\n\nSQL : Declarative\n\nA lot of streaming systems don’t have full SQL support (arbitrary joins, non-window join conditions etc.) .\n\n\nDo only what’s necessary: Reactive to input changes, little to no busywork\n\nExisting streaming processors have massive hardware footprint even if they’re ingesting low amounts of data due to complex queries.\n\n\nJoins: No mandatory temporal windows; Arbitrary join conditions\n\nExisting streaming systems require streaming joins be windowed along a temporal dimension (i.e. if input stream is changing over time, the joins is only evaluated over some fixed window)\nAs per their docs, ”JOINs work over the available history of both streams, which ultimately provides an experience more similar to an RDBMS than other streaming platforms.”\n\n\n\nMaterialize : Architecture §\n\nMaterialize is built on top of timely dataflow and differential dataflow.\n\nhttps://github.com/TimelyDataflow/timely-dataflow\n\nStreaming compute engine. Scale-out, stateful, cyclic dataflow engine.\nCan apply arbitrary operators (written in Rust) on data. These operators are passed timestamps along w/ other input data.\n\n\nhttps://github.com/TimelyDataflow/differential-dataflow\n\nHas an opinionated set of differential operators (like Join, Aggregate, Filter, Map, Range)\n\nRange: index building operator that takes care of state management\n\n\n\n\n\n\nMaterialize on top of these handles client connections, maintains catalogs, streams, views. It does parsing, planning, optimizing of queries and constructing dataflow plans from these queries.\n\n\nTraditional streaming systems achieve parallelism by sharding on operator.\n\nEg. Multiple workers for “Count” operator in a pipeline.\nThis can get expensive for complex queries even for low data volume since the data passes through each worker for an operator one by one. Some of the operations (eg. “Filter”) might even be no-op for a lot of data.\n\nTimely dataflow cooperatively schedules every operator on each worker and shards the entire dataflow graph by key.\n\nTimestamps drive the movement of data.\n\nBuilding Materialize : Experiences §\n\nWriting performant dataflow programs is very hard.\nEfficient state management is the key.\n\nExisting stream processors outsource state management to RocksDB which leads to loss of control over things like when compaction happens.\n\n\nSQL requires 100% coverage.\n\nQ/A: How do you cover the SQL standard entirely?\n\nWe don’t. We support Joins, Subqueries etc. Don’t support CTEs and a few other things yet.\n\n\n\n\n\nQ/A §\n\nDoes this dataflow uses multi-way joins with worst case optimality to do some kind of computations?\n\nWe’re using multi-way joins but not the algorithm with worst case join optimality .\n\n\nWhat is the idea of consistency in your system?\n\nWe don’t do any concurrency control on our end because our inputs are ordered for us and have a timestamp associated with them but our system is essentially maintaining snapshot isolation.\n\n\nCan I combine windows w/ different durations? (Sliding window semantics)\n\nNo.\n\n\nWhat aspect of the implementation in Materialized was the most difficult?\n\nQuery planning to get out a static dataflow graph.\nMaterialized optimizer is written from scratch.\n\n\n"},"notes/neon---serverless-postgresql!-2022":{"title":"Neon - Serverless PostgreSQL! (2022)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"Heikki is the co-founder of Neon &amp; a long-time PostgreSQL committer.\nWhat is Neon? §\n\nNew storage system for Postgres\nStorage and compute are separated\nMulti-tenant storage\n\nOne storage layer that is shared across all customers and databases\nShared cache\n\n\nSingle-tenant compute (Runs in K8s containers / VMs)\n\nFuture plan: BYO Postgres with specific version and extensions installed\n\n\nCheap copy-on-write branching and time-travel query\nSingle-writer system i.e. single primary that’s generating log &amp; processing updates at any given time\nDoesn’t try to solve the multi-master problem or conflicts across regions\nPostgres compatibility\n\nCurrently, they’ve modified low-level storage in Postgres (read, write a page) so that those requests could be sent to their storage\nOther things like Planner, Executor, index types, MVCC hasn’t been changed\nPostgres Core Changes\n\n\n\nSeparation of Storage and Compute §\n\n\nCompute = PostgreSQL running in a VM\nStorage = Neon storage system\n\nWritten in Rust\n\n\npg streams WAL to the safekeepers\n\npg has support for stream replication (used to stream WAL from primary to replica) which they modified to stream the WAL to their storage system.\n\n\npg reads pages from pageservers over network instead of local disk\nwrite() in pg is a no-op. They just throw the page away &amp; it’s queried by the storage system (pageserver) using WAL if needed again\n\nDon’t need to do traditional checkpointing since writing is a no-op. There’s no need to flush everything to disk in Neon.\nStill let pg run checkpoint since it performs other functions than just flushing to disk but they don’t flush pages to disk.\nLocal disk is only used for temporary files, sorting etc. Data is wiped when Postgres is restarted.\n\n\nWhy separate compute &amp; storage?\n\nCompute can be shut down completely &amp; started quickly\n\nCurrent startup time is 4s (includes launching the VM or K8s container, setting connection to storage &amp; replying back to client)\n\n\nSame storage can be shared by multiple read-only nodes\nScale independently\nCloud storage is cheap\n\nNeon uses S3 or S3-compatible stores\n\n\n\n\nQ/A : On the server-side when you boot up, do you pre-fetch anything in the buffer pool?\n\nNothing currently. The storage system does have its own cache so there’ll be any data in there if you recently re-started.\nThere are pg extensions to pre-warm the cache that are compatible with Neon. (Probably pg-prewarm)\n\nBut this brings in all the pages and not the ones present at last shutdown.\n\n\n\n\nQ/A : Is the server cache the WAL or materialized pages?\n\nBoth.\n\n\n\nWrite Path §\n\n3 safe-keeper nodes running at all times\nConsensus algorithm based on Paxos\n\nWait for majority to acknowledge txn. commit before sending acknowledgement to original client\n\n\nEnsures durability of recent txn. (This is pretty much what safe-keepers are for)\nSafe-keepers have local SSDs for storing WAL\nDetour : Postgres WAL\n\nClassic area-style representation\nPhysical, stores updated to 8kB pages\nMix of page images &amp; incremental updates\n\nDoesn’t store statements\n\n\nNo UNDO log, only REDO\n\n\nPage-servers\n\nKey of the storage system\nAfter log is durable in safe-keepers, it’s streamed to page-servers\nProcesses the WAL into a format (that lets them quickly find WAL records of a particular page) which is then written into immutable files on disk\nUploads files to cloud storage\nKeep a copy in page-server for caching\nLocal SSDs for caching\n\n\nDurability\n\nRecent WAL is made durable in safe-keepers\nOlder WAL is uploaded to cloud storage in processed format\nPage-servers are disposable\n\n\n\nRead Path §\n\nPage-servers\n\nReplays WAL to reconstruct pages on demand\nCan reconstruct any page at any point in time\n\nKeeps all history upto some retention period\nInstead of traditional backups &amp; WAL archive, they store data in their own format in cloud for random access\n\n\nRequest for Page from pg -&gt; Page-server finds the last image &amp; replays the log of that single page -&gt; Sends back the page\n\n\nQ/A: Does the page-server wait for the safe-keeper to send it or requests it when it doesn’t have a particular log record?\n\nWhen pg requests the page at a particular LSN, if the page-server doesn’t have it yet, it’ll wait\n\n\nQ/A: Does pg maintain internally that for page 123, it expects this LSN or is it something you’re adding?\n\nWe’ve to add that.\nWasn’t needed for correctness. The primary node could request the latest LSN it wrote &amp; it’d be correct but that’d cause a perf. problem because anytime you read anything from the page-server, you’d need to wait to get the latest version and most of the time there were no changes to that version.\nWe had to add a cache that tracks LSN numbers of pages evicted from cache. (Last 1000 evicted pages w/ LRU)\n\n\n\nControl Plane &amp; Proxy §\n\n\nWhen the client connects, it first connects to a proxy.\nProxy intercepts the connection, performs auth &amp; queries the control plane about the running pg instance and starts one if none is running for the client.\n\nVMs are shut down after 5mins of inactivity.\n\n\nControl plane starts &amp; stops compute nodes\n\nAlso provides the web UI &amp; user-facing API for creating DBs, branches etc.\n\n\nQ/A: Is the proxy written from scratch or did you use pgBouncer or something similar?\n\nFrom scratch. We don’t use the proxy for connection pooling. We use it as a pass-through.\n\n\n\nStorage Engine §\n\n\nTraditional Point-in-Time Recovery\n\nTake periodic backups and archive WAL to durable storage\nTo restore\n\nRestore last backup before the point-in-time\nReplay all the log\n\n\n\n\n\nNeon does this at page granularity\n\nKeeps backup of individual pages &amp; stores WAL records of these individual pages\nWAL contains a mix of full page images &amp; incremental WAL records\n\npg prefers to write the full image of a page in case of bulk loading, building an index etc. otherwise it’ll mostly store the incremental updates\n\n\nTo reconstruct a page version\n\nFind the last image of the page\nReplay all the WAL records on top of it\n\n\nTo make this perform:\n\nPage-server reorders &amp; indexes the WAL\nMaterialize &amp; store additional page images\n\npg might have a lot of updates for a page so the page-server decides to store some additional page images so that the entire log doesn’t have to be played back when the page is queried\n\n\n\n\n\n\n\nQ/A\n\nDo you store only the latest version of a page in the page-server or could you materialize multiple ones?\n\nWe can reconstruct any page version upto the retention period.\n\n\nDo you do any compression on the physical storage?\n\nNot currently. We plan to do it.\n\n\nIf you don’t set fill-factor right, that’s more just like an interaction with auto-vacuum where updates could span multiple pages &amp; you just need the auto-vacuum to clean that up but with this system we’re keeping page versions does that mean you get a bunch of write amplification if you don’t have your fill-factor knob set right?\n\nfill-factor: how full pages will be packed (in table or index)\nVacuum will create new versions of these pages but that’s okay since the WAL records are quire small so Vacuum will create new versions of these pages.\n\n\n\n\n\nGetPage@LSN\n\nWhen pg needs to read a page, it sends a request to page-server : GetPage(RelFileNode, block #, LSN)\n\nPrimary node uses “last-evicted LSN” of the page\n\nLast-evicted LSN is loosely tracked for each page\n\n\nRead-only node can be anchored at an old LSN\n\nFor doing a time-travel query, for eg: if you want to recover to the point where you say dropped the table then you launch a pg node and point it to the page-server &amp; you give it the LSN that you want to read the data &amp; pg will send all requests at that specific LSN &amp; you can see the data as it was at that point of time\n\n\nRead-only node that follows the primary\n\nThere’s a cache invalidation problem &amp; if you’ve a read-only replica that’s following the primary, the read-only node will still need to follow the WAL from the primary to figure out which pages are being modified because it might’ve a version of those pages in cache &amp; it needs to throw them away.\nFrom the page-server side, it looks like the read-only node requests the pages with an increasing LSN as it tracks the primary\n\n\n\n\n\n\n\nStorage Engine : Key-Value Store §\n\nKey: relation id + block number + LSN\n\nRelation ID tells which table or index a block belongs to\n\n\nValue: 8kB page or WAL record\nMetadata key+value pairs are stored for tracking things like relation size\nQ/A : What’s special about the multi-attribute key\n\nWe’re doing range queries when you request a page at particular LSN. We need to find the last version of the page &amp; it’s not a point lookup.\nThe LSN no. keeps incrementing as we digest new WAL. We don’t replace the old value. We add to it &amp; preserve the history too.\n\n\nInspired by LSM\nImmutable files\n\nWAL is buffered in memory (in a BTree)\nWhen ~1GB of WAL has accumulated, it’s written out to a new layer file (similar to an SSTable in LSMs)\nExisting layer files are never modified\nOld files can be merged &amp; compacted, by creating new files and deleting old ones\n\n\nAll files are uploaded to cloud storage\n\nAnd downloaded back on-demand, if missing locally\n\n\n\nWhy not use an existing LSM implementation? §\n\nNeed to access history\n\nUsed RocksDB in a earlier prototype &amp; use the block number + LSN as the key\nDidn’t behave very well since when you keep accumulating new versions of a page, you insert new key-value pairs but when you do compaction, you move those existing keys to the next level &amp; so on but we didn’t want to do that since we’re not going to modify those keys &amp; there’s never any tombstone since we don’t remove anything. Write amplification was quite bad with this.\nMany LSM tree implementation have support for snapshots and the capability to read older versions of key value pair &amp; they typically do that for MVCC &amp; Snapshot Isolation but they don’t really expose the functionality. Many of them wouldn’t allow using our LSN number or they’d only allow you to take a snapshot &amp; then read all of the data but it wouldn’t allow to take a snapshot in history &amp; they’d only keep the snapshot while the system is running.\n\n\n2 kinds of values: images &amp; deltas (= WAL records)\nNeed to control materialization\n\nSome implementations allowed hooking into the compact/merge operation &amp; re-write some of the keys at that point but not all of the keys.\n\n\nUpload/download from cloud storage\nBranching for cheap copy-on-write branches\n\nThis might’ve worked with other stores since it’s implemented at a higher level in our storage engine.\nWe create a new storage for each branch &amp; if you fall to the bottom of that storage w/o finding a version of that page, you look at the parents.\n\n\nWritten in Rust or another memory-safe language\n\nSince our storage system is multi-tenant, the buffer cache is shared across different DBs belonging to different customers &amp; we don’t want to have a segfault or leak data from one DB to another.\n\n\nWe already have WAL &amp; many key-value stores come with a WAL which we don’t need\n\nStorage Format §\n\n\nConsists of immutable files called layer files\n2 kinds of layer files\n\nImage layer: contains a snapshot of all key-value pairs in a key-range at one LSN\n\nCreated in background to speed up access and allow garbage collecting old data\nImage layer creation every ~20 seconds\n\n\nDelta layer: contains all changes in a key and LSN range\n\nIf a key wasn’t modified, it’s not stored\nIncoming WAL is written out as delta layers\n\n\n\n\n2-D storage\n\nX/Y :  block ID/LSN\nRectangles are delta layers\nHorizontal bars are image layers\nEach file is roughly the same size (~1GB which seems pretty good for dealing w/ cloud storage)\n\n\nSearch\n\nTo re-construct a page version, GetPage@LSN needs to find the last image of the page &amp; all WAL records on top of it\n\nSearch starts at the given block # and LSN, visit layers (downwards) until you find an image\nDelta layers may contain images\nSearch stops at image layers\n\n\nSearch is downwards. Look into the layer file &amp; collect the WAL records for the particular key (if any) and so on until we hit the image layer which contains images of all the pages in the key-range.\n\nWe’ve the last image &amp; the WAL records now which can be replayed\nIf a full image is found in the delta layer, we can stop the search earlier\n\n\n\n\nProcessing incoming WAL\n\nNew delta layers are added to the top\nLogs are re-ordered and stored in their format for faster lookups\n\n\nCompaction\n\nRe-shuffles data in delta layers that contain all of the changes for a larger LSN range but smaller key- range\n\nFor better locality in searches\n\nSince you’ve fewer files &amp; don’t need to visit too many layers\nMight’ve got similar benefit with something like a bloom filter (but isn’t implemented yet)\n\n\nTo aid in garbage collection (of frequently updated parts)\n\n\nMentioned that they aren’t entirely sure about compaction for their use-case yet\n\n\nGarbage Collection\n\nRemoves old layer files that aren’t needed anymore\n\n\nSomeone at Neon wrote a tool to visualise the cluster:\n\nBranching\n\nNeon supports cheap, copy-on-write branches &amp; this is how they do backups.\nWhen you read a key on the child branch &amp; it’s not found, continue to read it from the parent at the branch point.\n\n\nOpen questions for Neon\n\nWhen to materialize pages pre-emptively?\n\nWe don’t need to materialize until the compute requests the page.\nIf you’ve a workload that doesn’t fit into the cache in pg then you’d keep frequently requesting pages it wrote a while ago &amp; it’d affect your latency if we need to do a replay at that point. (It takes a few ms to collect the records &amp; do the replay)\nThey haven’t solved the problem of when they should request a page pre-emptively.\nQ/A\n\nWhen you decommission the compute layer every 5 mins, do you signal the page-server to cleanup the data?\n\nWe don’t currently.\n\n\nHow do compute servers know which page-servers to read from?\n\nThe control plane keeps track of this. There is currently only 1 page-server for 1 database. It’s not currently sharded but we plan to do it in future.\n\n\nDo you find yourself more limited by network bandwidth or disk bandwidth for reading from page-servers?\n\nOne thing we ran into was the sequential scan speed. pg relies heavily on OS cache for sequential scans so when you’re scanning it’ll request page numbers one-by-one in order. The network round-trip for each individual page made this very slow for us so we added pre-fetching support.\nIn other workloads, we’ve to face the overhead of reconstructing the pages doing the WAL replay if there were a lot of changes in the page.\nIf you’ve a lot of layer files, we had a dumb algorithm for keeping track of what layer files exist which consumed a lot of CPU but we’re addressing that with a different data structure there.\n\n\n\n\n\n\nWhen to create image layers?\nWhen to merge delta layers?\n\n\n\nQ/A §\n\nWhat % of DBs in Neon are swapped out?\n\nWe try to keep everything in the page-server right now.  We only rely on swapping out if we have to kill the page-server and reload it. We don’t have enough data to need to swap things out. We’ve had the service running for a few months &amp; that hasn’t been enough time to create enough data to swap anything else.\nFor the compute layers, we’ve b/w 50 and 100 active computes at any given time. We’ve about 3000 registered users.\n\n\nYou mentioned that it takes 4s if you’ve a decommissioned instance &amp; you connect to it. How does this compares against serverless Postgres from Amazon?\n\nAmazon doesn’t scale down to 0. Don’t know how much time it takes for them to startup.\nFor us, it takes ~1s to spin up the pod, few 100 ms to download a “base backup” (pg database directory w/o the data since the data is downloaded separately). We run some queries to check if pg works. We add a record to internal DB through control-plane for book-keeping to remember that we started this pod. And then there’s the round-trip latency going to the client.\n\npg database directory is needed so that pg can find anything that’s not just a table or index.\nGoal is to get it down to 1s.\n\n\n\n\nSince you modified the pg storage system &amp; pg relies on OS page cache so is there anything you had to change about the assumption that pg makes about having an OS page cache?\n\nThe sequential scan was one thing since pg depended on OS to do the read-ahead. We don’t get the benefit of the OS page cache.\n\n\n\nAppendix §\n\nhttps://github.com/neondatabase/neon\nhttps://neon.tech/\n"},"notes/novel-design-choices-in-apache-couchdb-2021":{"title":"Novel Design Choices in Apache CouchDB (2021)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"Adam is an IBM Fellow working on cloud file systems. Co-founder of Cloudant (based on Apache CouchDB) which was acquired by IBM.\nCouchDB\n\nDocument Store: JSON documents with primary keys\nBuilt in change feeds\nEvent-drive\nActive-active replication\nAsync Materialized View Maintenance\nWritten in Erlang\nSecondary indexes are maintained async\n\nCopy-on-Write Storage §\n\nUses a BTree-like data structure internally.\nAppend only on disk\n\nwrite document, updated leaf node, path up to the root.\nthen do an fsync followed by writing the header and another fsync for durable storage.\n\n\nSnapshot isolation is straight-forward\n\nReader grabs the last header that they can observe in that file and use whatever the header points to for all subsequent operations.\nAny concurrent writes will happen after that point in file and won’t be observed by the reader with that header.\n\n\nCrash recovery is simpler\n\nJust seek backward to find last committed header. Truncate everything after the last committed header.\nEvery prefix of a DB file is itself a valid DB.\n\n\nFiles becomes bloated over time and require regular compaction.\n\nGet all entries accessible by latest DB header, write to a new file and then drop the old one.\n\n\n\nQ/A\n\nIs there a BTree per table or is it per database?\n\nCouchDB doesn’t have tables. Just databases w/ documents in them. No native joins b/w databases.\n\n\nAre secondary indexes just B+ trees updating async?\n\nYeah. There are 2 indexes that are maintained atomically on write. Everything else is async.\nThe other atomically maintained index is for all documents that ever existed ordered by most recent update.\n\n\n\nChange Feeds §\n\nThe atomically maintained most recent update index is used for:\n\nCompaction\nMaterialized Views\nReplication\n\n\nExposed via a JSON endpoint\n\nDoesn’t require persistent connectivity. Just need to remember the last sequence number used.\n\n\nQ/A\n\nIs this index compacted?\n\nYes. But tombstone entries for deletion aren’t removed.\nAll other extra metadata is removed. (For eg: if a document is updated/deleted later on, you wouldn’t get the older update if you didn’t request the document before compaction happened)\n\n\n\n\n\nView Engine §\n\nOnly way to index documents by user defined attributes in CouchDB 1.x.\nUser supplied JS function executes in a sandboxed context.\nMaintenance is async using change feed.\n\nIf you query a view, CouchDB will refresh it automatically.\n\n\n\n// Eg: map function\nfunction (doc) {\n\tif (doc.type == &quot;post&quot;){\n\t\temit([doc._id, doc.created_at])\n\t} else if (doc.type == &quot;comment&quot;){\n\t\temit([doc.parent_id, doc.created_at])\n\t}\n}\n \n// DB\n{\n\t_id: &quot;p1&quot;\n\ttype: &quot;post&quot;\n\tcreated_at: &quot;2009/01/30 08:04:11&quot;\n\tcontent: &quot;Hello World ...&quot;\n}\n \n{\n\t_id: &quot;c1&quot;\n\tparent_id: &quot;p1&quot;\n\ttype: &quot;comment&quot;\n\tcreated_at: &quot;2009/01/30 12:30:15&quot;\n\tcontent: &quot;Amazing blog&quot;\n}\n \n{\n\t_id: &quot;c2&quot;\n\tparent_id: &quot;p1&quot;\n\ttype: &quot;comment&quot;\n\tcreated_at: &quot;2009/01/30 12:50:00&quot;\n\tcontent: &quot;Insightful post&quot;\n}\n\nHow it works?\n\nCreate a special class of document called a design document which has one or more JS functions insides that creates views.\nJS functions get executed against all documents in the DB. The functions can choose to emit 0 or 1 key-value pair for each document that they process.\n\n\nCan be used to create indexes, mimic joins, perform aggregations etc.\nView is stored using a BTree.\n\nAggregations are stored in the inner nodes of the tree.\nIn the main indexes of the DB, only basic statistics is stored. Not user controlled.\nIn the view engine, user can specify custom statistics that need to be maintained. Some functions like sum, count, min, max, approx distinct count are built-in. JS functions can be used to define custom stats. (reduce function)\n\n\nViews are maintained incrementally.\n\n// Eg:\n \n// map function\nemit([dt.year, dt.month, dt.date], doc.total_sale)\n \n// reduce function\nreduce:_sum\n \n// query\n_view/by_date?startkey=[2010, 1, 1]&amp;endkey=[2010, 2, 1]\nQ/A\n\nDoes the BTree-like data structure have variable node sizes?\n\nYeah. The chunking function on the nodes won’t have the same no. of children everytime.\nIf the aggregation gets too large,  you end up with a tall BTree as a result.\n\n\n\nReplication §\n\nYou can setup active-passive replication using change feeds.\nBut CouchDB also supports active-active replication.\nPeople take a replication of a database and then take the second instance into a disconnected environment. Eg. airline infotainment systems\n\nThe disconnected system might also take updates in its disconnected state.\n\n\nServers can disconnect for long periods of time. Writes can land in multiple locations. No edit should get lost.\n\nRevision Tracking: Hash History Forests §\n\nEach document maintains its own revision history.\nRevision ids are deterministically generated from contents of the document (including previous revision id)\nNo notion of actor like you’d get in a vector clock or a dotted version vector\nNo actual merge operation. Only one branch of history is kept alive. Update is submitted to one edit branch and other branches are marked as deleted.\nOnly the last N entries are kept but no branch is ever discarded.\n\nN = 1000 by default\nSince servers can disconnect for long periods of time, you can get a spurious conflict and when you do replication, there’s no way to link together larger edit histories and the disconnected document will appear to be a sibling of the original one. ???\nQ/A: Is the cleanup of history done on a dedicated background thread or is it cooperative?\n\nCleanup of the bodies of documents is a background thread (happens during compaction).\nCleanup of metadata (revision history information) happens on connect.\n\n\n\n\nAll “leaf” revision contents are preserved for user-driven resolution.\n\nFinal entry of each edit branch is always preserved.\n\n\n\nClustering §\n\nWas initially implemented in 2.x\nDatabases are split into shards.\nShards are replicated across nodes.\nDocuments are mapped to shards using consistent hashing on document id.\nEach replica independently chooses whether to commit and update.\n\nHow?\n\n\nSecondary indexes built local to each shard. Coordinator needs to check in with every shard to gather the final result.\n\nSince there’s no quorum on secondary indexes, a replica with stale data might respond.\n\n\nQ/A: What’s the worst behavior you’ve observed?\n\nWe keep patching the systems to avoid incorrect behaviors. Replicas that have been down for a period of time knows that its not up to date and opts out of responding to requests.\n\n\nIssues w/ this old Clustering Design (from 2.0 - 3.1.1)\n\nQuery scaling\n\nLow throughput for global query processing even with a lot of computing resources\n\n\nLagging / non-monotonic index reads (for secondary indexes)\nReplayed change feeds\n\nThe sequence index is not necessarily identical amongst all the replicas of an individual shard. Updates can get applied out of order.\nUpdates won’t be missed but users can see an update more than once.\n\n\nUnavoidable edit conflicts\n\nFor concurrent writers modifying the same piece of data\n\n\n\n\nAttempts at fixing\n\nTried to work on a Raft-style consensus mechanism among individual replicas of individual shards.\nWanted to preserve existing API, have a reliable solution, support scaling up and down.\nExplored integration with FoundationDB.\n\n\nWith FoundationDB : Consistency at Scale\n\nFDB provides strict serializability in the underlying K-V store.\nEdit conflicts for apps targeting a single CouchDB instance were eliminated (considering a single cloud region).\nHelped refocus CouchDB’s active-active replication on cross-region, hybrid and multi-cloud use cases.\nDistributed txns. in FDB enable secondary indexes (for views, search, query) to scale alongside primary data without sacrificing consistency.\nChange feed is provides a totally ordered, sortable list of edited documents. Rewinds (update can re-appear in the feed) are eliminated.\n\nFDB allows adding the version stamp into your key at commit time which allows having an ordered list of updates.\n\n\n\n\n\n\nFDB integration was later abandoned due to sponsorship issues from IBM. See https://lists.apache.org/thread/9gby4nr209qc4dzf2hh6xqb0cn1439b.\n\nQ/A\n\nIn terms of market, why do you think MongoDB and CouchDB succeeded while other NoSQL DBs like Rethink failed? Why do you think Mongo has the larger “mindshare”?\n\nWe tried offering DB-as-a-service very early on and didn’t focus on any other distribution mechanism which helped since people wanted to offload DB admin work.\nMongo had a really large DX team (much larger than core DB) and had a lot of client libraries for every new framework. CouchDB neglected this since they though that HTTP and JSON was enough.\n\n\nIf you had to write CouchDB from start again, would you go with Erlang?\n\nErlang’s process isolation and runtime debugging abilities were helpful as a startup shipping untested DB versions every week.\nHiring was an issue. The community is doing interesting things now (like Elixir) and there are other production users (like WhatsApp).\nI would probably use it again but write more lower level stuff in Rust.\n\n\nWhat’s the  biggest engineering challenge you’re facing at CouchDB now?\n\nToo lax about limits on things like max txn. duration, size of individual fields etc.\nNow, there are users who exceed these limits but their data is already in production so hardening those limits is difficult.\n\n\n\nAppendix §\n\nIntroduction to Views — Apache CouchDB\nSource: https://github.com/apache/couchdb\n"},"notes/odyssey---postgresql-connection-proxy!-2022":{"title":"Odyssey - PostgreSQL Connection Proxy! (2022)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"\nAndrey has worked on the database team at Yandex. Also a Postgres contributor since 2016.\nHe has worked on a disaster recovery system for DBs: https://github.com/wal-g/wal-g &amp; a stateless postgres query router: https://github.com/pg-sharding/spqr for sharded pg.\nOddysey was developed for Yandex services based on Postgres.\nYandex\n\nYandex.Mail\n\n~ half a billion email. inboxes\n1+ trillion+ rows, 1+ million RPS\n\n\nYandex.Cloud\n\n~10+ PB of Postgres\n\n\nAnd other services like taxi, maps, weather forecast, car-sharing, food delivery etc.\n\n\nTypical Postgres Cluster\n\n1 primary node, 3 replica nodes, 2 quorum replicated nodes (which only process read-only queries)\nno analytics on postgres, use logical replication to copy the data to Clickhouse or Greenplum\ncollect WAL &amp; backups &amp; store in S3-compatible Yandex Object Storage\n\n\n\nWhy Pool Connections? §\n\n./pgbench -j8 -c 10000 -T 60 postgres\n\nc is for no. of concurrent clients simulated\nj is no. of threads\nFor out of the box pg, on varying c, (ignoring initial connection time)\n\n-c 10000 =&gt; 86 TPS\n-c 1000 =&gt;  548 TPS\n-c 100 =&gt; 996 TPS\n\n\n\n\nSidenote: Sometimes setups w/o connection pooling can be faster when the client &amp; server app are on the same machine so not much time is lost due to network latency.\n1 backend (server connection) == 1 (unix) process\n\nEach process has its own cache for relations (tables, columns etc.), compiled PL/pgSQL data, query plans\n\n\nOLTP throughput\n\nInside the postgres core, many algorithms depend on number of connections\nThis was probably the code shown in the slide: https://github.com/postgres/postgres/blob/eae7be600be715b2f393b018fc4b98c5b89296da/src/backend/storage/ipc/procarray.c#L1474-L1540\n\nMVCC snapshot traverses the array of connections &amp; checks if the connection is active &amp; analyses it’s state.\nMemory consumed by each connection is pre-allocated on start.\nMany small algorithms depend on the size of proc array so we always want to make the proc array smaller to make those algorithms faster.\n\n\nRelated work\n\nMicrosoft Azure :  Improving Postgres Connection Scalability: Snapshots\nPercona : Scaling PostgreSQL with PgBouncer: You May Need a Connection Pooler\n\n\nPerformance is pg degrades even with idle connections.\nThe idea of connection pooling is to reduce the number of connections to the global optimum for the database.\nSidenote: You can set a connection limit for DB users in Postgres.\n\n\nWhere can we pool connection?\n\nApplication-side\nB/w app &amp; DB\nBuilt-in DB\n\nEnterprise MySQL has connection pooler built-in.\n\n\nCombinations\n\n\nIn multi-AZ or sharded setups, you can end up w/ a lot of connections.\n./pgbench -j8 -c 10000 -T 60 postgres takes ~15 seconds just for the initial connection time.\n\nSay we’ve 30K inbound connections &amp; some network switch is rebooted, we’d have to re-establish all those connections which would cost us ~10mins ??? of CPU time just due to some network glitch.\nQ/A : How frequently can this (network connection breaking) happen?\n\nAt Yandex, switches are updated every month in waves.\nConnections b/w multi-AZ are routed through external internet which isn’t predictable.\n\n\n\n\nCost of having a server connection\n\nTLS Handshake: upto 100ms\nPagefaults for fork(): upto 100ms\nProcarray, Snapshot etc. costs on each txn\nNetworking code on client’s side\nCopying system catalog for each connection is cache unfriendly\n\n\n\nWhat We Tried §\n\nThese are some of the proxy poolers that for Postgres:\n\nCrunchy-Proxy\n\nOnly session pooling\n\n\nPgBouncer\n\nTxn. pooling\nUsed by most cloud providers\n\n\nPgCat\nSPQR\n\n\nYandex used PgBouncer earlier.\nIssues w/ PgBouncer\n\nTxn. mode limitations\n\nCan’t use objects that survive through txn boundaries\n\nSession params (eg. lock timeouts)\nTemp tables\nAdvisory locks\nPrepared statements\n\n\n\n\nPgBouncer connections still cost us\n\nTLS handshake isn’t free\nPgBouncer processes connections in FIFO order\nMost clients impose 3s connection timeout\n\n\nInversion of priorities\n\nIf we’ve &gt;100% utilization, we must not shut service entirely &amp; serve new connections first (because they’ve a better chance of surviving the connection timeout).\n\n\n\n\nHAProxy\n\nWe initially used HAProxy to process traffic through many PgBouncers.\nDidn’t work well because HAProxy didn’t understand Postgres protocol well &amp; used too many PgBouncer connections sometimes.\n\n\nPort Reuse\n\nThen, we tried using Port Reuse (multiple processes bind to the same port) which worked quite well for some time.\nSee The SO_REUSEPORT socket option\nTransparent for clients. No extra moving parts.\nBut if a PgBouncer gets overloaded, it doesn’t accept new connections.\nFragmentation of idle connections among PgBouncers.\n\n\nCascading System\n\nWe then used a 2-level cascading system where we had an external array of PgBouncer processes, an internal array of PgBouncer processes &amp; then the Postgres instance.\nClient --TLS-&gt; PgBouncers[][][][][][] -&gt; PgBouncers[][] -&gt; Postgres\nStill transparent for client &amp; can withstand any load peak.\nAlso has control over idle connection count &amp; smooth restarts.\nBut, maintenance is difficult (CPU gets saturated) &amp; it didn’t provided any control over distribution of load by instances of PgBouncers.\n\n\n\nFeatures we wanted §\n\nControllable CPU scaling\nFlexible tuning\nTracing client session\nMixed pooling types\nBetter error codes forwarding\n\nOdyssey §\n\nWritten in C (along with some Assembly). Only dependency is openssl.\n\nThe assembly code is used to save the current state of the client connection &amp; swaps the call stack.\n\n\nBuilding Blocks\n\nKiwi\nMachinarium\n\n\nMachinarium : Cooperative concurrency framework\n\nLike a state machine that remembers the state of client &amp; server connection and what to do when the next byte arrives\nMainly responsible for wakeing up the server thread without using system calls.\n\nWe avoid context swap using syscalls &amp; we avoid using several processes to do multi-tasking. Why??\n\n\n\n\nKiwi : Format postgres messages\nInternal Architecture\n\n\nWe’ve a system thread which is governing other threads.\nAfter accepting TCP connect, before starting any TLS connection, it bounds client connection to the system process.\nProcess subscribes to IO events using epoll &amp; (runs ?) in response to wake up execution of “front-end” code for each client.\nMain thread is responsible for retiring old server connections &amp; running queries on the console database?.\n\n\nMulti-threading\n\nEach OS thread (maps to a Machinarium) has its own context of file descriptors that it must be in response of.\nDetails\n\nAccept(2) in separate thread\n\nTrying to solve a problem we observed in PgBouncer. We must distribute new connections evenly among all machines.\n\n\nPipelining small packets\n\nPack many small packets into a large packet.\n\n\nCache-friendly pipelining\n\nTrying to connect the client to the same server connection if possible (server connection isn’t busy)\n\n\n\n\n\n\nSome Odyssey feature\n\nEnhanced txn pooling\n\nCancel queries that no on waits for\nSidenote:\n\nMentioned that he probably wouldn’t add this feature if he was re-writing Odyssey since sometimes you cancel queries that are about to succeed &amp; CANCEL in Postgres creates a new fork.\nIf you’ve to cancel a lot of small queries, you create a wave of forks to accept cancel request on pg\n\n\n\n\nReplication support\n\nClients can migrate from cloud managed services\n\n\nPgBouncer console compatibility\n\n\nEnhanced transaction pooling\n\nTrying to keep server connection\nAutomatic ROLLBACK\nOptimization of parameter setup (SET, DISCARD)\nProtocol-level Prepared Statements support\n\nSteps: PARSE -&gt; BIND -&gt; DESCRIBE -&gt; EXECUTE -&gt; SYNC\n\nPrepared Statement is created in the Parse step &amp; is readied for execution using a Bind message.\n\n\nSee: Message Flow : Extended Query\n\n\n\n\nPrepared statement survive through txn boundaries which is why they’re not compatible with txn pooling in PgBouncer.\nHow we solve for this:\n\nFrom pg docs:\n\nIf successfully created, a named prepared-statement object lasts till the end of the current session, unless explicitly destroyed. (From pg docs)\n\n\nDifferent SQL statements can have the same name in different server connections.\nIn Odyssey, we’ve 2 hash-tables\n\nClient HT: ps_name to sql_hash\nStorage HT: sql_hash to ps_name\n\n\nps_name is Prepared Statement Name\nOdyssey doesn’t issue the PARSE statement initially. In the BIND step,\n\nLookup cmd_hash by client_ps_name\nLookup server_ps_name by cmd_hash\n\nIf it doesn’t exists: PARSE server_ps_name “original SQL cmd from client”\n\n\nActually BIND server_ps_name\n\n\nDownsides of this solution:\n\nServer Prepared statements are allocated forever.\n\nWe don’t allow the server connection to live forever to avoid cache bloat. (expire after 1 hr)\nMappings (hash-tables) consume RAM.\nHashing takes some additional time.\n\n\n\n\n\n\nStandby lag polling\n\nhttps://youtu.be/VEYdZL0bU-I?si=wLKlM29xqf0Od4—&amp;t=2816\nSometimes replicas can show stale data.\nHaving a replication lag of more than 1s is usually undesired.\nIn Odyssey, you can specify a watchdog query &amp; if this watchdog query returns some value which isn’t acceptable then the client will retry &amp; get other replica which prevents client from seeing stale data.\n\nSee https://github.com/yandex/odyssey/releases/tag/1.3\n\n\n\n\nOpen questions\n\nOptimum workload : You’ve to specify the no. of concurrent connections for each user at Yandex. We want to be able to do this automatically.\nRead-after-write consistency\nProtocol compression : Compression is currently vulnerable to CRIME attack\n\n\nAttempt : Lightweight Sharding via Proxies\n\nCurrently existing sharding solutions are based on custom Plan node which decides which shard to route to. For eg: Citus works this way. A lot of time is consumed in checking the query against the system catalog. A proxy wouldn’t have to do this &amp; could be completely stateless.\nCurrently, we rewrote Odyssey in Go (SPQR) to prototype PG sharding.\n\nSidenote: Mentioned that the Go prototype isn’t much slower than C.\nNot used in production yet.\n\n\nPgCat is another project in Rust attempting to do this.\n\n\n\nQ/A §\n\nDid you look into kernel bypass, io_uring w/ socket support etc.? Is it something that would help w/ Odyssey?\n\nDidn’t look into io_uring but we were thinking about moving closer to kernel.\nWe did look into dpdk &amp; didn’t find it to be too much of an improvement as per benchmarks.\n\n\nI’m assuming that you never put the proxy in the same box at Yandex.\n\nWe use connection pooler on the same node as the DB &amp; we use it for high-availability fencing too. When we’re unsure if we should accept connections or not, we take down connection poolers so that no client is able to access the DB. It’s faster to kill the Odyssey pooler for maintenance work.\n\n\n\nAppendix §\n\npgbench\nhttps://github.com/yandex/odyssey\nOdyssey architecture and internals\n"},"notes/postgresql-vs.-fsync-2019":{"title":"PostgreSQL vs. fsync (2019)","links":[],"tags":["db","talks"],"content":"Tomas is a long-time Postgres contributor (and now committer). Has been working with pg for 20 years.\nHow pg handles durability? §\n[ Shared Buffers (managed by PG) ]            [ WAL buffers ]\n\n[ Page Cache (kernel managed) ]\n\n[ Data Files ] | [ WAL (uses direct IO, no page cache) ]\n\n\nAny changes (Insert / Update / Delete) are written to the WAL using the very small WAL buffer through direct IO.\nChanges are then done in the shared buffers.\nOn commit, only the transaction log is flushed.\nProcess for checkpointing (happens regularly)\n\nGet current WAL position\nWrite data to page cache\nCall fsync on all files\nDelete unnecessary WAL\n\n\n\nWhat if there’s an error?\n\non write\n\npossible but not common since the copy is in memory and we can repeat the write from the original copy\n\n\non fsync\n\ncan happen quite easily due to interaction w/ drives, SAN (storage area network) etc.\nmanaged by kernel (pg doesn’t have any copies)\ncan’t retry since page cache is managed by the kernel\n\n\n\nPast Expectations §\nExpectation 1 §\nIf there’s an error during fsync, the next fsync call will try to flush the data from page cache again.\nReality\n\nThe first fsync can fail with an error and data is discarded from page cache. The next fsync doesn’t even try to flush this data again.\nThe exact behavior depends on the filesystem.\n\next4 leaves “dirty” data in page cache but the page gets marked as “clean” i.e. unless the data is modified again, it won’t be written again making failures unpredictable\nxfs &amp; btrfs throws away the data but the page is marked as not up to date (this behavior is better but not POSIX compliant)\n\n\n\nExpectation 2 §\nThere may be multiple file descriptors (fd) per file possibly from multiple processes. If the fsync fails in one process, the failure is reported in other processes too. (Eg. someone connects and calls fsync from the console on a running database)\nReality\n\nOnly the first process (initializing the fsync) gets the error.\nFile may be closed/opened independently i.e. the other process may not see the file descriptor w/ the error.\nBehavior also depends on kernel version\n\nup to 4.13, some errors may be quietly ignored\n2016 - first process calling fsync gets the error, other processes get nothing\n2017 - both processes get error, newly opened descriptors get nothing\n2018 - both processes get error, newly opened descriptor only gets it when no one saw it yet\n\n\nThe reliable way to receive the error is to keep the oldest file descriptor around.\nBut, pg didn’t do this so far. It has a small cache for file descriptors (so when one process closes the file descriptor and another process needs the file, it won’t need to do a system call)\nBehavior not limited to Linux\n\nBSD systems behave similarly (with the exception of FreeBSD/Illumos using ZFS)\n\n\n\nWhy did it take so long? §\nWhy not an issue in the past?\n\nStorage was specifically designed for DBs.\nYou built locally connected drives with a RAID controller with write cache and a small battery. It was really reliable.\nFailures were obvious since the system crashed.\nI/O errors were permanent rather than transient.\n\nWhy a problem now?\n\nSAN, EBS, NFS\nThin provisioning (using virtualization to give appearance of having more resources than available)\nTransient I/O errors are more common due to thin provisioning and ENOSPC (no space on drive error)\nWas not noticed earlier.\n\nMis-attributed to other causes like NFS\nPlenty of other sources of data corruption fixed\n\n\n\n\nOther DBs (like Oracle) manage cache internally and don’t use the kernel page cache unlike pg.\n\nHow to fix the issue?\n\nModify the kernel\n\nNot relevant in the short / mid-term since changing fsync and page cache to retry would take a lot of effort.\nPushback from kerne; developer community.\nMany systems have been running old kernel versions for a long time and would be reluctant to update.\n\n\nTrigger PANIC in pg\n\nMake sure we get the error correctly.\nTrigger Panic (crash) and recovery from WAL\nHas been implemented in pg now. See https://wiki.postgresql.org/wiki/Fsync_Errors\n\n\n\nQ/A\n\nWould using a pluggable storage engine prevented this error?\n\nNo. Pluggable storage engines also use buffered IO w/ the kernel page cache.\n\n\n\nAppendix\n\nhttps://www.postgresql.org/message-id/flat/CAMsr%2BYHh%2B5Oq4xziwwoEfhoTZgr07vdGG%2Bhu%3D1adXx59aTeaoQ%40mail.gmail.com\nhttps://www.postgresql.org/message-id/flat/20180427222842.in2e4mibx45zdth5%40alap3.anarazel.de\nhttps://lwn.net/Articles/752063/\nhttps://lwn.net/Articles/724307/\nhttps://www.pgcon.org/2018/schedule/events/1270.en.html\n\nhttps://www.youtube.com/watch?v=74c19hwY2oE&amp;ab_channel=PGCon\nhttps://docs.google.com/presentation/d/1D6wTVgLK701CDzUJ3iwcnp5tVwbSlzCd5aPJAdLbq8Q/edit#slide=id.p\n\n\n"},"notes/predictive-scaling-in-mongodb-atlas,-an-experiment-2024":{"title":"Predictive Scaling in MongoDB Atlas, an Experiment (2024)","links":[],"tags":["talks","nyc-systems","db"],"content":"Both Humeau (Staff Data Scientist) and Davis (Sr. Staff Research Engineer) work at MongoDB and were experimenting with MongoDB to see if predictive scaling would work for MongoDB Atlas (their cloud hosted service).\nMongoDB\n\nNoSQL Document Database\nMongoDB Query Language\nStrong consistency, high availability, ACID transactions\n\nMongoDB is usually deployed as a 3-node replica set.\nThe primary handles both reads and writes while the secondaries allow query scaling but their main purpose is being a hot standby.\nMongoDB can also be deployed as a sharded cluster so each replica set ends up owning a portion of the data. A routing service (mongos) routes the queries to these replica sets from the client.\nMongoDB Atlas can be multi-region, multi-cloud.\nProblem Statement: Can we save money by predicting customers’ workload fluctuation and choosing the smallest server that’ll serve them for each quarter hour or so?\nCustomers can select cluster tiers on MongoDB Atlas with different storage, RAM and vCPUs which map internally to various instances on the cloud provider.\nVertical Scaling in Atlas §\nOnce a customer requests to upgrade/downgrade their instance, Atlas takes over and automatically goes through these steps:\n\nTake a secondary offline.\nDetach its network storage.\nRestart it w/ a different server size.\nReattach storage.\nWait for it to catch up to the primary.\nScale the other secondary likewise.\nStep down the primary and scale it.\n\nThere’s a co-ordinated handoff where Atlas chooses a secondary and starts it up so that there’s even less downtime than normal.\n\n\n\nThis process takes about 15 mins (for a 3-node set).\nAuto-scaling Today §\nPrimitive currently. Atlas scales infrequently and reactively basis some rules like:\n\nScale up by 1 tier after 1 hour of over- load.\nScale down by 1 tier after 24 hours of under-load.\n\nEven if the load dis-appears entirely, the cluster won’t scale down multiple levels at once.\n\n\n\nIssues\n\nClusters can be over/under-loaded for long periods.\nClusters are already overloaded when they start scaling up.\n\nIdeal Future\n\nForecast each cluster’s resource needs.\nScale a cluster up before it’s overloaded.\nScale it down as soon as it’s underloaded.\nScale directly to the right size, skipping intermediate tiers.\n\nPredictive Scaling Experiment §\n\nThe experiment was possible because MongoDB had kept record of server’s performance metrics (workload, CPU, memory) in 1-minute intervals in a data warehouse for a few years then for 160K clusters.\nThey chose 10K clusters (of customers who had opted in to auto-scaling) and analyzed their history from previous year.\nSplit the history into a training period and a testing period.\nTrained models to forecast the clusters’ demand and CPU utilization.\nBuilt a prototype and tested how a predictive scaler would’ve performed during testing period, compared to the reactive scaler that was running at that time.\n\nComponents of the Autoscaler\n\nForecaster : forecasts each cluster’s future workload.\n\nupdated continuously (every minute or every few mins)\n\n\nEstimator: estimates CPU% for any workload, any instance size.\n\ngiven some inputs (like instance size, QPS), outputs CPU required\ndifficult because they can’t see the customer’s data or queries\nlong-lived and doesn’t depend on an individual cluster’s day to day fluctuations\nonly trained (again) when there’s a new and more efficient version of mongodb available or there’s a new cloud instance type\n\n\nPlanner: chooses cheapest instance that satisfies forecasted demand.\n\n\nPlanner §\n\nIf the planner realizes that you’re going to be overloaded on the cheap server (M40) then it’ll start the migration and upgrade the instance to the pricier M50 before you’ll be overloaded.\nSimilarly, if you’re not forecast to be overloaded, it’ll downgrade the instance back again.\nForecaster (Long-term) §\nInitially, they thought to just forecast the CPU usage.\n\nBut if you forecast the CPU usage and then use that to make scaling decisions that’ll affect the usage then it creates a circular dependency.\nEg: If the system forecasts a spike tomorrow and scales accordingly to absorb it then it’s not going to be able to forecast it.\n\nSo, they needed to forecast metrics that won’t be affected by their scaling decisions. They chose customer-driven customers like:\n\nQPS\nConnections in the cluster\nQuery complexity (repr. by the no. of items scanned by the query engine)\nDB size\n\nThese metrics can be considered to be (almost be) independent from the instance size and the state of the cluster.\n\nThey found that more than half of the customers had daily patterns in their usage and about a quarter of them had weekly patterns too.\n\nForecasting Model\n\nUsed the MSTL (Multi-Season Trend decomposition using LOESS) model for forecasting.\nMSTL takes the whole time series and tries to extract individual components in that time series.\nUsed ARIMA forecast for residuals (which are mostly randomly distributed)\n\n\nThe seasonal component is forecasted with the trend and the residuals to get a forecast for the required metrics.\nThe forecaster is called long-term because it needs several weeks of data to be trained. It’s cheap to run so it can be run often.\nAccuracy\nMedian MAPE (Mean Absolute Percentage Error)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonal ClustersNon-seasonal ClustersConnections3%50%Query Rate19%71%Scanned Objects Rate27%186%\nNot usable for non-seasonal clusters.\nInstead of maintaining a system that knows which cluster is seasonal or not, when the model runs, it also produces a score for confidence and if that’s too low, it’s not used for predictive scaling.\nForecaster (Short-term) §\nWhat about unexpected (i.e. non-seasonal) changes in demand? Or if a cluster doesn’t have a seasonal component? Or the seasonality changes?\nCan it do better than the reactive scaler?\nThe short-term forecaster uses only the last hour or two of data to try to infer what’s going to happen in near future.\nDoes simple trend interpolation (if load is increasing, it’s going to keep increasing and so on.)\n\nNot supposed to be very accurate but to save a few minutes and scale early thus avoiding scaling an overloaded cluster.\n\n\nLocal trend approach beats the naive approach 68% of the time (29% reduction in error)\nEstimator §\nThe forecasted customer-driven metrics are turned into CPU estimates.\nTakes\n\nDemand\n\nTop metrics: Scanned objects, insert rate, query rate, connections\n\n\nInstance Size\n\nvCPUS, RAM\n\n\n\nAnd outputs the CPU% (can be RAM or IOPS too)\nIn ML terms, they’re solving a regression problem to predict CPU utilization.\n\nThe model used to Gradient Boosted Trees.\nTrained successively by always training on the errors of the previous ones.\nTrained on 25mn samples (of demand, instance size and the target value which is CPU utilization)\n\n\nFor 45% of customer clusters, error is under 7%.\n\n\nConclusion §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive auto-scalerReactive auto-scalerAvg. distance from 75% util. target18.6%32.1%Avg. under-utilization18.3%28.3%Avg. over-utilization0.4%3.8%\n\nAvg. Estimated Cost Savings: $0.09 / cluster / hour\n\nFuture Work\n\nAddressing the Estimator’s shortcomings (to expand the portion of customers that they’re not able to serve now). Exploring things like:\n\nadditional data about hardware spec to the model (eg. cloud provider, generation of instance etc.)\nmodeling memory with CPU\nadding more granular data about query patterns\n\n\nRunning live experiments on real hardware to validate accuracy results\nGoal: Integrate w/ current reactive scaler. Not released yet.\n\nQ/A\n\nWhen you talk about modeling memory, what’s the unit there? Is it cache-hit rates? How can you be memory bound in a database?\n\nWe’ve 15 or 20 different memory metrics. Haven’t explored much. It includes cache size, RAM being consumed etc.\n\n\nDoes Mongo prevent queries from running if there’s not enough memory?\n\nIt doesn’t. But the diff. b/w accessing data from disk and RAM is enormous so mongodb cache as much as possible.\nCertain workloads are cache-killers while others are cache-friendly. If the DB is spending all its time waiting for the disk then the CPU will seem idle &amp; you’ll think that it’s not a bad workload whereas it’s an awful workload so we’re trying to model the whole system as one thing for the ML system.\n\n\nDo you ever run into issues where instances are just not available from the cloud provider?\n\nHaven’t heard about that happening. We use common instances and there’s always enough supply.\n\n\nHow quickly are you making the decision to scale up or scale down? The workload might keep fluctuating up and down.\n\nScaling takes about 15 minutes. The planner tries to find the cheapest instance that’ll satisfy the demand for as long as would be required before it’d have to scale again.\nWe make up plans that don’t require us to scale more often than every 15 mins.\n\n\nCompared to the reactive scaler (which only scales 1 instance at a time), how much does the predictive scaler benefit from prediction vs. from scaling by multiple tiers?\n\nDidn’t look into this in detail. From looking into the data, it’s rare that we need to scale more than 1 tier at a time. The gains are from scaling at the right time rather than having to do with scaling multiple tiers.\n\n\nHow do you determine confidence in the model?\n\nWe check how well-fitted it is to the near history. We train it on the long history but look at fitted score on recent history.\n\n\n\nAppendix §\n\nInspired by Rebecca Taft’s PhD thesis\n\nReview: https://emptysqua.re/blog/e-store-and-p-store/\nhttps://rytaft.github.io/estore.pdf\nhttps://rytaft.github.io/pstore.pdf\n\n\nAlso see, Is Machine Learning Necessary for Cloud Resource Usage Forecasting?\n"},"notes/qdrant---vector-search-engine-internals-2023":{"title":"Qdrant - Vector Search Engine Internals (2023)","links":[],"tags":["db","search","talks","cmudb-seminar"],"content":"\nAndrey is co-founder, CTO at Qdrant. Working w/ search engines since 2014.\nQdrant : Vector Similarity Search Engine, OSS, Written in Rust\n\nVector Search : Overview §\n\nYou’ve an encoder (typically a neural network) which can convert some input data into dense vector representations (also called embeddings).\n\nPair of vectors in vector space which are close to each other usually corresponds to objects which are also similar in some sense.\n\n\nThe type of similarity we want to catch is defined by the model. The distance function b/w the vectors is also defined by the model but it’s a simple dot product in most cases.\n\nQdrant : Architecture §\n\nHierarchy: Collection &lt;- Shard &lt;- Segment\n\nCollection\n\nIsolates types of data. Contains shards.\nSimilar to Table in relational DBs.\nRaft is used to keep track of metadata (eg. location of collection, configuration etc.)\n\n\nShard\n\nIsolates subsets of data. Contains segments.\nIt’s guaranteed that shard contains only non-overlapping subset of records.\nCan be moved b/w nodes &amp; replicated for higher availability.\n\n\nSegment\n\nIsolates index &amp; data storage.\nEach segment is capable of performing the same operations as a Collection but on a small subset of data.\n\n\n\n\nACID vs BASE\n\nBASE stands for Basically Available Soft-state Eventually-consistent\nA system like Postgres is a typical example of ACID database with strict transactional guarantees &amp; cares a lot about consistency of data but it’s scalability is very limited (upto the size of a single machine). On the other hand, a system like Elasticsearch is an example of a BASE system since it has weaker consistency guarantees but is very scalable.\nSidenote: https://aws.amazon.com/compare/the-difference-between-acid-and-base-database/\n\n\nFor Qdrant, scalability &amp; performance is more important.\n\nShould be treated as a search engine &amp; not as a database.\nShouldn’t be used as a primary storage. (10:30)\nA new version of model (common operation in vector DBs) will need to wipe the whole DB if the encoder is changed.\n\n\nShard Internals\n\nWAL -&gt; Segments Holder &lt;- Optimizer\nShards don’t usually have a second level of separation (segments) in other DBs.\nReasons for not putting all the data in a single segment\n\nImmutability\n\nIf the structure is only built once &amp; never extended then:\n\nData structure becomes more compact &amp; we don’t need to jump b/w different locations in memory so we’ve less cache misses.\nAll data statistics is known in advanced so we can perform various optimizations like pre-computing histograms, data distributions etc.\nAllocate the exact amount of memory needed so no memory fragmentation.\nLoading computable data structures is faster since no disaggregation needs to be done since you just copy a row chunk from disk in memory. You could even do mmaping which is even faster.\nCompress data using Delta encoding, variable byte encoding etc.\n\n\n\n\nLatency vs Throughput\n\nConcurrency  of single request is only efficient to a certain point. The closer we get to low-level index, the less efficient concurrency becomes.\nFor low latency, we can optimize CPU utilization by just assigning each CPU core with 1 segment.\nFor high throughout,  we can have a single large segment &amp; in this case it’ll maximize the throughput of the whole system by just serving each request on a dedicated core using the whole segment in read mode.\n\n\nQ/A: What’s the size of the segment?\n\nVariable. Default size is 0 if there’s no data. It just grows with the data.\nWe prefer to configure the number of segments instead of the size.\nYou can go as big as you want if you’ve the resources.\n\n\nQ/A : Is there any advantage of having segments of different sizes within the same shard?\n\nNo because we prefer to have even distribution. In practice though, it may happen because segments are joined during the insertion of data &amp; you always need to have atleast 1 segment that’s not immutable.\n\n\nQ/A : Is the segment size configurable at runtime? No. Only number of segments.\n\n\n\n\nSegments Management\n\nSome segments in a shard are immutable while others are just mutable &amp; used to insert new data.\nWe need to maintain illusion for the user that the collection is mutable &amp; that the user can insert, delete, update any data at any time.\nHow to update data in immutable data structure?\n\nUse copy-on-write. Whenever user inserts or updates data in the immutable segment,  the data is copied into a mutable segment &amp; marked as deleted in the old segment.\n\n\nHow to obtain the immutable data structure?\n\nWe need to perform long running optimizations in the segment so index building is quite long &amp; that’s why we need to keep the segment available for read &amp; updates from the user.\nWe use a proxy segment which is a special type of segment that wraps the segment being currently optimized. Also holds the list of modifications it needs to apply to resolve conflict which is happening when you copy data from old segment into new.\nWhen optimization is done, the proxy segment is converted back into regular segments (optimized segment + small copy-on-write segment).\n\n\n\n\nSegments Internals\n\n\nDidn’t describe any details. Mentioned that the concrete implementation depends on configuration. They’ve 3 different implementations of vector storage currently.\n\n\n\nVector Index §\n\nApproximate Nearest Neighbours (ANN)\nAll vectors are candidates for the response\nDifferent flavours\n\nAnnoy : tree-based\nIVFPQ : clustering-based\nHNSW : graph-based (used in Qdrant)\n\n\n\nHNSW (Hierarchical Navigable Small World) §\n\nInternally appears as a proximity graph (each vector is represented as a node in the graph &amp; those nodes are connected with a number of closest neighbours)\nGreedy search on the proximity graph (choose closest node -&gt; repeat with new selected node until the distance b/w the node &amp; target can’t be improved)\n\nNo guarantee of search resulting in the closest target\n\n\nPrecision can be controlled (more precision, less speed)\nChallenges\n\nIndex building is CPU intensive &amp; might affect search if it doesn’t have dedicated resources\nIndex has a random data access pattern. Techniques like pre-fetching, block-reading aren’t efficient.\nRead pattern is sequential (we go from one graph to another)\n\n\nSolving the challenges of HNSW index\n\nQuantization + Oversampling\n\nGenerate compressed in-memory representation of vectors &amp; use it to generate a selection of vectors\n\nSidenote: Recently added binary quantization. Single dimension represented by single bit. 32x compression. Allows comparing vectors in just 2 CPU instructions (bitwise sort, pop-count). Works well with large vectors.\n\nOpenAI provides vectors with ~1.5K dimensions.\nDocs recommend using this feature for vectors w/ 1K+ dimensions.\n\n\n\n\nThe generated selection is replaced by the original vectors. Scoring process can be parallelized unlike traversal as we already know offsets of IDs of candidates. We can also use slower storage devices.\n\n\nQ/A: What do you do with the quantized vectors against the original vectors?\n\nHNSW on quantized vector -&gt; Get selection -&gt; Re-scoring using original representation (Fetch the vectors from disk in parallel -&gt; Compute the score)\n\n\n\n\n\nFilterable HNSW §\n\nCombining vector search with other filters (in-place, not pre or post)\n\nPre or Post filtering is inefficient compared to in-place filtering (filters checked using graph traversal).\n\n\nProblem arises when the filtering condition is so strict that the graph becomes disconnected &amp; we can no longer find the path b/w the entry-point of the graph &amp; the section which contains the desired result.\nPercolation Theory\n\nHow many nodes should be removed from the graph to make it disconnected?\nCritical Threshold, pc = 1 / &lt;k&gt;\n\npc : leaved nodes fraction\n&lt;k&gt; : average node degree\n\n\n\n\nIn practice, precision collapses entirely after removing 90% of nodes.\nPayload-based refinement\n\nWe know that the filters aren’t random &amp; are based on some metadata associated with vectors.\nBuild additional links in accordance w/ expected filtering conditions so that when a filter condition is applied, the graph stays connected regardless of the strictness of search.\nThis approach doesn’t increase the search complexity. We can still perform search using original links where it’s needed &amp; utilize the extra links where it’s being filtered out. It’s also compatible with multiple (filter) fields at once.\n\nSub-graphs are merged into the main graph. We can also de-duplicate links so only a fraction of memory will be required for this &amp; search speed won’t be effected.\n\n\n\n\nNote: Important thing about this additional payload &amp; associated payload indexes is that data type of payload does matter but in most cases it’s possible to come up w/ a strategy to cover filter conditions w/ a subgraph.\n\nFor eg: For a numerical field where we don’t have exact keyword which define a strict subset of points but in this case we can build a sub-graph for overlapping intervals. We know which interval covers how many points &amp; we know the minimum threshold of how many points should be in a graph so we build overlapping intervals &amp; we build sub-graphs for those additional intervals.\n\n\n\nQ/A §\n\nIs the search guaranteed to be exact or is it also approximate?\n\nSearch is approximate. But filter conditions will be satisfied.\n\n\nAre you vectorizing those additional metadata fields?\n\nNo. We keep the payload alongside the vector &amp; check the condition during search.\n\n\nDoes quantization affect filtering?\n\nNo. It only affects the precision of the vector search &amp; not filtering.\n\n\nDoes Qdrant do any cardinality estimations to determine the query plan?\n\nYes.\n\n\nIf I had something like “customerId” &amp; know that they are always going to be separated out, would you recommend using different collections or is there a filter for that?\n\nQdrant can build HSNW graph only based on payload so we can skip building the whole main graph for all points &amp; only build sub-graphs for specific user ids. We can search for the user or still scan the whole collection &amp; we don’t have to spend as much resources for building vector index for all points together.\nNo overhead of creating multiple collections.\n\n\nWhat’s the complexity of creating this HNSW index as the no. of records &amp; dimensionality of each vector grows?\n\nDimensionality affects how fast you can compare a pair of vectors. Linear complexity.\nComplexity of graph search &amp; building it is approximately logarithmic?. It’s quite expensive to build large indexes.\nThe process of indexing involves search too since you need to first search for its neighbours &amp; then perform changes in the graph.\n\n\nCan these links cross across segments or shards?\n\nNo. Links are isolated to 1 segment.\nEach segment is queried individually during search. Similar approach is used in full text search engines.\n\n\nAre the segments random or based on some sort of spacial partition?\n\nSegments are completely random. We don’t do any kind of clustering inside when inserting points because clustering depends on type of vectors &amp; model which we don’t know in advance. Could be a good approach for a specific project where you know what embedding model you’re going to use but not applicable in the general case.\n\n\nHow are the expected filtering conditions determined? Are they pre-determined is some sort of prepared query fashion?\n\nUser can specify what fields need to be indexed.\n\n\nHow do you decide how many new links to add &amp; how much does this blow up the graph? If you’ve any predicate that has any attribute that’s even relatively high (say 100 values), you’d end up adding a lot of new edges to make it work?\n\nWe allow users to configure this value (no. of additional links) in the configuration of collections. By default we use the same amount as the original graph has.\n\nThis is usually not a big problem since they’re being de-duplicated. If a link exists in the original graph, a link won’t be added explicitly on top of it after the merge process.\nHNSW has a special heuristic which allows it to reduce redundant nodes on its own. (Is this Qdrant implementation or just in general?)\n\n\n\n\nHow does your approach with HNSW + quantization compare w/ Microsoft’s disk ANN?\n\nDisk ANN doesn’t use quantization. Data structure is similar w/o any hierarchy. Difference is in how you build it. HNSW assumes that when you insert a new point, you create a new link for this point in the existing graph.\nDisk ANN on the other hand builds a fully connected graph &amp; then prunes it.\nDisk ANN doesn’t have any in-memory storage.\n\n\nDo you’ve experience w/ checking similarities for time-series data?\n\nQdrant assumes that embeddings are made from an external model. If you’ve a model which can translate time series data into vector then you can use it.\nYou can have a model representation the series of actions as a vector. Similar to how word2vec works.\n\n\nWhat’s the biggest unsolved problem you face in your system?\n\nMaking it cheap to store 1B vectors.\n\n\n\nAppendix §\n\nhttps://qdrant.tech/\nhttps://github.com/qdrant/qdrant\n"},"notes/rockset---realtime-indexing-for-fast-queries-on-massive-semi-structured-data-2020":{"title":"Rockset - Realtime Indexing for Fast Queries on Massive Semi-structured Data (2020)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"Dhruba is the co-founder and CTO at Rockset. Founding engineer of RocksDB and HDFS. Also worked on HBase, Hive, Andrew File System (AFS).\nWhere has data processing been?\n\n2006-12: Hadoop\n\nBatch processing optimized for efficiency.\nHadoop mainly got popular because it was able to handle a large amount of data.\n\n\n2012-18: Apache Spark, Kafka\n\nStream processing optimized for throughput.\n\n\n2018-now: Rockset\n\nAnalytical applications optimized for: data latency, query latency, QPS\n\ndata latency: how much duration after the data is produced can you query it\neg of analytical apps: realtime fleet management, game leaderboards\n\n\n\n\n\nWhat is Rockset?\nRealtime indexing on massive datasets for building realtime apps on live data without ETL or pipelines.\nThe Aggregator Leaf Tailer (ALT) Architecture §\n\n\nTailers “tail” data from each data stream and translate it into internal format.\nData is sent to leaf nodes where its processed and kept ready for queries.\nThe 2-level aggregator serves SQL queries coming from applications or the dashboard.\nDifferent from a traditional Lambda Architecture &amp; Kappa Architecture. Follows CQRS pattern (writes are separated from reads). Separating the writes allows them to handle high write throughput without impacting read query latency.\nGot inspired while building the FB Newsfeed app which needs to look at a lot of data, rank it by relevance and show it.\nArchitecture is completely disaggregated so each component can be scaled independently.\n\nHigh write volume: increase tailers\nAmount of data storage increases: more leaf nodes\nMore queries: more aggregators\n\n\nSidenote\n\nScuba, Nemo at FB also do this. LinkedIn also uses the same architecture.\nRef: Aggregator Leaf Tailer: An Alternative to Lambda Architecture for Real-Time Analytics\n\n\n\nKey Benefits of ALT §\n\nMakes queries fast: indexing all data at time of ingest\nRuns complex queries on the fly: Distributed aggregator tier that scales compute and memory resources independently\nCost-effective: All components can be independently scaled up or down.\nOptimizes read &amp; writes in isolation w/ CQRS (write compute vs. read compute separation, not storage separation)\n\nConverged Indexing §\n\nRef: Converged Index™: The Secret Sauce Behind Rockset’s Fast Queries\nRockset is a NoSQL database that accepts JSON, CSV, XML or any other semi-structured data.\nRockset stores every column of every document in a row-based index, column-store index and an inverted index by default.\nQ/A: Are the indexes updated one at a time? Could I observe the row-based index being updated before the inverted index and so on?\n\nRockset doesn’t support ACID transactions but updates are atomic i.e. changes to all indexes will be observed at once and not separately.\n\n\nHow does converged indexing fit into ALT?\n\nThe tailers extract all the fields inside the provided semi-structured data.\nThe leaf houses Rockset’s converged indexes.\nThe optimizer can pick the index for the fastest query enhancing the performance of aggregators.\n\n\n\nInternals §\n\nBuilt on top of key-value store RocksDB\nShreds document into many key-value pairs\n\nTypes of keys\n\nR (Row) Keys\n\nGiven the id and field, you can find its values &amp; scan through them quickly and recreate the document you stored.\n\n\nC (Column Store) Keys\n\nData for a particular column is stored together so you can do vectorization, scanning through all value etc.\nThe columnar store is also built on top of the key-value store.\n\n\nS (Inverted Index) Keys\n\n\n\nInverted Indexing for Point Lookups §\n\n\nFor each value, store documents containing that value.\nQuickly retrieve a list of document IDs that match a predicate.\nNote: Rockset supports nested fields like JSON, Arrays.\n\nColumnar Store for Aggregations §\n\n\nStore each column separately.\nGreat compression.\nOnly fetch columns the query needs.\n\nQuery Optimization Examples §\nHighly Selective Query (will use the inverted index)\nSELECT *\nFROM search_logs\nWHERE keyword = &#039;hpts&#039;\nAND locale = &#039;en&#039;\n\n\nAssumption: As per statistics, this query will have very few results\n\n—\nLarge Scan (will use Columnar Store)\nSELECT keyword, count(*)\nFROM search_logs\nGROUP BY keyword\nORDER BY count(*) DESC\n\nQ/A\n\nAre you storing all the indexes in the same table-space of RocksDB?\n\nRocksDB has column families but we get better performance if we keep data in the same table-space (called column family in RocksDB)\n\n\nIs RocksDB the right tool for this? It seems like you’re sacrificing a lot of efficiency? RocksDB supports page-level compression but nothing else.\n\nRocksDB has delta encoding. Overhead for real-life datasets is very less.\n\n\n\nChallenges with Converged Indexing §\n\nOne new record = multiple servers update\n\nIn a traditional database w/ term sharding &amp; n indexes, 1 write incurs updates to n different indexes on n servers.\nRequires a distributed txn. (paxos, raft) b/w n servers.\nAddressing the Challenge : Doc Sharding\n\nRockset doesn’t use term sharding but doc sharding\n\nTerm sharding: splitting the terms (keywords) in the inverted index across multiple shards, each shard handles a subset of terms\n\nMost traditional DBs do term sharding. They’re optimized for throughput and efficiency but not latency.\n\n\nDoc sharding: all indices for a doc stored on 1 machine\n\nElasticsearch, Google Search, FB Search use this\n\n\nOnly paper I could find that mentions both the terms: https://www.vldb.org/pvldb/vol12/p709-archer.pdf\n\n\nDoc sharding means all new keys will only affect a single shard/lead\nUpdates are durably buffered to a distributed log\n\nWrites are piped through the distributed log and then split basis keys (eg. doc-id)\n\n\nLeafs only tail the documents in the shards they’re responsible for\nDisadvantage: Query needs to fan out to all machines and get results even though only 1 of the machines has the data\n\nWhat prevents them from keeping a metadata store to point to shards?\n\n\n\n\n\n\nOne new doc = multiple random writes\n\nTraditional systems use BTree storage structure\nKeys are sorted across tables\nA single record update with multiple secondary index would incur writes to multiple different locations\nAddressing the Challenge : RocksDB LSM\n\nMultiple records updates accumulate in memory and are written into a single SST file.\nKeys are sorted b/w SST files via compaction in a background process.\nMultiple index updates from multiple docs result in one write to storage.\n\n\n\n\n\nSmart Schema SQL §\n\nWhat?\n\nAutomatic generation of a schema based on the exact fields and types present at the time of ingest. Ingestion is schema-less.\n\n\nWhy?\n\nAvoid data pipelines that can cause data latency.\nSemi-structured data is complex and messy.\nIngest any semi-structured data (similar to NoSQL)\n\n\nUnder the Hood\n\nType information stored with values, not “columns”.\nStrongly types queries on dynamically types fields.\nDesigned for nested semi-structured data.\n\n\n\nEg:\n// Documents\n{\n  &quot;name&quot;: &quot;John&quot;,\n  &quot;age&quot;: 31,\n  &quot;city&quot;: &quot;New York&quot;\n}\n \n{\n  &quot;name&quot;: &quot;Michael&quot;,\n  &quot;age&quot;: &quot;notfound&quot;,\n  &quot;experiences&quot;: [\n    {\n      &quot;title&quot;: &quot;XYZ&quot;,\n      &quot;years&quot;: 4\n    }\n  ]\n}\n \n// collection schema\n \n| field                         | occurrences | total | type   |\n| ----------------------------- | ----------- | ----- | ------ |\n| [&quot;name&quot;]                      | 2           | 2     | string |\n| [&quot;age&quot;]                       | 2           | 2     | string |\n| [&quot;age&quot;]                       | 1           | 2     | int    |\n| [&quot;experiences&quot;]               | 1           | 1     | array  |\n| [&quot;experiences&quot;, &quot;*&quot;]          | 1           | 1     | object |\n| [&quot;experiences&quot;, &quot;*&quot;, &quot;title&quot;] | 1           | 1     | string |\n| [&quot;experiences&quot;, &quot;*&quot;, &quot;years&quot;] | 1           | 1     | int    |\n| [&quot;city&quot;]                      | 1           | 2     | string |\nQ/A: Is this an actual table that’s stored? No. This is materialized on query. There are type counters maintained on every leaf node. When we need to describe table, it queries all leaf nodes and produces the table using the stored counters.\nSchema Binding at Query Time\n\nTailers ingest data without pre-defined schemas.\nAggregator use the schema to make queries faster.\n\nChallenges with Smart Schemas §\nRef: https://rockset.com/blog/why-real-time-analytics-requires-both-the-flexibility-of-nosql-and-strict/\n\nAdditional CPU usage for processing queries\n\nUse type hoisting to reduce CPU required to run queries.\n\n\nThe S type is hoisted at beginning since the values have the same type. If a lot of values have the same type, then Rockset won’t have much overhead.\nSchemaless is storing the type with every value.\n\n\nRockset’s query price / performance is on par with strict schema systems.\n\n\nRequires increased disk space to store types\n\nUse field interning to reduce the space required to store schema.\n\nInstead of storing duplicate strings multiple times, the database stores a single copy of each unique string and uses references (or pointers) to that single instance wherever the string appears.\n\n\n\n\nInstead of 0: S &quot;City&quot;, it should’ve been 0 : S&quot;Rancho Santa Margarita\n\n\n\n\n\nCloud Scaling Architecture §\nKey insight into economics of cloud\n\nCost of 1 CPU for 100 mins = Cost of 100 CPU for 1 min\n\nWithout cloud : statically provision for peak demand\nWith cloud: dynamically provision for current demand\n\n\nGoal : resources scale up and down as needed to achieve desired performance\n\nCloud Autoscaling\n\nWhat?\n\nEach tailer, leaf or aggregator can be independently scaled up and down as needed.\n\n\nWhy?\n\nNo provisioning\nPay for usage\nNo need to provision for peak capacity\n\n\n\nTailers are easy to scale up since they’re stateless. They’re scaled up using K8s and AWS autoscaling.\nScaling Leaf Nodes §\n\nScale down : Use durability of cloud storage.\n\nRocksDB Cloud is a layer on top of RocksDB. Every time new SST files get produced, they’re pushed to cloud storage (like AWS S3, GCS) by RocksDB Cloud.\nNo data loss even if all leaf servers crash.\n\n\nScale up : Use zero-copy clones of rocksdb-cloud\n\nTakes SST files from an existing leaf shard and starts filling it in. It also starts tailing new data that tailers are generating.\nIt then becomes a part of the query process and queries through the aggregators start coming to the new leaf process.\nNo peer-to-peer replication needed so this has no performance impact on existing leaf servers.\n\n\nQ/A : Since S3 is eventually consistent, where are you storing S3 keys in your system to do strong consistent reads?\n\nRocksDB  has something called a “manifest” inside the DB. The replica reads the manifest and finds what S3 files is part of the database. If the replica doesn’t find an S3 file yet, it retries and obtains it.\n\n\n\nSeparate write compute from query compute §\nRef: https://rockset.com/blog/remote-compactions-in-rocksdb-cloud/\n\nSummary : Analytics Application on Massive Datasets §\n\nIndex rather than partition-and-scan\nSeparate write compute from query compute\nOptimized for low data latency, low query latency and high QPS\n\nQ/A:\n\nThe SST files are written to Cloud but not the WAL. Does that mean that the tailers are stateful or do they rely on upstream durability for log replay?\n\nOther than tailers that tail a data source, Rockset also has a write API which writes directly.\nRockset uses a distributed log to make things durable before it hits S3. If you already have data in sources like data-streams (Kafka) or data-lakes then you don’t need this durability.\nIf using the write API to Rockset, it uses a distributed log and does three-way replication for the last 1 or 5mins of logs before it actually hits the S3 storage system.\n\n\n\nAppendix §\n\nhttps://github.com/rockset/rocksdb-cloud\nScuba: Diving into Data at Facebook\nNemo: Data discovery at Facebook\nSlides from a similar presentation: https://files.devnetwork.cloud/DeveloperWeek/presentations/2020/Dhruba-Borthakur.pdf\nRockset was acquired by OpenAI.\n"},"notes/scalable-analysis-on-dynamic-semi-structured-data-2024":{"title":"Scalable Analysis on Dynamic Semi-Structured Data (2024)","links":[],"tags":["talks","db","search","nyc-systems"],"content":"Dan Harris is a Principal Software Engineer at Coralogix (an observability platform) where he works on the remote query system.\nThey store customer data is stored in a data lake and analytics are provided on top of that.\nObservability data is semi-structured (eg. JSON) and varies over time a lot (since new attributes keep getting added or removed to logs in a codebase).\nThe way to build a modern high scale OLAP system is with columnar data &amp; store it using formats like Parquet. But columnar data doesn’t mesh very well with complex nested data structures.\nHow do you map heterogeneous semi-structured data to a columnar format?\n\nDifficult balance b/w ingestion throughput, write amplification and query efficiency.\nPrior art: Record shredding (2010)1, Sinew (2014), JSON Tiles (2021).\n\nRecord Shredding aka Dremel Encoding §\n\nFlatten record and store the leaf value at the same path in columns.\nUse additional metadata to reconstruct the nested structure on read:\n\nRepetition levels: For repeated value (eg. an array of arrays), identifies at which level the value is repeated.\nDefinition levels: If you’ve a null value in a column then it tells you which level is the column actually null at (since if you’ve a null value at higher level in the tree then all sub-values are implicitly null).\n\n\n\n\nThis works well if you’ve a nested relatively static structure with a fixed schema. But they had some issues:\n\nIn very wide tables, construction of nested values during scan can be very expensive\n\nTo produce the full record, you need to read all columns.\nSlows down reading but also data compaction during ingestion.\n\n\nRequires tracking globally merged schema.\n\nVery difficult when ingesting many GB/s or very heterogeneous data.\n\n\nRecords get coerced into a globally merged schema.\n\nAdds a lot of random noise from spares values which get “imputed” into all records when read. (even an originally small document would end up with a lot of extra fields with null values when read)\n\n\nDealing with type conflicts requires another layer of complexity.\n\nThe Arrow format supports union types so you can resolve type conflicts by lifting types into union but it adds a lot of complexity.\n\n\n\nSinew §\nThe key insight of the Sinew paper is that the logical structure of the data that doesn’t need to match the physical representation.\n\nLogical schema is the merged schema of all records in the table.\nPhysical schema is partitioned into physical and virtual columns.\nPhysical columns are materialized and stored in their own column (or row in a row-oriented DB).\nVirtual columns are stored together in an efficient binary format.\n\nThe schema analyzer decides what columns to materialize into physical columns as data evolves.\nDuring query time, you can track which columns are physical or virtual. Anything that’s not a physical column and materialized can be materialized on the fly\nThe schema analyzer proposed in the paper was in the context of a single node DB. Very difficult to implement it for hundreds of nodes. So, they decided to not go ahead with this.\nJSON Tiles §\nExtends the original ideas behind Sinew. It provided a new method for clustering records to maximize efficiency of materialized columns.\n\nDuring data ingestion, group records into fixed size chunks (“tiles”).\nUse FP-Growth algorithm to identify “frequent item-sets” (eg. common fields in records).\nExchange records b/w tiles to group more similar records together in the same tile.\nNon-materialized columns are stored in a binary json format.\n\nThis can be just done locally on a node and you don’t need to do this in a distributed manner.\nFP-Growth isn’t efficient and becomes expensive to run at scale.\nSynthesis : CX-Data §\nThey created their own format: CX-Data.\n\nUses Parquet as physical file format.\nBuild on insights from the Sinew and JSON Tiles paper.\n\nDoes partial materialization of leaf values into separate Parquet columns.\nStore the original raw record in an efficient binary format.\nDynamically materialize virtual columns during query execution if required.\nAvoid materializing complex nested structured, only scalar values and arrays of scalars.\n\n\nAvoid costly coordination during ingestion &amp; query execution.\n\nDecisions about materialization are local to an ingestion task.\n\nThey decide what to materialize by taking the top N least sparse columns in the chunk of data.\n\n\nAll schema information during query planning is inferred from the query itself with no external catalog.\n\n\n\n\nSince parsing JSON strings on the fly while scanning data for reads is really slow, the raw record needs to be stored in an efficient format. There were a lot of existing binary JSON formats like: Postgres JSONB, BSON (MongoDB), SQLite JSONB, Amazon ION, MessagePack etc.\nBut, they made their own binary JSON format based on Postgres JSONB.\nPostgres JSONB\nJsonbValue\n+---------------------------------------------------------------+\n| [JEntry]  [JEntry]  [JEntry]  |  [Value]  [ ] [   ] ... [  ]  |\n+---------------------------------------------------------------+\n \nJEntry (32 bits)\n+---------------------------------------------------------------+\n| 0 | 1 1 0 | 1 0 1 0 0 ...                                     |\n+---------------------------------------------------------------+\n  |    |          |                                           \n  |    |          +-- length/offset of value (scalar) OR children count (container)\n  |    |                \n  |    +------------- type (object, array, string etc.)                                 |  \n  +----------------- length/offset                          \n \nA JsonbValue has fixed length JEntries followed by variable length values.\nFor simple scalar values, either the length or offset is stored in the remaining 28 bits.\n\nAt like every 32nd JEntries, they store an offset and the rest store the length so you’ve to never traverse more than 32 JEntries to find the offset into the value buffer for a JEntry.\nContainers like array or an object store the no. of children i.e. items in the array OR key-value pairs in the object.\n\nJsonA / JSON Arrow : Binary JSON format for the columnar world §\nIssues with existing formats\n\nDesigned for row/document stores.\nDoesn’t cleanly separate structure from data.\nYou store everything together so you can’t reduce IO. You’ve to read the whole column as a column chunk and you can then sort and traverse though there &amp; it’s fast to traverse through and find the value but in a data lake architecture you want to eliminate IO as much as possible.\n\nSmall tweaks to adapt the Postgres JSONB format to column stores\n\nTake the JEntries and variable length values and separate them into different columns.\nSplit the variable length columns into 2 separate buffers. One for keys and another for values.\n\nThe keys are repeated a lot so you can use dictionary encoding here. Also, if you want to find a particular path in an object, you just need to read the keys without reading the value buffer (where the bulk of the data lives).\nYou can also prune I/O by converting a query like path = 1 to path != NULL AND path = 1 for predicate filtering.\n\n\n\nSeparating the structure from value data:\n\nMade traversing structure (key lookup) cache-friendly\nMinimized IO when queries can be resolved only based on structure\n\nOther benefits of JsonA\n\nFaster to parse from JSON string\n\nBuild the structure directly from parse w/o intermediate representation\nIn their benchmarks, ~40% faster to parse to JsonA than to serde_json::Value2\nThere’s no intermediate state (like a parse tree) in parsing JsonA that you’ve to build unlike JSON. You build the structure as you process the input tokens.\n\n\nFast value extraction\n\nCan extract multiple values from single traversal\n\n\nIsNull, IsNotNull, IsTrue, IsFalse checks can be done purely with nodes and keys\n\nFor predicate pushdown, IO can be eliminated for values\n\n\nString values are stored already unescaped\n\nEliminates parsing overhead for full-text search\n\n\n\nQ/A\n\nIf the decision for what column to materialize is done on a per-chunk basis, does that mean during query execution each chunk has very different set of columns?\n\nIt makes things like planning partitioning (for distributed query execution) difficult since you don’t know where data is materialized and where it’s not materialized. One partition may take significantly longer than other since a column might be virtual for those chunks.\n\n\nDo you have any background process that looks through all the chunks and reorganizes them?\n\nNo. It’s not currently feasible due to the amount of data. Working on smarter ways to do materialization.\n\n\nWhat’s the target size for ingesting Parquet files?\n\n~ 50-300 MBs.\nThis can get compacted into larger files.\n\n\nIn practice, how varied are schemas are w/ customer workloads?\n\nExtremely varied.\nWe dump data into big silos. All logs (even from different systems) go into a single table.\nEvery once in a while, you’ve someone who tries to make UUID a key in the object. We’ve ways of preventing that from blowing anything up but it’s a constant problem.\n\n\n\nFootnotes §\n\n\nIntroduced in Dremel: Interactive Analysis of Web-Scale Datasets. Also see A Look at Dremel. Parquet (https://blog.x.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet) also uses Dremel encoding. ↩\n\n\nhttps://github.com/serde-rs/json ↩\n\n\n"},"notes/snowflake-database-architecture-overview-2022":{"title":"Snowflake Database Architecture Overview (2022)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"\nRemarks\nSnowflake has a lot of moving parts so it was great to hear from multiple engineers on different things they’ve worked on.\nWhile using aggregations to skip scanning data isn’t new in analytical systems, I liked how aggressively Snowflake filters unnecessary data at every step (data file downloads, join, query optimization) using aggregates on smaller portions of data in a cloud-native context.\n\n\nArchitecture Parts\n\nStorage Layer\n\nCloud Blob Storage (eg. S3)\n\n\nCompute Layer (Virtual Warehouse)\n\nCloud Compute Instances (eg. EC2)\n\n\nCloud Service Layer\n\nMetadata, Query Optimizer, Data Sharing etc.\n\n\n\n\n\n\nCloud Storage / Compute §\n\n\nMicro Partitions\n\nImmutable columnar files for data storage (~10MB)\nData in each column grouped together\n\nFile header stores the region of each column\nNon-selected columns can be skipped when downloading the file (file header is downloaded first to check this)\n\n\nColumn metadata (eg. MIN/MAX, distinct) of each file is generated\n\nAvoids unnecessary file download\nEssential for query performance\n\n\n\n\n\nQuery Execution : Overview\n\nColumnar\n\nData of the same column is grouped together when possible\nAllows effective use of CPU caches, SIMD instructions, and compressions\n\n\nVectorized\n\nOperators handle batches of a few thousand rows in columnar format\nAvoid full materialization\nImproves cache efficiency\n\n\nPush-based\n\nOperators push results to the downstream\nData is processed in a pipelines fashion\nEnables efficient processing of DAG-shaped plans (in addition to tree-shaped plans)\n\n\n\n\n\nQuery Execution : Major Components\n\nTable Scans\n\nResponsible for downloading data from remote locations, decompressing &amp; providing input to other operators\nFiles are distributed among a set of workers using consistent hashing (to minimize movement if case the warehouse is resized etc.)\nWorkers can steal file from others if they finish early\nFiles are cached on the local disks for later queries to reuse\n\n\nJoin\n\nDistributed Hash Join w/ runtime-adaptive distribution method selection (i.e. join method is selected based on the runtime data)\nBuild bloom vectors &amp; maintain ranges on the build keys for effective runtime data pruning\nAutomatic skew detection &amp; mitigation (hot keys are detected in the build or probe phase &amp; mitigation is done by distributing data to different workers)\n\n\nScheduling of Operators\n\nPipeline-wise lockstep execution (workers work on the same execution pipeline until they’re done and then move on to the next one), exploiting intra-operator parallelism\nPlan fragments with not much data can be scheduled on single nodes in parallel, exploiting inter-operator parallelism\n\n\n\n\n\nCloud Services Layer §\n\nCoordinator b/w client, metadata, compilation, warehouses &amp; the cloud provider\nResponsibilities:\n\nSecurity: Authorization &amp; Authentication\nBackground Services: Clustering, Compaction etc.\nCloud Infrastructure\nService Management\n\nMulti-tenant system w/ a variety of customer workloads\nChallenges\n\nEnsure no one customer overloads the system\nEnsure failed nodes &amp; jobs are retried\nProviding predictable performance for all workloads\nEnsure enough cloud VMs are available\n\n\n\n\nMetadata\n\nSnowflake uses FoundationDB as its metadata storage layer\n\nFDB is a distributed key/value store w/ ACID properties.\nMetadata for all DB objects is stored in FDB.\nStores data about tables, columns, users, roles, masking policies, views, schemas\nPartitioned to silo each customer by using account as the top-level key\nAlso used to grab locks on table\n\n\nCloud service layer retrieves metadata during query compilation\nBg services run regularly to clean up &amp; compact metadata\n\n\nConcurrency Control\n\nSnapshot Isolation &amp; MVCC\nImmutable data files make SI a good choice. Why?\nTable versions are updated for every DML operation\n\nDML: Data Manipulation Language (insert, update, delete etc.)\nEvery DML operation generates new data files\nCan consider the set of files before &amp; after the operation as entirely different version of the table\nQueries use their version &amp; thus don’t end up competing for locks on some resource. They don’t see uncommitted data or read outside their version\n\n\nVersioning also facilitates time-travel, cloning &amp; result reuse\n\nSnowflake retains older versions upto some time (on S3) which the customer can use to recover lost data\nTable version is also used in the query cache which allows result re-use if nothing has changed on previous computation\n\n\n\n\nQuery Compilation: Parsing, Optimization &amp; Query Planning\n\n\n\nQuery Compilation: Parsing, Optimization &amp; Query Planning §\n\nOverview\n\nParsing: generates an AST from the query text.\nSemantic analysis: tables &amp; columns are looked up to verify they exist &amp; type-checking is done on the query\nLogical Rewrites: Parse tree is rewritten\nPruning: Prune out data files using the predicates for the query with micro-partitions. More pruning is done during plan rewrite since more detailed information is available.\n\n\nSome Optimizations\n\n\n(Meta) Data Dependent\n\nPruning\n\nFor eg. A query needing all values of c1 &gt; 5 can check the min, max value of the c1 column in data files to eliminate them instead of reading all the files\n\n\nConstant Folding (if you detect that a column will always have the same value, you can replace it with a constant &amp; save execution time and IO)\n\nFor eg: The min,max value for a column c2 in some data file is 2 then all the values for the column are 2. So that file can just be replaced by a constant value.\n\n\nConstant File Detection (Compiler was able to detect that for some table scan, all the data files remaining for the table scan have the same values in all their columns so its replaced by a constant set of values)\n\n\nAdaptive (Query planner doesn’t give a fixed plan &amp; indicates to execution plan that it can make a decision)\n\nJoin Filters\n\nFilter pushed down from join to run earlier\n\n\nAggregation Placement\n\nSome aggregates can be evaluated earlier or later for better performance\n\n\nAdaptive Links\n\nAllows execution engine to determine whether to keep all data in a single worker or distribute it depending upon cardinality\n\n\n\n\n\n\nSome interesting problems\n\nCompilation Time (balancing w/ execution time)\nCore optimizations\n\nSnowflake is a newer system that doesn’t have a lot of optimizations that older systems do. Snowflake doesn’t allow customers to disable optimizations so they need to figure out releasing optimizations to all customers w/o regression on varying workloads.\n\n\nAdaptive optimizations\n\nRe-optimization\nRuntime pruning\n\n\nOptimizing non-OLAP workloads (OLTP, Data Science, External Functions, Spark-like)\n\n\n\nWorkload Optimizations §\n\nFeatures that benefit the performance &amp; cost of customer workloads.\n\nClustering\nMaterialized Views\n\nTrack new data to keep the view updated.\nDo runtime rewrites (combine results from materialized views &amp; new data) to get the final result\nInstead of maintaining a materialized view for every DML, maintain materialized views of different partitions\n\n\nResult Reuse (exact match caching)\nQuery Acceleration Service (QAS)\n\n\n\nQuery Acceleration Service (QAS) §\n\nProblem : Warehouse size has to be decided by human. Unable to scale based on actual compute demand of each individual query. Workload is different for large &amp; small queries. Keeping a large warehouse has unnecessary costs while a smaller warehouse would have high query execution time for large queries. Table scan is to blame in many cases for these execution times.\nQAS executes a fragment of the plan at larger scale. It targets plan fragments with:\n\nLarge Table Scans\nHigh reduction (lots of data in -&gt; small amount out after filtering, grouping)\nNo data exchange (b/w servers with different fragments) required until the last step when the fragments merge\n\n\n\nIntermediate results are materialized as regular data files. They can also be re-used.\nQAS Query Execution\n\nNormally, files for a scan are distributed during compilation time. In a Continuous ScanSet, files are distributed in batches at runtime upon request. (Server asks for files from query coordinator which provided it in batches)\n\nFailed batches are sent to warehouse for retry. Successful ones are checkpointed in the staging area &amp; removed.\nWarehouse fetches the materialized result files after everything is done.\n\n\nQAS Insert Optimization : For some INSERT queries (where a table scan is being done to insert data into some other table), the entire plan can be accelerated directly.\n"},"notes/storing-small-things-in-big-places-2025":{"title":"Storing Small Things in Big Places (2025)","links":[],"tags":["db","talks"],"content":"Carl Sverre\n\nAuthor of SQLSync (multi-player SQLite for local-first apps).\nWorked at SingleStore earlier.\n\nMotivation\n\nWhile SQLSync got popular, Carl realized that the backend wouldn’t scale when he was trying to turn SQLSync into a product.\nWhat would be the ideal backend to make SQLSync work?\n\nTransactional object storage providing lazy partial edge replication at global scale.\n\n\n\nLazy\n\nClients choose when to sync.\nChanges are rolled up, allowing clients to fast forward.\nPartial\nClients choose what to sync.\nClients learn what changes via sync and pull the actual data separately.\nEdge\nClients choose where to sync.\nClient is lightweight and embedded in the application.\nReplication\nClients see consistent snapshots of data. (internal consistency, snapshot isolation)\nOptimized for cost and reliability (at the cost of performance).\n\nCarl ended up building Graft since he couldn’t find anything that met all his requirements.\nPhysical Organization §\n\nData is organized into Volumes.\nVolumes are sparse ordered sets of Pages.\n\nSQLite stores data in BTrees which map down to Pages.\nGraft Pages are small (4KB).\nFor this talk, a page is a fixed-size block of bytes w/ the size being a power of 2.\n\n\n\nArchitecture §\n\n\nPage Store: stores pages durably on an object store.\nMeta Store : maintain commit history of a volume.\n\nMetadata of snapshots (version no., set of segments and offsets that’ve changed inside of those segments).\nPerformance is limited by object storage. SQLSync isn’t intended for realtime high performance but for async offline-first replication.\n\n\n\nClient uploads changed pages (Durability Phase) §\n\n\nPage Store is hosted on Fly.io. Client can connect to any page store world-wide. Page store instances buffer up many pages from different writers into segments. Writes are buffered and flushed every second.\nThe object store returns a list of segment IDs that contain the pages corresponding to the client’s write request along w/ which offsets the client uploaded in each segment. Eg: Upload 10 pages which get split into say 2 segments. Offsets 1-5 landed in the first segment and so on.\nAt this point, the data has been fully synced to object storage.\n\nClient commits segment metadata §\n\n\nHow do we commit data into the next snapshot version of the volume?\nClient takes an array of segment metadata (segment and page offsets) and sends it to the meta-store which is responsible for ensuring that the version no. of a volume is monotonic and atomic.\nOnly a single writer commits on a particular snapshot version.\nEg: Client is at v1. Wants to make a write and transition to v2. It’ll send the segment metadata to the meta store and if v1 is the current latest version on cloud then meta-store will commit the change. Uses CAS (Compare-and-Swap) on object store. If it works then the meta-store confirms that snapshot is committed to the client.\nIf there’s a conflict, the meta-store denies the request. The client has a list of segments &amp; all the pages it wrote. It also retrieves the new snapshot version from the server and it can see what offsets have changed.\n\nThe client can do a variant of MVCC locally because it has its own read and write set. It can check if the pages uploaded to the page store are valid or if they need to be recomputed. If the client decides that pages need to be updated, it can replay the commit on the new snapshot version.\nDoing client-side MVCC allows keeping the page store distributed instead of centralizing it.\n\n\nQ/A\n\nAre you actually waiting for the write to flush entirely to S3?\n\nYes. Potentially in the future, a raft consensus layer w/ fast writers on NVMe disks can respond immediately.\n\n\nHow tightly coupled are Graft pages and SQLite pages? Can a SQLite page cross multiple Graft pages? Would Graft work for other DBs?\n\nTechnically, anything that can be mapped to the 4KB page size of Graft can be used w/ it. He has used it w/ key-value store, file-systems. Mis-aligned pages will have overhead though.\n\n\n\n\n\nClient pulls a set of changed page offsets §\n\n\nNever need to store more data than what describes changing every page in the set because if the same page is changed multiple times, we only need to know that it’s changed and not that it changed multiple times.\nClient requests changes since a version. Meta Store is stateless so it first refreshes the commit index and then computes all of the offsets that’ve changed b/w the original version and the latest version.\nIf the volume is a 1000 pages, the max. data the client will receive is a 1000 u32’s and a single u64.\n\nClient downloads pages as needed §\n\n\nOnce the client receives the set, it knows that new versions of pages exist and it needs to retrieve the pages it wants to read (doesn’t need to read the rest but it can).\nClient maintains a set of snapshots in its local state.\nSome threads in an app might still be operating on a previously downloaded snapshot.\nGraft allows the client to maintain local snapshots at different versions.\nIf a page@version isn’t available in the local Graft cache, the client requests the page from the pre-fetcher.\n\nThe pre-fetcher expands the requests for a single fetch into multiple based on patterns (like forward and reverse sequential reads, strided reads) using the Leap algorithm.\n\nstrided : constant offset b/w consecutive reads.\n\n\n\n\nThe requests are then sent to the page store. Page store either has the segment cached in memory, NVMe disk or requests it from the object storage.\n\nQ/A\n\nDo you calculate roll-ups on the metadata read everytime?\n\nYes. When you do a poll, the metadata server calculates the rollups. To do this efficiently, a small set optimized (alternative to Roaring Bitmaps) is used.\nRoaring Bitmaps is a compressed bitset. It doesn’t work well w/ small no. of bits that are highly sparse. So Carl wrote a compressed bitmap (Splinter) which also allows extremely quick intersections and is zero-copy. Pages can be mapped from object storage directly to memory w/o any de-serialization. After verifying that the pages aren’t corrupt, they can be directly sent to and used by the client since it uses the same compressed bitset representation for calculations.\n\n\nWhat’s the relation b/w the pages and the actual files on S3? You mentioned batching for fewer requests.\n\nGraft uses segments which is just a collection of pages by many clients, into many volumes &amp; at potentially different versions.\nIn the case where you’ve millions of volumes, each segment would have thousands of pages.\nFuture work: Like LSM trees that run on object storage, you’d want to merge and optimize data from sparse segments (different volumes) to segments with fewer (and larger) volumes. Files would be as dense as possible and optimized to read subset of segments into memory.\n\n\n\nAppendix §\n\nhttps://github.com/orbitinghail/graft\nhttps://sqlsync.dev/posts/stop-syncing-everything/\n"},"notes/testing-distributed-systems-w-deterministic-simulation-2014":{"title":"Testing Distributed Systems w Deterministic Simulation (2014)","links":[],"tags":["talks","db"],"content":"FoundationDB is a distributed stateful system with strong ACID guarantees.\nDebugging distributed systems is notoriously hard. Especially because there are sources of randomness that we don’t control (like the network, disk etc.).\nDon’t debug your system, debug a simulation instead.\n\nThey wrote a totally deterministic simulation of a database first, exhaustively debugged it and then wrote the database which is just the simulation + talking to network and disks for real.\n\nFor a couple of years, there was no database, only a simulation.\nIt involved simulating a network of communicating processes plus all the other interactions they’ve with their environment (network, disk, os) all within a single physical process.\n\n\n\n3 Ingredients of Deterministic Simulation\n\nSingle-threaded pseudo concurrency\n\nActual concurrency is a source of non-determinism.\nThey implemented a syntactic extension to C++ (called Flow) to let them use actor model concurrency but compiles to single threaded C++ callbacks.\nImplementation of the actor gets broken up across methods in the class. Everytime there’s a wait statement, a function in the class is going to end and set a callback on that future (which points to the next part of the actor). Then, its going to yield immediately to the centralized scheduler.\nAt the very end, it’s going to fulfill the future and clean itself up.\nNote: Other languages might not require an implementation like this.\n\n\n\n// Source\nACTOR Future&lt;float&gt; asyncAdd(Future&lt;float&gt; f, float offset) {\n    float value = wait(f);\n    return value + offset;\n}\n \n \n// Generated\nclass AsyncAddActor : public Actor, public FastAllocated&lt;AsyncAddActor&gt; {\npublic:\n    AsyncAddActor(Future&lt;float&gt; const&amp; f, float const&amp; offset) : Actor(&quot;AsyncAddActor&quot;),\n        f(f),\n        offset(offset),\n        ChooseGroupbody1W1_1(this, &amp;ChooseGroupbody1W1)\n    {}\n \n    Future&lt;float&gt; a_start() {\n        actorReturnValue.setActorToCancel(this);\n        auto f = actorReturnValue.getFuture();\n        a_body1();\n        return f;\n    }\n \nprivate:\n    // Implementation\n    ...\n};\n \nFuture&lt;float&gt; asyncAdd(Future&lt;float&gt; const&amp; f, float const&amp; offset) {\n    return (new AsyncAddActor(f, offset))-&gt;a_start();\n}\n\nSimulated implementation of all external communication\n\nThey already had a multi-platform program with interfaces for talking to disk, network etc. so they just added a simulated version of those.\n\nThe network implementation waits and simulates latency on the receiving and sending side, copies data around and lets the sender know that it received some bytes.\nEverytime you read from a simulated network, there’s a chance that something terrible will happen. This helps smoke out any code in the system that might’ve been assuming that the network is reliable.\n\n\n\n\n\n// Reads as many bytes as possible from the read buffer into [begin, end] and returns the number of \n// bytes read (might be 0)\n// (or may throw an error if the connection dies)\nvirtual int read(uint8_t* begin, uint8_t* end) {\n    rollRandomClose();\n \n    int64_t avail = receivedBytes.get() - readBytes.get();  // SOMEDAY: random?\n    int toRead = std::min&lt;int64_t&gt;(end - begin, avail);\n    ASSERT(toRead &gt;= 0 &amp;&amp; toRead &lt;= recvBuf.size() &amp;&amp; toRead &lt;= end - begin);\n    for (int i = 0; i &lt; toRead; i++)\n        begin[i] = recvBuf[i];\n    recvBuf.erase(recvBuf.begin(), recvBuf.begin() + toRead);\n    readBytes.set(readBytes.get() + toRead);\n    return toRead;\n}\n \n \nvoid rollRandomClose() {\n    if (g_simulator.enableConnectionFailures &amp;&amp; g_random-&gt;random01() &lt; .00001) {\n        double a = g_random-&gt;random01(), b = g_random-&gt;random01();\n        TEST(true); // Simulated connection failure\n        TraceEvent(&quot;ConnectionFailure&quot;, dbgid)\n        .detail(&quot;MyAddr&quot;, process-&gt;address)\n        .detail(&quot;PeerAddr&quot;, peerProcess-&gt;address)\n        .detail(&quot;SendClosed&quot;, a &gt; .33)\n        .detail(&quot;RecvClosed&quot;, a &lt; .66)\n        .detail(&quot;Explicit&quot;, b &lt; .3);\n        \n        if (a &lt; .66 &amp;&amp; peer) peer-&gt;closeInternal();\n        if (a &gt; .33) closeInternal();\n        // At the moment, we occasionally notice the connection failed immediately.\n        // In principle, this could happen but only after a delay.\n        if (b &lt; .3)\n            throw connection_failed();\n    }\n}\n \n\nDeterminism\n\nNone of the control flow in your program should depend on anything outside of the program + its inputs.\nFor eg: if you’re using a random number generator, you should be able to control it and its seed should be part of the program.\nIf you do things like check time, disk space etc., your program becomes non-deterministic.\n\nThey just try to avoid it. ??\n\n\nA small percentage (~1%) of their simulation runs twice with exactly same inputs and check if their assumptions match what happened.\n\n\n\nThe Simulator\nThey find bugs with the help of test files. Test files declare\n\nwhat the system is going to try to achieve\nwhat its going to prevent the system from achieving\n\ntestTitle=SwizzledCycleTest\n\ttestName=Cycle\n\ttransactionsPerSecond=1000.0\n\ttestDuration=30.0\n\texpectedRate=0.01 // only expecting 1% to complete\n\t\n\ttestName=RandomClogging\n\ttestDuration=30.0\n\tswizzle=1 // takes a subset of network, stops on rolling basis\n\t          // and brings back up in reverse order\n\t\n\ttestName=Attrition\n\tmachinesToKill=10\n\tmachinesToLeave=3\n\treboot=true\n\ttestDuration=30.0\n\t\n\ttestName=ChangeConfig // of the database | designed to provoke coordination change\n\tmaxDelayBeforeChange=30.0\n\tcoordinators=auto\n\nIn a cycle test, they insert a ring of keys and values into the database such that they’re each pointing to the next one (eg. value 1 is pointing to key 2) and then they execute txns. concurrently.\nEach txn. is going to mutate the keys and values in the database subject to a constraint that each txn. taken as a whole preserves the ring. This provides an easy to check invariant to tell whether ACID was violated.\nIn case of atomicity failure, one of the txns. would partially execute and the ring would be broken. If there was a lack of isolation, the ring would break.\nOther simulated disasters\nIn parallel with the workloads, they run other things like:\n\nbroken machine\n\nsimulate gradual failure\nall future system calls have like 1% chance of returning an error\n\n\nclogging\nswizzling\nnukes\n\ndatabase is aware of rack topology etc. so they also simulate killing an entire datacenter\n\n\ndumb sysadmin\n\natomically swap the IP addresses of two machines\ncopy data files from 2 computers onto each other and switch their hard drives and see if that results in any data loss\n\n\netc.\n\nHow do you find bugs before the customer does?\n\nDisasters here happens more frequently than in the real world.\n\nConnections are dropped pretty regularly.\nIn the real world, disk fails every 2-3 years. They make it fail after every 2-3 mins.\nIf there’s a bug, they’ve many more failures per CPU hour than their customer.\nA lot of the time the cluster might be waiting for a fixed amount of time for something to happen during recovery etc. They make simulated time pass which gives them more real world time to  find bugs.\n\n\nBuggify!\n\nBuggify is a macro that randomly changes what your code does with some probability when the code is running under simulation.\nAlso see: https://transactional.blog/simulation/buggify\n\n\n\nif (BUGGIFY) toSend = std::min(toSend, g_random-&gt;randomInt(0, 1000));\n \nwhen( Void _ = wait( BUGGIFY ? Never() : delay( g_network-&gt;isSimulated() ? 20 : 900 ))) {\n    req.reply.sendError( timed_out() );\n}\n\nThe Hurst Exponent\n\nHardware failures are not random independent events. It could’ve been a bad batch, there might’ve been a humidity problem at the data center etc.\nMultiple cascading failures are hard to test in real life but easy to do in a simulation.\n\n\n\nStepping through code infused with callbacks is horrible. Traditional debugging approach doesn’t work here. Left with printf debugging (but atleast it’s in a deterministic simulation).\nWhat if the simulation is wrong?\n\nThe simulation isn’t brutal enough.\n\nThere’s a pattern that occurs in the real world but not in the simulation.\n\n\nMisunderstanding of the contracts of OS and hardware\n\nSome syscall had a slightly different guarantee on a new platform.\nThe OS could have a bug.\n\n\n\nThey’ve a backup cluster called “Sinkhole” connected to programmable network power supplies. It turns off and on all night while the database is running.\n\nNetwork power supplies burn out after 2 days.\nThey haven’t found a bug in their database doing this.\n\nSinkhole found 2 bugs in other software.\n\nPower safety bug in Zookeeper\n\nThe bug was fixed but they removed Zookeeper from their stack and wrote their own Paxos in Flow.\n\n\nLinux package managers (apt) writing conf files don’t sync.\n\nFuture directions\n\nDedicated red team\n\nHave folks intentionally introduce bugs and verify if the simulation catches them or not.\n\n\nMore hardware\n\n10K simulations are run after changes which takes a lot of time. A larger suite runs 10mn simulations every night.\nThis takes a lot of time. Adding more hardware would increase productivity.\n\n\nTry to catch bugs that “evolve”\n\nProgrammers might get habituated to the simulation framework and code in bugs that’d pass the simulation.\nThey’re trying to have 2 different simulations. One for frequent development use and another that runs before a release.\n\n\nMore real world testing\n"},"notes/umbra---a-disk-based-system-with-in-memory-performance-2022":{"title":"Umbra - A Disk-Based System with In-Memory Performance (2022)","links":[],"tags":["db","talks","cmudb-seminar"],"content":"\nRemarks\nUmbra has done a lot optimisations to get in-memory performance on a hybrid system. They adaptive query compiler that uses LLVM &amp; generates machine code is the most interesting part.\n\nMotivation §\n\nWorked on a purely in-memory system (HyPer) earlier.\nPure in-memory systems are fast but expensive.\nGrowth in memory sizes stalled compared to what was expected.\nNow, SSD-based systems are quite fast &amp; cheaper than main memory (DRAM). ($500 vs $20000 for 2TB).\nSSD-based systems can still use large in-memory buffers.\n\nWorking set (not the entire dataset) often fits entirely into main memory.\nBut, using traditional buffer management limits performance even though everything is in memory.\n\n\n\nWhat Umbra Does §\n\nCombine large in-memory buffers w/ fast SSDs\nNovel buffer manager allowing in-memory performance\nUmbra is an evolution of HyPer towards an SSD-based system\n\nComparable performance if working set fits in main memory\nTransparent scaling beyond main memory when required\nSome parts like the compiling query execution engine is the same as HyPer\n\n\nKey additions &amp; adaptations\n\nScalable buffer management with variable-size pages\nAdaptive compilation &amp; modular execution engine\nStatistics maintenance\nString handling (didn’t talk about this)\n\n\n\nBuffer Management §\n\nTraditional approach : Fixed-size pages\n\nBenefits of using fixed size pages: Recovery, memory management is easy, Less fragmentation\nRequires explicit partitioning of large objects\nComplex code to handle large objects\nSimple buffer manager but complex system\nReduced performance due to partitioning logic\n\n\nUmbra’s approach: Variable-size pages\n\nLarge objects are handled transparently\nCan store in-memory representation of large object on single page (for faster lookups)\nNo complex partitioning logic required\nIncreased complexity of buffer manager\nMain problem: External fragmentation of buffer pool\n\n\nHow they fixed the problem with external fragmentation\n\nExploit flexible mapping b/w virtual &amp; physical memory\n\nPages are organized in exponentially growing size classes\nFixed virtual addresses are reserved for each size class\nMapping to physical memory only for pinned buffer frames\n\n\nAvailable virtual memory range is allocated repeatedly w/ different size classes to get a space with multiple granularities.\n\nIf a large page is needed &amp; consecutive space isn’t available then free small pages can be de-allocated (which tells the OS to cut the link b/w virtual &amp; physical address space) to get the contiguous physical space for the large page.\nSidenote: https://pages.cs.wisc.edu/~remzi/OSTEP/vm-freespace.pdf (see Buddy Allocation)\nQ/A : Are there any issues w/ changing the virtual-physical mapping very often like increasing the size of kernel data strictures or blowing out the TLB cache? I’ve heard that using mremapto do this kind of fast re-alloc can cause other memory access to be slower.\n\nWe don’t use mremap because that’s indeed bad. We just tell the OS to unlink which keeps the mapping simple &amp; linear. There’s still a cost though when you change the size which shoots down the TLB of other cores so we don’t like to do this very often. We prefer to keep memory is a certain size &amp; only move it if we really have to. But it’s not terribly expensive. You can do this a few hundred times / second and that’s okay but you don’t want to do this a million of times.\n\n\nQ/A : How does flush work when backed by variable size pages?\n\nWriting to disk isn’t any different compared to fixed size pages. The thing to be careful about is recovery. You don’t want to change the size of the page since 2 small pages could be over-written by a large page which is unsafe since you aren’t sure if the header with the LSN number is in the correct place.\n\n\nQ/A : Do you use huge page for mapping so that when you unmap a 64KiB slot, it can be remapped elsewhere instead of using 2MiB &amp; ending up with a page splitting on a map?\n\nWe don’t use transparent huge pages.\nExtra TLB pressure for finer granularity.\n\n\n\n\nVariable-size pages alone aren’t sufficient for high performance.\n\nCompared to a purely in-memory systems, traditional buffer managers have other bottlenecks like acquiring a latch for every page that is accessed, pinning &amp; unpinning pages &amp; the root of the BTree being highly contentious.\nPerformance Optimizations:\n\nPointer swizzling\nVersioned latches\n\n\n\n\nPointer Swizzling\n\nDecentralized mapping from page IDs to memory addresses\n\nCentralized hash table is a major bottleneck in highly parallel architectures\nUse tagged pointers to encode both PIDs (for disk-resident pages) &amp; memory addresses (for memory-resident pages) so that you can avoid the hashtable lookup for already resolved pages\n\n\nPointer swizzling lets you access a page cheaply without a hashtable lookup.\nInner BTree node is composed of “swip” &amp; “key” where “swip” is a potentially swizzled pointer (page id if page not in memory OR the physical pointer to the page if in memory).\nNow, if you follow a page id from a page, we check if you already resolved this &amp; if yes then we just jump there. If not, then we have to acquire a lock &amp; do IO anyway.\nCost: When you evict the page, you must invalidate the swip otherwise another reader might think that the page is there when it’s not.\nHydra currently has only 1 incoming swip. Ideally, you must have a low number of incoming swips because you need to invalidate them when you evict a page.\n\n\nVersioned Latches\n\nTraditionally, you’d grab a R-W lock / latch on the root even if you just need to do reads. Determine where you need to go, then latch the child, release the root &amp; so on. Most of the time is spent on acquiring the mutex for the root even if there’s no logical contention.\nA versioned latch is just a single 64-bit integer counter which can be locked in different modes (optimistic, shared or exclusive).\n\nWe remember how many exclusive unlocks we did in the versioned latch. This allows you to check if there was a change.\n\n\nModes\n\nShared: Read-only access\nExclusive: Read-write access\nOptimistic: Read-only access, no locks, requires validation &amp; may fail\n\nNeeds to wait if the node already holds an exclusive lock\n\n\n\n\nNote: If you optimistically latch a page, the page might disappear while you’re reading. In the buffer manager implementation, if a page is evicted, we just read the zero page. Reader must be aware of this.\n\n\nRelations\n\nRelations are stored in B+ trees.\n\nKeys are synthetic (not visible to user) 8-byte tuple IDs.\nPAX layout within leaf pages.\n\n\nConcurrent access synchronized through optimistic latch coupling\n\nUser versioned latches provided by buffer manager\nRely on optimistic latches during non-modifying traversal\nLeaf pages shared mode for scans, optimistic mode for point access\n\nYou need to validate after every read in optimistic locking. In particular, the pages holding a string can go away at any time. For point access, you can just copy the string &amp; then check after if it’s still valid but we don’t want to do this during scans to avoid copying strings.\n\n\n\n\n\n\n\nMVCC on disk §\n\nWe want to avoid writing to disk due to the high overhead. So a mixed solution is used.\n\nRegular Case: Use an in-memory representation of the txn\n\nArgument: Majority of txn are small (a few hundred tuples)\nMaintain version chains exclusively in-memory\nAttach to DB pages through local mapping tables\nRetained by buffer manager when pages are evicted (re-attached when page comes back)\nWhen txns are large (eg. touching all the tuples in the DB) , we don’t want to have in-memory representation because it’s very expensive.\n\n\nBulk Operations\n\nArgument: Bulk operations are infrequent.\nGive exclusive bulk write access to relation.\nUse two bit (created, deleted) per tuple to create “virtual versions”\nVirtual version consume no physical memory\nProper isolation of arbitrarily large bulk operations\n\n\n\nStatistics §\n\nStatistics are essential for query optimization to determine things like how many tuples qualify for a predicate, how many distinct values they’ve for a group by etc.\nIn HyPer, they used sample for this. When the data changed too much, it was recomputed in-memory. We don’t want to do this on disk due to read amplification.\nUmbra uses reservoir sampling. While the data is inserted, updated or deleted; a sample is maintained all the time so it’s always available &amp; doesn’t need to be recomputed. They use some algorithm to predict the skip length b/w the hits &amp; the reservoir sample. Then figure out which tuple would be relevant for the sample &amp; insert it in the sample.\nHow to do this in a multi-threaded way? Skips b/w tuples are pre-computed. Skips are handed to threads on-demand which are responsible for skipping this many tuples until one qualifies.\n\nEach thread locally counts down until a relevant tuple is encountered.\nNote: Length of skips increases over time. Larger the sample, more likely that tuple is less relevant.\n\n\nSamples are great for predicated but poor for distinct (/group by) counts.\n\nMaintain HyperLogLog sketches for all columns.\n\n\nSidenote: There are some systems that try to solve for the IO cost by sampling pages instead of tuples but avoid doing this since this creates sampling bias in the data.\n\nCompilation &amp; Execution §\n\nIn the I/O case, we might want to pause our query but if our query is just a function we called then it’s difficult to pause. So we compile queries (physical execution plans) into state machines.\nQuery pipelines consist of multiple steps i.e. generated functions.\nViewed as states w/ well defined transitions.\nQuery executor orchestrates invocation of steps.\nThis is useful for both the I/O case &amp; for scheduling  long running queries (we don’t want someone to wait until the long running query is gone to run a cheap query).\nEg: \nQ/A: Is this implemented as a co-routine or as small functions?\n\nBehaves as co-routine but implemented as generated functions with explicit state.\n\n\nQ/A: Did you consider the SQL OS approach where they have small functions (at most 6ms)?\n\nWe didn’t because the SQL OS co-routines does memory allocation for its own state. We want to control memory.\n\n\nAdvantages of modular execution plans\n\nQuery execution can easily be suspended (eg. on high IO pressure)\nEnables fine-grained parallelization of pipelines.\n\n\nSteps are compiled adaptively\n\nEach function can be compiled separately.\nCode generation employs custom IR similar to LLVM IR.\nInitially translated into machine code using a low-latency backend.\nIf beneficial, translated into LLVM IR &amp; just-in-time (JIT) compiled.\n\nEg: Very few instructions are needed for something like “allocate a hashtable” so no point in optimizing that. Only done when the step is determined to be expensive.\n\n\nNote: The query is running &amp; not stopped for this. They compile in background &amp; switch while executing.\n\n\nLow-latency compilation\n\nCompile time is a problem is compiling engine. LLVM has super linear compile time for large functions.\nSo they started directly emitting machine code.\nMachine code generation wasn’t as difficult as register allocation (which requires detecting loops &amp; doing lifetime computations) but implementing register allocation guarantees almost linear compile time.\nOptimized LLVM is used adaptively &amp; only when beneficial.\nSidenote:\n\nLLVM will take 14s to compile a function w/ 2K joins &amp; 4s if asked to do it cheaply. Their custom compiler does it in 33ms.\nLLVM w/ optimizations enabled produces more performant code but takes more time so it’s not worthwhile for most parts of the query.\n\n\nQ/A: Would a vectorized runtime do any better? We’re trying to optimize for I/O stalls in our query plan. Would a vectorized engine be easier/better to use?\n\nVectorized engine is fine for OLAP workloads but compiled engines are significantly better for OLTP workloads since the vectorization overhead becomes noticeable when only a few tuples are used.\n\n\nQ/A: Do you cache query plans to avoid compiling?\n\nCompile times are so low that we don’t cache query plans anymore.\n\n\n\n\n\nExperiments §\n\nSystem comparison: In memory, warm cache\n\nOn Join Order Benchmark (JOB): 3x faster than HyPer, 4.6x than MonetDB.\nOn TPCH: 1.7x faster than Hyper, 1.9x faster than MonetDB.\nNote: Faster than HyPer not because of faster runtime but because of better statistics &amp; query plans.\n\n\nCost of buffer manager\n\nReplaced buffer manager by flat mmap files.\nSimilar performance w/ minor performance fluctuations (6% avg., 30% max).\n\n\nRead perf. w/ cold caches\n\nBuffer manager &amp; flat mmap files perform equal.\nFull utilization of SSD read bandwidth in both cases.\nBuffer manager much faster than mmap under high concurrency.\n\n\nSidenote: mmap is awful for highly parallel reads due to lock contention.\n\nConclusion §\n\nUmbra offers the best of both worlds\n\nPerf. of an in-memory system if data fits into main memory\nScalability of a disk-based system beyond main memory\n\n\nMade possible by\n\nEfficient &amp; scalable buffer management w/ variable-size pages\nNovel modular execution plan model &amp; adaptive query compilation\nFurther adaptations to statistics maintenance &amp; string handling\n\n\n\nQ/A §\n\nDo you use a fixed page size for the internal leaf node or a fixed tuple count? Do you target being just under a size that’s power of 2 to reduce fragmentation?\n\nWe try to reach the 64KiB size in the B-Tree leaf pages but we’ll use a larger page for larger data. Inner nodes always have 64KiB pages because the keys are tuple ids.\n\n\nDo you use 64KiB size for under-full pages too? Do you merge pages on delete?\n\nIt’s very unlikely for pages to be under-full. We try to put as many tuples as possible in the pages.\nWe merge on delete.\n\n\nIn the indirection trick (pointer swizzling) do you use using pre-fetching or just cache misses?\n\nIIRC, first time you go down, you just fill the cache. For multiple lookups, we remember where we have to go next &amp; pre-fetch the page.\n\n\nDo you have a fixed no. of threads running the queries or do you allocate new threads as queries come?\n\nWe’ve a fixed no. of threads equal to the number of cores. It doesn’t make any sense to have more threads than cores because they can’t run concurrently. We schedule query to thread.\n\n\nIf you’ve a fixed no. of threads, then you’ve some OLAP queries taking a lot of time &amp; then there’ll be some OLTP queries. How do you decide the scheduling of these queries so that OLTP queries have low latency &amp; OLAP queries can still execute fast?\n\nWe use decaying priority i.e. the longer the query runs the lesser its priority.\n\n\nWhen you pause a query, do you place some checkpoint or do you just pause the thread?\n\nThe multi-threaded steps are using morsel driven parallelism i.e. the threads process a few thousand tuples &amp; then fall back to the scheduling function &amp; if someone decides that you thread should yield then you stop at the next morsel boundary &amp; switch to another query.\n\n\nYou mentioned that you use only 1 thread per core so do none of these threads block on I/O. Does the CPU not get saturated?\n\nWe aren’t happy w/ this part yet. We use NVMe SSDs which have pretty low latency so we block synchronously on I/O (which is unlikely in itself). But if your device isn’t a very fast SSD then you don’t want to do this. You need another thread which hands out pages to other workers. Workers should never block for long times.\n\n\nIf you’re using the NVMe PCI SSDs then could you redirect the I/O to those threads that are running on the cores that are directly attached to those PCI slots?\n\nGood idea. Didn’t implement this in Umbra. We did something similar in HyPer where we prefer to read memory from local NUMA nodes. We’d re-schedule threads to read from the local node. Could do something similar in Umbra too.\n\n\nHow does the DB decide to use LLVM to compile the query plan? Is there some statistics to help predict the speed-up from the optimized LLVM?\n\nWe keep track of the progress in each pipeline. We predict how long we’ll need to finish. We’ve a model (basically a polynomial function) that tells how expensive it’s to compile something w/ LLVM &amp; the expected speed-up. If worthwhile then we start compilation &amp; switch but if the remaining running time is not significant then we don’t.\n\n\nIs this similar to what you were doing w/ HyPer as mentioned in the ICD paper?\n\nWe switched b/w a virtual machine or 2 LLVM nodes in HyPer but we stopped doing this. Our initial backend is different now.\n\n\nDo you have async I/O abilities enabled w/ something like io_uring or the Windows I/O ring? Does that impact your ability to schedule things into async work?\n\nWe use io_uring for background threads that do I/O &amp; hands pages to worker threads when available.\n\n\nPAX encoding on leaf pages. How do you navigate the tension b/w dense encodings, low runtimes &amp; CPU overhead on decoding those encodings? Do you use run-length encoding on the PAX pages?\n\nText layout is optimized for OLTP workload. Not heavily compressed.\nWe’ve a different backend for OLAP where we do run length encoding but not for OLTP.\n\n\nSince you mentioned OLAP, what do you think about non-CPU execution resources since CPU is tapering off in frequency &amp; core counts. What do you think about GPU or even CPU SIMD instructions?\n\nWe use SIMD instructions for filtering during scans for tables etc.\nWe tried using GPU for acceleration but the issue was that when the data is large than GPU memory (which is usually the case), you’ve to transfer data in &amp; out of the GPU which tends to ruins the perf. advantage you gain from GPU. Gains weren’t impressive enough to do in our main systems.\n\n\nIs Umbra still pg compatible? What level of compatibility does it have (SQL grammar, catalogs, other functionality)?\n\nSame grammar &amp; wire protocol so a pg driver can be used.\nA few catalog tables are exposed for drivers to work.\n\n\n\nAppendix §\n\nhttps://umbra-db.com/\nPaper : Umbra: A Disk-Based System with In-Memory Performance, Thomas Neumann and Michael J. Freitag\nhttps://dbdb.io/db/umbra\n"},"notes/using-scale-to-our-advantage-...-and-yours-(dive-deep-on-amazon-s3)-2024":{"title":"Using scale to our advantage ... and yours (Dive deep on Amazon S3) (2024)","links":[],"tags":["talks","aws-reinvent"],"content":"Seth Markle : Senior Principal Engineer ; working on S3 for the last 15 years, worked on object indexing, disk fleet, frontend systems, S3 metadata systems\nJames Bornholt  : Principal Engineer at S3 ; worked on disk fleet, S3 Tables &amp; connectors like mountpoint, pytorch\nMost talks on scale focus on how to tolerate getting larger . This talk focuses on how the system improves because it’s larger.\nStats on AWS S3 at scale\n\nOver 400 trillion objects\nOver a quadrillion requests per year\nOver 200 billion events daily\nOver 1 PB/s transferred at peak\n\nPhysics of Data §\n\nS3 has tens of millions of drives and they run really hot in terms of their workload.\nA drive can only absorb a limited amount of work.\nIf traffic can’t be spread across those 10s of millions of drives, then it limits what our customer can do even though we’ve the capacity on hand to serve that traffic.\nSome patterns emerge at scale.\nActors in the system: Customer, Hardware, Software\n\nThe hardware, HDDs §\nMechanical components with 2 primary movements required to read data: spinning platters, actuators (with read/write head).\nWhen you read data, you provide an address (called a Logical Block Address or LBA) which is just an offset from 0 to the capacity of the drive. The actuator swings back and forth to move into location. Once it’s on the right track, it needs to wait for the data to come around if it’s not directly underneath it (half a rotation on average).\nFactors that cause drives to take time when you’re reading data\n\nSeek time: Seeking is the act of actuator moving. Time for that to happen is called seek time.\nRotational latency: Time spent waiting for rotation is called rotational latency.\n\nSee https://animagraffs.com/hard-disk-drive\n\nThe software, Replication §\n\n\nWhen you store an object in S3, it’s broken apart into pieces called shards.\nShards are stored redundantly across the disk fleet using erasure coding.\nThe drives run use a filesystem software called ShardStore. ShardStore is a log structured file system.\n\n\n\nWrites becomes more efficient since the actuator doesn’t have to move back and forth.\nReads require random IO since the customer can request any data.\n\nIndividual workloads §\n\nStorage workloads tend to be bursty (idle for a long time and then suddenly demanding)\nCustomer Example: FINRA\n\nRegulates securities market. Collect, process, analyzes all txns. across equity and option markets.\nInvolves ingesting 100s of billions of records each day.\n\nIn August ‘24, they had a peak day w/ 906 billion records processed two days in a row.\n\n\nOn average, they process 30TB of data everyday.\nThey’ve a strict 4 hour SLA for processing this data.\nRest of the time they ingest records but bulk processing happens in a short window only.\n\n\nAnother example: How many drives would an example workload need to operate?\n\nWorkload: Hold 1PB of data, 1MB per object, Process the data in a 1hr\ni.e. Access Rate: 275GB/s at peak;  ~ 1 billion objects\nPhysics of data\n\nRotation = 4ms, average (7200 RPM drive i.e. 8ms per rotation and half a rotation on avg.)\nSeek = 4ms, average (move half the radius on the platter on avg.)\nSay, 1MB objects are chunked into 0.5MB shards.\n0.5MB transfer = 2ms, average\nTotal = 10ms per read (rotation + seek + transfer)\n\ni.e. 50MB/s at 0.5MB per read\n\n\n\n\nFor just storing 1PB of data they’d need 50 drives at 20TB per drive\nTo access the data at 275GB/s, they’d need 5500 drives at 50MB/s\n\n100x difference to support bursts\nSystem is idle the rest of the time\n\n\n\n\n\n\nWhile individual workloads are bursty, aggregated de-correlated workloads aren’t. When the workloads are layered together, they become more predictable at S3’s scale. There are some patterns though like younger data is “hotter” data, small objects are accessed more frequently.\n\n\nShards for a customer can thus be spread across a large number of disks than their storage would typically require and get the additional throughput.\nPeaks for some workloads map to valleys for others.\n\nThermodynamics : Balancing the aggregates §\n\nWhile aggregate workloads are predictable, you still need to pay attention to how traffic distributes across the fleet to prevent hot &amp; cold spots across the disk. Doesn’t happen automatically.\nObjects follow some predictable patterns in aggregate (eg. younger data is hotter data) but over time drives “cools off” as it gets older.\n\nDeletes might poke some holes and make some data on the old drive “hot” but not nearly as much as the new drives.\nUncorrected, the system tends towards “coldness”.\n\n\nAt S3’s scale, they’ve storage racks of every traffic profile in the system. There’s a really large surface area for rebalancing &amp; re-encoding when they get things wrong. They constantly move data around to balance storage temperatures across racks and hard drives.\nOne useful opportunity for them to do rebalancing is when they get a new storage rack.\n\nThey weight 1000s of pounds.\nCalled “JBODs” (just a bunch of disks).\n20TB / disk. Racks have physical capacity of 20PB.\n\n\nCan’t start sending all traffic to the new drives because it’d overwhelm those drives.\n\nSo, new racks are brought online &amp; filled ~80% w/ existing cooler S3 data.\nThis frees up capacity in aggregate across pre-existing spindles &amp; thus there’s a lot of space on old spindles for hot data to land &amp; not just in the new spindles.\n\n\n\nDesigning de-correlated systems §\n\nEvery part of S3 is designed to de-correlate workloads.\nEg: If you’ve 2 customers running workloads then those 2 workloads should be de-correlated from each other. And even a single customer, your workloads should be de-correlated from itself.\n\nAssigning buckets to their storage §\nA straw-man approach: You assign a hard drive to a bucket and you keep putting objects in the same drive until its full. You then assign another drive and so on.\n\nAdvantage: Simple &amp; easy to operate and reason about.\nIssues\n\nHard drives fail so data needs to be stored on multiple drives.\nNot cost-efficient to assign an entire drive esp. for a small customer.\nA small customer is going to share the drive w/ other customers &amp; many workloads will hit the same drive.\nFor large customers, performance is constrained by their storage footprint.\n\n\n\nShuffle sharding §\n\nWorkloads are randomly spread across the fleet, across the drives. Even for a PUT to the same bucket or the same key.\n\nNot constrained by static resources i.e. if you’ve 1PB of data, you can be spread across more than 1PB worth of drives.\nA small customer is insulated against the “noisy neighbor” problem. If an object from another customer is “hot”, you won’t be affected by it even if you share some drives since there are other drives where you can get that data.\nShuffle sharding is also used for DNS resolution of S3 buckets so that your requests are spread across multiple (front-end) hosts.\n\n\nPlacing data for shuffle sharding\n\nCan’t look at every drive and figure out the optimal set of drives for every PUT operation because S3 uses 10s of millions of drives &amp; gets millions of RPS.\nCompletely randomly leads to a bell curve distribution in a graph for capacity usage over time. Over a long time, some drives end up being 10-20% less full than other drives. Wasted capacity.\nThe power of two random choices: Pick 2 drives totally at random &amp; figure out which one is least used and use that drive.\n\n\n\n\n\nSidenote: Shuffle sharding is also used in AWS Common runtime (CRT) which is a low-level library for working with S3 and implements retries, parallelization of requests etc. to get high throughput for S3. It’s not used by most customers directly and is included in SDKs.\nTo improve performance when you talk to S3, the CRT keeps a live estimate of the tail latency of the requests you’re making. CRT intentionally cancels &amp; retries requests that go past a certain threshold (say p95).\nIt’s very likely that the new request will go to a different front-end host &amp; different drive &amp; the retry will be fast.\n\n\nEngineering for failure is engineering for velocity §\n\nAt scale, knowing that a system has failed is surprisingly hard.\nAll bytes stored are constantly scanned to check for integrity.\n\nThere are alarms on p100 longest time, the most time a byte has gone w/o being scanned.\n\n\nErasure coding for fault tolerance\n\nReplication is a simple way to tolerate faults - both individual drives and entire AZs but comes w/ high overhead.\nErasure coding splits an object into K shards &amp; creates extra parity shards. The object can be rebuilt from any K of the shards.\n\nEg: Object is split into 5 shards, 4 extra parity shards are added. These 9 shards can be distributed across 3 AZs. Even if an entire AZ goes done, the original data can still be recovered from any 5 shards.\n\n\nCompared to replication where the overhead would be 3x for storing a copy in 2 separate AZs (in practice it’d much higher to guarantee S3’s 11 nines durability), erasure coding only has 1.8x overhead.\nErasure coding allows them to deploy new software or hardware safely by exposing only a few shards to the changes. Eg: some new software could just be deployed on a single host or rack, a new hard drive type can just be used for 1, 10 or 100 drives.\n\n\nUsing shuffle sharding at scale, there’s going to be some objects that are overly exposed to new buggy hardware or software.\n\nS3’s shuffle sharding system has a guardrail. It knows about new hardware and software. When it picks a new random assignment, it limits itself to putting only 1 shard on that new hardware or software.\nWhen ShardStore (storage node software) was rolled out for S3, they started w/ only storing 1 shard for a while. The system was tuned to eventually raise the no. of shards being stored &amp; eventually the guardrail was removed.\n\n\nImproved performance through fault tolerance\n\nSimilar to AWS CRT, additional shards &amp; shuffle sharding allow hedging against tail latency by over-reading. Reading an extra shard lets them ignore the slowest shard &amp; pick another to get the object back. The request to the slowest shard is cancelled.\n\n\n\nAppendix and Further Reading §\n\nSlides: https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/stg/STG302_Dive-deep-on-Amazon-S3.pdf\nBuilding and operating a pretty big storage system called S3\nWorkload isolation using shuffle-sharding\nUsing Lightweight Formal Methods to Validate a Key-Value Storage Node in Amazon S3\n"},"notes/cmudb-15721/lakehouse---a-new-generation-of-open-platforms-that-unify-data-warehousing-and-advanced-analytics-2021":{"title":"Lakehouse - A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics (2021)","links":[],"tags":["db","paper"],"content":"What is the research? §\nThe paper introduces the Lakehouse architecture for data storage and analytics on open file formats. Their implementation provides comparable performance with better consistency by directly using the source data unlike the two-tier data lake + warehouse architecture which requires additional ETL and storage.\nArchitecture §\n\n\nIt might not be evident in the figure but there’s usually a staging area where data is processed before loading it into a data warehouse.\n\nSystems using the lakehouse architecture access and store data in an object store (owned by the customer) and run a transactional metadata layer on top to keep track of objects and which table they belong to.\nThe metadata layer enables safer access to the underlying data and allows multi-object updates (required since a single table is split across multiple files).\n\nThey don’t mention it in the paper but there’d be some kind of a catalog on top of the object metadata layer to track all the tables and their schema.\n\nThe data is stored using open file formats like Parquet or ORC.\nTheir implementation uses Delta Lake for metadata storage and the Delta Engine for execution.\nDelta Lake stores the metadata as Parquet files in the data lake (object store) itself.\nFor better performance, they maintain additional data on top of the cloud data lake. Their execution engine caches files (on SSDs, RAM) and maintains auxiliary data (like indexes and stats).\nApart from this, they also reorder records to cluster.\n\nThey use space-filling curves to reorder data which based on multiple parameters instead of just one. Space-filling curves provide a way to encode those multi parameters into a single value while preserving the closeness of those records for better ordering.\nI’d suggest checking out Hilbert’s curve if you’re interested.\n\nEvaluation §\n\nThey’ve used the TPC-DS benchmark with 30TB of data (stored as Parquet files). Even in their own evaluation, the Delta Engine doesn’t do too well compared to one of the undisclosed vendors on test duration and cost for on-demand instances.\nBut, getting similar performance to data warehouses (that require additional storage, data pipelines and have a proprietary format) is definitely a win.\nDiscussion §\nWhile the paper mentions supporting multiple formats, as per current Delta Lake docs1, it only supports storage in Parquet files.\nYou can import CSV, JSON and other formats but Delta Lake would need to convert them to Parquet files first. While Parquet is more efficient for storage and easier to optimize for execution, I’d like to see support for more unstructured formats.\nThey also mention decompressing the Parquet data in cache for faster execution. Using the Arrow in-memory format could make reads faster for “hot data”.\nI also came across Building a serverless Data Lakehouse from spare parts in which the authors implement the lakehouse architecture entirely with open source tools.\nFile and table formats seem standard now. With recent efforts in commoditizing the catalog2 and execution3 layer, the lock-in will eventually move from the infra to the overall platform. Hopefully, this leads to vendors focusing on better UX for business and data science teams.\nFootnotes §\n\n\nhttps://web.archive.org/web/20240810113526/https://docs.delta.io/latest/delta-faq.html#where-does-delta-lake-store-the-data ↩\n\n\nhttps://materializedview.io/p/make-lakehouse-catalogs-boring-again ↩\n\n\nhttps://datafusion.apache.org/ ↩\n\n\n"},"til/catching-compromised-cookies-2024":{"title":"Catching Compromised Cookies (2024)","links":[],"tags":["til"],"content":"session forking : cookie being used from more than one device at the same time\n\ninitial approach\n\nmatch last access timestamp\n\ncan’t know which forked session is legitimate, only that session is forked\nfalse positive in some cases (eg: client didn’t receive the cookie after the server has generated it and registered the new timestamp)\n\n\nmatch ip address too\n\nif last timestamp is different but ip address is same then the request is likely from the same client\nreduced false positives from only matching last access timestamp\n\n\n\n\nbetter approach\n\nonly update timestamp when it’s confirmed that the client has stored the new cookie\nhow?\n\n2-phased approach ; each request is idempotent\nto update the session cookie, send a separate “session candidate” cookie\nif client makes a request with the session candidate cookie then its upgraded to the session cookie\n\n\naside: in case of a race condition (client sends a group of requests in quick succession), the db is updated for the first request that comes in &amp; the timestamp value is ignored for other requests since the timestamp was set recently\n\n\nfurther\n\nthey use device and network information now too in addition to timestamp\nrisk level is categorized into different levels (low, mid, high) &amp; high level events send an alert\nperf: if last access timestamp in the cookie is recent, db is not checked for it to avoid too many reads (if the session was forked, they would’ve already detected it)\n\n\n"},"til/checking-dirty-shutdowns":{"title":"Checking dirty shutdowns","links":[],"tags":["db","til"],"content":"How you can check if a clean exit or a dirty shutdown happened in a DB?\n// https://github.com/tursodatabase/libsql/blob/49e3c04d191592e7a17f7a51d767b0a76bf12007/libsql-server/src/lib.rs#L237-#L250\n \n/// initialize the sentinel file. This file is created at the beginning of the process, and is\n/// deleted at the end, on a clean exit. If the file is present when we start the process, this\n/// means that the database was not shutdown properly, and might need repair. This function return\n/// `true` if the database is dirty and needs repair.\nfn init_sentinel_file(path: &amp;Path) -&gt; anyhow::Result&lt;bool&gt; {\n    let path = sentinel_file_path(path);\n    if path.try_exists()? {\n        return Ok(true);\n    }\n \n    std::fs::File::create(path)?;\n \n    Ok(false)\n}"},"til/how-discord-indexes-billions-of-messages-2017":{"title":"How Discord Indexes Billions of Messages (2017)","links":[],"tags":["search"],"content":"\ntechnical requirements: fast, self-healing, linearly scalable, lazily indexed\n\nlazily indexed - don’t index until someone attempts to search messages at least once\n\n\ndecided to use elasticsearch (evaluated solr vs elasticsearch)\nwanted to avoid large es clusters since they’re difficult to maintain and team didn’t have much experience\nidea: delegate sharding to &amp; routing to application layer which would allow them to index messages into a pool of smaller es clusters\n\ncluster outage would only affect limited messages\ncan throw away unrecoverable clusters easily (and lazily re-index when needed)\n\n\nes prefers bulk indexing\n\nqueue messages and process in bulk\ndelay is reasonable since users mostly search for historical messages and not recent ones\n\n\ntooling\n\nmessage queue (used Celery)\nindex workers (routing and bulk inserts from queue)\nhistorical index workers\nshard mapping (discord_guild -&gt; es cluster + index)\n\npersistent (on Cassandra)\ncache (used mget in Redis)\n\n\nshard allocator (one-time assignment of “shard”)\n\nstore shards ranked by score that represents their load (used sorted sets in Redis)\nscore is incremented w/ each new allocation\neach indexed message in es had a probability to increment the score of its shard\n\n\ndiscovery (used etcd)\n\nof clusters and hosts within them\n\n\n\n\nraw message data not stored in ES\n\nonly fields actually stored (store: &quot;yes&quot; in es) are message id, server id &amp; channel id\nmessage metadata is stored in inverted index\ntimestamp isn’t included in index since ids contain a timestamp (snowflake ids)\n\n\nfor search results, message is fetched from Cassandra along w/ message context (2 messages before &amp; after)\nwhen testing\n\nobserved unexpected disk &amp; cpu usage\nes’s index refresh interval is 1s by default (to support near real-time search) so every second es was flushing in-memory buffer to a lucene segment &amp; opening the segment to make it searchable across a thousand indexes\njust increasing the refresh interval to 1hr didn’t work since a discord server could go hours w/o needing to execute a single query\n\ncontrol refreshing from application layer\n\nafter bulk insert, marking guild ids &amp; es shard containing them as dirty\n\nexpire after 1 hour (refresh would’ve happened by then)\n\n\non search, check if the es shard AND guild id are dirty &amp; refresh the shard’s es index if so\n\n\n\n\n\n\n\nAppendix\n\n“guild” is just the internal name for a server\n“shard” in es vs. “shard” in this post\n\nes index is split into shards for distributing data\nin this post, a shard just maps to a guild\nan es shard can contain data for multiple guild ids\n\n\nIndex Template\n\n{\n    &#039;template&#039;: &#039;m-*&#039;,\n    &#039;settings&#039;: {\n        &#039;number_of_shards&#039;: 1,\n        &#039;number_of_replicas&#039;: 1,\n        &#039;index.refresh_interval&#039;: &#039;3600s&#039;\n    },\n    &#039;mappings&#039;: {\n        &#039;message&#039;: {\n            &#039;_source&#039;: {\n                &#039;includes&#039;: [\n                    &#039;id&#039;,\n                    &#039;channel_id&#039;,\n                    &#039;guild_id&#039;\n                ]\n            },\n            &#039;properties&#039;: {\n                // This is the message_id, we index by this to allow\n                // for greater than/less than queries, so we can search\n                // before, on, and after.\n                &#039;id&#039;: {\n                    &#039;type&#039;: &#039;long&#039;\n                },\n                // Lets us search with the &quot;in:#channel-name&quot; modifier.\n                &#039;channel_id&#039;: {\n                    &#039;type&#039;: &#039;long&#039;\n                },\n                // Lets us scope a search to a given server.\n                &#039;guild_id&#039;: {\n                    &#039;type&#039;: &#039;long&#039;\n                },\n                // Lets us search &quot;from:Someone#0001&quot;\n                &#039;author_id&#039;: {\n                    &#039;type&#039;: &#039;long&#039;\n                },\n                // Is the author a user, bot or webhook? Not yet exposed in client.\n                &#039;author_type&#039;: {\n                    &#039;type&#039;: &#039;byte&#039;\n                },\n                // Regular chat message, system message...\n                &#039;type&#039;: {\n                    &#039;type&#039;: &#039;short&#039;\n                },\n                // Who was mentioned, &quot;mentions:Person#1234&quot;\n                &#039;mentions&#039;: {\n                    &#039;type&#039;: &#039;long&#039;\n                },\n                // Was &quot;@everyone&quot; mentioned\n                // (only true if the author had permission to @everyone at the time).\n                // This accounts for the case where &quot;@everyone&quot; could be in a\n                // message, but it had no effect, \n                // because the user doesn&#039;t have permissions to ping everyone. \n                &#039;mention_everyone&#039;: {\n                    &#039;type&#039;: &#039;boolean&#039;\n                },\n                // Array of [message content, embed title, embed author,\n                // embed description, ...] for full-text search.\n                &#039;content&#039;: {\n                    &#039;type&#039;: &#039;text&#039;,\n                    &#039;fields&#039;: {\n                        &#039;lang_analyzed&#039;: {\n                            &#039;type&#039;: &#039;text&#039;,\n                            &#039;analyzer&#039;: &#039;english&#039;\n                        }\n                    }\n                },\n                // An array of shorts, specifying what type of media the message has.\n                // &quot;has:link|image|video|embed|file&quot;.\n                &#039;has&#039;: {\n                    &#039;type&#039;: &#039;short&#039;\n                },\n                // An array of normalized hostnames in the message,\n                // traverse up to the domain. Not yet exposed in client.\n                // &quot;http://foo.bar.com&quot; gets turned into [&quot;foo.bar.com&quot;, &quot;bar.com&quot;]\n                &#039;link_hostnames&#039;: {\n                    &#039;type&#039;: &#039;keyword&#039;\n                },\n                // Embed providers as returned by oembed, i.e. &quot;Youtube&quot;.\n                // Not yet exposed in client.\n                &#039;embed_providers&#039;: {\n                    &#039;type&#039;: &#039;keyword&#039;\n                },\n                // Embed type as returned by oembed. Not yet exposed in client.\n                &#039;embed_types&#039;: {\n                    &#039;type&#039;: &#039;keyword&#039;\n                },\n                // File extensions of attachments, i.e. &quot;fileType:mp3&quot;\n                &#039;attachment_extensions&#039;: {\n                    &#039;type&#039;: &#039;keyword&#039;\n                },\n                // The filenames of the attachments. Not yet exposed in client.\n                &#039;attachment_filenames&#039;: {\n                    &#039;type&#039;: &#039;text&#039;,\n                    &#039;analyzer&#039;: &#039;simple&#039;\n                }\n            }\n        }\n    }\n}"},"til/web-server-with-tls-in-go":{"title":"Web Server with TLS in Go","links":[],"tags":["til"],"content":"package main\n \nimport (\n\t&quot;github.com/gin-gonic/autotls&quot;\n\t&quot;github.com/gin-gonic/autotls&quot;\n)\n \nfunc main() {\n\trouter := gin.Default()\n\t\n\trouter.GET(&quot;/&quot;, func (c *gin.Context) {\n\t\tc.String(200, &quot;Hello World!&quot;)\n\t}\n \n\tautotls.Run(router, &quot;example.com&quot;)\n}\n// From Caddy&#039;s Author\n \ncertmagic.HTTPS([]string{&quot;example .com&quot;}, mux)\nAlso see:\n\nSetting Up Vanity Subdomains for Your SaaS Using Next.js and Caddy\nauto manage ssl/tls certificates in go (has AWS Route53 setup)\n"},"til/psql-shows-null-as-empty-string":{"title":"psql shows NULL as empty string","links":[],"tags":["db","til"],"content":"This query returns null and not an empty string but that’s not really evident in psql.\npostgres=# SELECT to_char(&#039;1997-02-28 10:30:00&#039;::TIMESTAMP, null);\n to_char\n---------\n\n(1 row)\n\nTo change this behaviour, set this configuration\npostgres=# \\pset null null\nNull display is &quot;null&quot;.\n\nContext:\nI was working on https://github.com/apache/arrow-datafusion/pull/9689 and just assumed that the value being returned was an empty string since I didn’t see anything when I ran the above SELECT query in psql.  I should’ve verified my assumption like this:\nSELECT\n\tCASE\n\t\tWHEN your_column IS NULL THEN &#039;Value is NULL&#039;\n\t\tELSE &#039;Value is not NULL&#039;\n\tEND;"}}